# AI Processor Block

## Overview

The AI Processor Block is a component within the `kwargify-core` project designed to process input text using a Large Language Model (LLM). It leverages the `litellm` library to interact with various LLM providers, allowing for flexible AI-powered text generation and processing tasks. The block takes input data (content) and an optional set of instructions (prompt), sends them to the configured AI model, and returns the model's response.

## Inputs

The AI Processor Block requires the following inputs:

- **`content` (str):** This is the primary text data that needs to be processed by the LLM. This input is mandatory.
- **`instructions` (str, optional):** This input provides system-level instructions or a prompt to guide the LLM's behavior and response generation. If not provided directly as an input, it can be set via configuration (see below). If neither is provided, it defaults to "You are a helpful assistant.".

## Outputs

The block produces the following output:

- **`response` (str):** This is the text generated by the LLM based on the provided `content` and `instructions`.

## Configuration Options

The AI Processor Block can be configured with the following options:

- **`model` (str):** Specifies the LLM to be used for processing. This should be a model identifier compatible with `litellm` (e.g., 'gpt-4o-mini', 'claude-3.5-sonnet', 'gpt-4o'). Defaults to 'gpt-4o-mini' if not specified.
- **`temperature` (float, optional):** Controls the randomness of the LLM's output. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more focused and deterministic. Defaults to `0.2`.
- **`instructions` (str, optional):** Similar to the `instructions` input, this configuration option allows setting a default system-level prompt for the LLM if no `instructions` input is provided during runtime.

## Interaction with AI Services

The AI Processor Block interacts with AI services through the `litellm` library. `litellm` acts as a unified interface to various LLM providers (OpenAI, Anthropic, Cohere, etc.).

- **API Keys:** To use specific LLMs, you will typically need API keys from the respective providers. These keys are **not** configured directly within the block's definition in a workflow file. Instead, `litellm` expects these API keys to be set as environment variables (e.g., `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`). Refer to the `litellm` documentation for the specific environment variable names required for different models. This approach keeps sensitive API keys out of the workflow configuration files.

## Example Usage

Here's an example of how to configure and use the AI Processor Block within a hypothetical workflow definition (e.g., in a Python script or a YAML configuration if your workflow engine supports it):

```python
# Assuming a workflow structure where blocks are defined as dictionaries
# or instantiated from a class.

workflow_definition = {
    "steps": [
        {
            "name": "summarize_text",
            "block_type": "AIProcessorBlock",  # Or the registered name for this block
            "config": {
                "model": "gpt-4o-mini",
                "temperature": 0.3,
                "instructions": "Summarize the following text concisely, highlighting the key points."
            },
            "inputs": {
                # 'content' would typically be an output from a previous block
                # or loaded from an external source.
                "content": "{{ previous_step.output_text }}"
            }
        },
        # ... other steps
    ]
}

# In a workflow execution:
# 1. The 'content' (e.g., a long article) is passed to the 'summarize_text' step.
# 2. The AIProcessorBlock sends this content along with the configured
#    'instructions' to the 'gpt-4o-mini' model.
# 3. The LLM generates a summary.
# 4. The 'response' output of this block will contain the generated summary,
#    which can then be used by subsequent blocks in the workflow.
```

**Note on API Keys for the example:**
Before running a workflow with this block, ensure the necessary API key for the chosen model (e.g., `OPENAI_API_KEY` for `gpt-4o-mini`) is set as an environment variable in the execution environment.

For example, if you are using a model from OpenAI, you would need to set:

```bash
export OPENAI_API_KEY="your_openai_api_key_here"
```

Or for Anthropic:

```bash
export ANTHROPIC_API_KEY="your_anthropic_api_key_here"
```

Consult the `litellm` documentation for the correct environment variables for other models.

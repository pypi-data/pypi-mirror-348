Metadata-Version: 2.4
Name: kamiwaza-mlx
Version: 0.1.0
Summary: Unified MLX server & CLI (language and vision) with OpenAI-compatible endpoints
Author-email: Matt <opensource@kamiwaza.ai>
License: MIT License
        
        Copyright (c) 2024-2025 Kamiwaza.AI
        
        Permission is hereby granted, free of charge, to any person obtaining a copy
        of this software and associated documentation files (the "Software"), to deal
        in the Software without restriction, including without limitation the rights
        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
        copies of the Software, and to permit persons to whom the Software is
        furnished to do so, subject to the following conditions:
        
        The above copyright notice and this permission notice shall be included in all
        copies or substantial portions of the Software.
        
        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
        SOFTWARE.
        
Project-URL: Homepage, https://github.com/kamiwaza-ai/kamiwaza-mlx
Project-URL: Bug Tracker, https://github.com/kamiwaza-ai/kamiwaza-mlx/issues
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: fastapi>=0.115.0
Requires-Dist: uvicorn[standard]>=0.34.0
Requires-Dist: pydantic>=2.8.0
Requires-Dist: requests>=2.31.0
Requires-Dist: Pillow>=11.0.0
Requires-Dist: mlx>=0.25.0
Requires-Dist: mlx-lm>=0.24.0
Requires-Dist: mlx-vlm>=0.1.25
Dynamic: license-file

# Kamiwaza-MLX ğŸ“¦

Easily installable **PyPI** wrapper around the original MLX server / client
scripts. Once published you can simply do:

```bash
pip install kamiwaza-mlx

# start the server
a) python -m kamiwaza_mlx.server -m ./path/to/model --port 18000
# or, if you enabled the optional entry-points during install
b) kamiwaza-mlx-server -m ./path/to/model --port 18000

# chat from another terminal
python -m kamiwaza_mlx.infer -p "Say hello"
```

The remainder of this README documents the original features in more detail.

# MLX-LM ğŸ¦™ â€” Drop-in OpenAI-style API for any local MLX model

A FastAPI micro-server (server.py) that speaks the OpenAI
`/v1/chat/completions` dialect, plus a tiny CLI client
(`infer.py`) for quick experiments.
Ideal for poking at huge models like Dracarys-72B on an
M4-Max/Studio, hacking on prompts, or piping the output straight into
other tools that already understand the OpenAI schema.

---

## âœ¨ Highlight reel

| Feature | Details |
|---------|---------|
| ğŸ”Œ OpenAI compatible | Same request / response JSON (streaming too) â€“ just change the base-URL. |
| ğŸ“¦ Zero-config | Point at a local folder or HuggingFace repo (`-m /path/to/model`). |
| ğŸ–¼ï¸ Vision-ready | Accepts `{"type":"image_url", â€¦}` parts & base64 URLs â€“ works with Qwen-VL & friends. |
| ğŸ¥ Video-aware | Auto-extracts N key-frames with ffmpeg and feeds them as images. |
| ğŸ§® Usage metrics | Prompt / completion tokens + tokens-per-second in every response. |
| âš™ï¸ CLI playground | `infer.py` gives you a REPL with reset (Ctrl-N), verbose mode, max-token flagâ€¦ |

---

## ğŸš€ Running the server

```bash
# minimal
python server.py -m /var/tmp/models/mlx-community/Dracarys2-72B-Instruct-4bit

# custom port / host
python server.py -m ./Qwen2.5-VL-72B-Instruct-6bit --host 0.0.0.0 --port 12345
```

Default host/port: `0.0.0.0:18000`

### Most useful flags:

| Flag | Default | What it does |
|------|---------|--------------|
| `-m / --model` | `mlx-community/Qwen2-VL-2B-Instruct-4bit` | Path or HF repo. |
| `--strip-thinking` | off | Removes `<think>â€¦</think>` blocks from model output. |
| `--default-frames` | 4 | How many frames to grab per video URL. |
| `--col-a / --col-b` | qwen / vl | Tokens that trigger the special Qwen-VL vision pipeline. |

---

## ğŸ’¬ Talking to it with the CLI

```bash
python infer.py --base-url http://localhost:18000/v1 -v --max_new_tokens 2048
```

### Interactive keys
- Ctrl-N: reset conversation
- Ctrl-C: quit

---

## ğŸŒ HTTP API

POST `/v1/chat/completions`

```json
{
  "model": "Dracarys2-72B-Instruct-4bit",
  "messages": [
    { "role": "user",
      "content": [
        { "type": "text", "text": "Describe this image." },
        { "type": "image_url",
          "image_url": { "url": "data:image/jpeg;base64,..." } }
      ]
    }
  ],
  "max_tokens": 512,
  "stream": false
}
```

Response (truncated):

```json
{
  "id": "chatcmpl-d4c5â€¦",
  "object": "chat.completion",
  "created": 1715242800,
  "model": "Dracarys2-72B-Instruct-4bit",
  "choices": [
    {
      "index": 0,
      "message": { "role": "assistant", "content": "The image showsâ€¦" },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 143,
    "completion_tokens": 87,
    "total_tokens": 230,
    "tokens_per_second": 32.1
  }
}
```

Add `"stream": true` and you'll get Server-Sent Events chunks followed by
`data: [DONE]`.

---

## ğŸ› ï¸ Internals (two-sentence tour)

* **server.py** â€“ loads the model with mlx-vlm, converts incoming
OpenAI vision messages to the model's chat-template, handles images /
video frames, and streams tokens back.
* **infer.py** â€“ lightweight REPL that keeps conversation context and
shows latency / TPS stats.

That's it â€“ drop it in front of any MLX model and start chatting!

<div align="center">
  <a href="https://github.com/OEvortex/Webscout">
    <img src="https://img.shields.io/badge/WebScout-OpenAI%20Compatible%20Providers-4285F4?style=for-the-badge&logo=openai&logoColor=white" alt="WebScout OpenAI Compatible Providers">
  </a>
  <br/>
  <h1>WebScout OpenAI-Compatible Providers</h1>
  <p><strong>Seamlessly integrate with various AI providers using OpenAI-compatible interfaces</strong></p>

  <p>
    <img src="https://img.shields.io/badge/Python-3.7+-3776AB?style=flat-square&logo=python&logoColor=white" alt="Python 3.7+">
    <img src="https://img.shields.io/badge/License-MIT-green?style=flat-square" alt="License: MIT">
    <img src="https://img.shields.io/badge/PRs-Welcome-brightgreen?style=flat-square" alt="PRs Welcome">
  </p>

  <p>
    Access multiple AI providers through a standardized OpenAI-compatible interface, making it easy to switch between providers without changing your code.
  </p>
</div>

## üöÄ Overview

The WebScout OpenAI-Compatible Providers module offers a standardized way to interact with various AI providers using the familiar OpenAI API structure. This makes it easy to:

*   Use the same code structure across different AI providers
*   Switch between providers without major code changes
*   Leverage the OpenAI ecosystem of tools and libraries with alternative AI providers

## ‚öôÔ∏è Available Providers

Currently, the following providers are implemented with OpenAI-compatible interfaces:

- DeepInfra
- Glider
- ChatGPTClone
- X0GPT
- WiseCat
- Venice
- ExaAI
- TypeGPT
- SciraChat
- LLMChatCo
- FreeAIChat
- YEPCHAT
- HeckAI
- SonusAI
- ExaChat
- Netwrck
- StandardInput
- Writecream
- toolbaz
- UncovrAI
- OPKFC
- TextPollinations
- E2B
- MultiChatAI
- AI4Chat
- MCPCore
- TypefullyAI
- Flowith
- ChatSandbox
- Cloudflare
- NEMOTRON
- BLACKBOXAI
---

### <img src="https://img.shields.io/badge/DeepInfra-0A0A0A?style=flat-square&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiM1OGE2ZmYiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIj48cGF0aCBkPSJNMjAgMTFhOCA4IDAgMCAwLTE2IDAiPjwvcGF0aD48cGF0aCBkPSJtMTIgMTEgOS0xIj48L3BhdGg+PHBhdGggZD0iTTEyIDExIDMgMTAiPjwvcGF0aD48cGF0aCBkPSJNMTIgMTFWMiI+PC9wYXRoPjxwYXRoIGQ9Ik0xMiAxMXY5Ij48L3BhdGg+PC9zdmc+" alt="" height="20" style="vertical-align: middle; margin-right: 8px;"> DeepInfra

Access DeepInfra's powerful models through an OpenAI-compatible interface.

**Available Models:**

*   `deepseek-ai/DeepSeek-V3`
*   `google/gemma-2-27b-it`
*   `meta-llama/Llama-4-Maverick-17B`
*   `meta-llama/Llama-3.3-70B-Instruct`
*   `microsoft/phi-4`
*   `mistralai/Mistral-Small-24B`
*   `Qwen/QwQ-32B`

[View all models ‚Üí](https://deepinfra.com/models)

---

### <img src="https://img.shields.io/badge/Glider-5C5CFF?style=flat-square&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiNmZmZmZmYiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIj48cGF0aCBkPSJNMTQgMmMxLjgyLjgyIDMgMi41NyAzIDQuNSAwIDMtMy41IDUuNS02LjUgNS41YTQuMzUgNC4zNSAwIDAgMS0yLjUtLjc4QTYgNiAwIDAgMCAxNiAxM2MzLjMxIDAgNi0xLjggNi01LjVDMjIgMy4yIDE5LjMxIDEgMTYgMXoiPjwvcGF0aD48cGF0aCBkPSJNMiAyMi41QzIgMTkuNDYgNS41NSAxNyA5LjUgMTdzNy41IDIuNDYgNy41IDUuNVMxMy40NSAyOCA5LjUgMjggMiAyNS41NCAyIDIyLjV6Ij48L3BhdGg+PHBhdGggZD0iTTExIDE0Yy0xLjgyLS44Mi0zLTIuNTctMy00LjUgMC0zIDMuNS01LjUgNi41LTUuNWE0LjM1IDQuMzUgMCAwIDEgMi41Ljc4QTYgNiAwIDAgMCA5IDRDNS42OSA0IDMgNS44IDMgOS41YzAgMi42OSAyLjY5IDQuOSA2IDUuNXoiPjwvcGF0aD48L3N2Zz4=" alt="" height="20" style="vertical-align: middle; margin-right: 8px;"> Glider

Access Glider.so's models through an OpenAI-compatible interface.

**Available Models:**

*   `chat-llama-3-1-70b`
*   `chat-llama-3-1-8b`
*   `chat-llama-3-2-3b`
*   `deepseek-ai/DeepSeek-R1`

---

### <img src="https://img.shields.io/badge/ChatGPTClone-10A37F?style=flat-square&logo=openai&logoColor=white" alt="" height="20" style="vertical-align: middle; margin-right: 8px;"> ChatGPTClone

Access ChatGPT Clone API through an OpenAI-compatible interface.

**Available Models:**

*   `gpt-4`
*   `gpt-3.5-turbo`

---

### <img src="https://img.shields.io/badge/X0GPT-000000?style=flat-square&logo=x&logoColor=white" alt="" height="20" style="vertical-align: middle; margin-right: 8px;"> X0GPT

Access X0GPT API through an OpenAI-compatible interface.

**Available Models:**

*   `gpt-4`
*   `gpt-3.5-turbo`

---

### <img src="https://img.shields.io/badge/WiseCat-FF6B6B?style=flat-square&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiNmZmZmZmYiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIj48cGF0aCBkPSJNMTIgMjJjNS41MjMgMCAxMC00LjQ3NyAxMC0xMFMxNy41MjMgMiAxMiAyIDIgNi40NzcgMiAxMnM0LjQ3NyAxMCAxMCAxMHoiPjwvcGF0aD48cGF0aCBkPSJNOCA5aDJhMiAyIDAgMCAwIDIgMnYyYzAgMS4xLjkgMiAyIDJoMiI+PC9wYXRoPjxwYXRoIGQ9Ik0xMCAxNGgtMmEyIDIgMCAwIDEtMi0ydi0yYzAtMS4xLS45LTItMi0ySDIiPjwvcGF0aD48L3N2Zz4=" alt="" height="20" style="vertical-align: middle; margin-right: 8px;"> WiseCat

Access WiseCat API through an OpenAI-compatible interface.

**Available Models:**

*   `chat-model-small`
*   `chat-model-large`
*   `chat-model-reasoning`

---

### <img src="https://img.shields.io/badge/Venice-3498DB?style=flat-square&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiNmZmZmZmYiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIj48cGF0aCBkPSJNMTIgMTlsNy03IDMgMyAtNyA3LTMtM3oiPjwvcGF0aD48cGF0aCBkPSJNMTggMTNsLTEuNS03LjVMMiAybDMuNSAxNC41TDEzIDE4bDUtNXoiPjwvcGF0aD48cGF0aCBkPSJNMiAybDcuNTg2IDcuNTg2Ij48L3BhdGggZD0iTTExIDExbDUgNSI+PC9wYXRoPjwvc3ZnPg==" alt="" height="20" style="vertical-align: middle; margin-right: 8px;"> Venice

Access Venice AI API through an OpenAI-compatible interface.

**Available Models:**

*   `mistral-31-24b`
*   `llama-3.2-3b-akash`
*   `qwen2dot5-coder-32b`
*   `deepseek-coder-v2-lite`

---

### <img src="https://img.shields.io/badge/ExaAI-6236FF?style=flat-square&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiNmZmZmZmYiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIj48cGF0aCBkPSJNMTIgMmExMCAxMCAwIDEgMCAwIDIwIDEwIDEwIDAgMCAwIDAtMjB6Ij48L3BhdGg+PHBhdGggZD0iTTEyIDhhNCA0IDAgMSAwIDAgOCA0IDQgMCAwIDAgMC04eiI+PC9wYXRoPjxwYXRoIGQ9Ik0xMiAydjQiPjwvcGF0aCBkPSJNMTIgMTh2NCI+PC9wYXRoPjxwYXRoIGQ9Ik00LjkzIDQuOTNsMyAzIj48L3BhdGggZD0iTTE2LjA3IDE2LjA3bDMgMyI+PC9wYXRoPjxwYXRoIGQ9Ik0yIDEyaDQiPjwvcGF0aD48cGF0aCBkPSJNMTggMTJoNCI+PC9wYXRoPjxwYXRoIGQ9Ik00LjkzIDE5LjA3bDMtMyI+PC9wYXRoPjxwYXRoIGQ9Ik0xNi4wNyA3LjkzbDMtMyI+PC9wYXRoPjwvc3ZnPg==" alt="" height="20" style="vertical-align: middle; margin-right: 8px;"> ExaAI

Access ExaAI's O3-Mini model through an OpenAI-compatible interface.

**Available Models:**

*   `O3-Mini`: ExaAI's O3-Mini model

> **Important Note:** ExaAI does not support system messages. Any system messages will be automatically removed from the conversation.

---

### <img src="https://img.shields.io/badge/TypeGPT-4B32C3?style=flat-square&logo=typescript&logoColor=white" alt="" height="20" style="vertical-align: middle; margin-right: 8px;"> TypeGPT

Access TypeGPT.net's models through an OpenAI-compatible interface.

**Available Models:**

*   `gpt-4o-mini-2024-07-18`: OpenAI's GPT-4o mini model
*   `chatgpt-4o-latest`: Latest version of ChatGPT with GPT-4o
*   `deepseek-r1`: DeepSeek's R1 model
*   `deepseek-v3`: DeepSeek's V3 model
*   `uncensored-r1`: Uncensored version of DeepSeek R1
*   `Image-Generator`: For generating images

---

### <img src="https://img.shields.io/badge/SciraChat-FF5700?style=flat-square&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiNmZmZmZmYiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIj48cGF0aCBkPSJNMjEgMTVhMiAyIDAgMCAxLTIgMmgtOWE2IDYgMCAwIDEtNi02VjhoMTBhMiAyIDAgMCAxIDIgMnYyaDRhMiAyIDAgMCAxIDIgMnoiPjwvcGF0aD48cGF0aCBkPSJNMTQgMTFhMiAyIDAgMCAxLTIgMkg0YTIgMiAwIDAgMS0yLTJWN2EyIDIgMCAwIDEgMi0yaDEwYTIgMiAwIDAgMSAyIDJ6Ij48L3BhdGg+PC9zdmc+" alt="" height="20" style="vertical-align: middle; margin-right: 8px;"> SciraChat

Access Scira.ai's models through an OpenAI-compatible interface.

**Available Models:**

*   `scira-default`: Grok3 model
*   `scira-grok-3-mini`: Grok3-mini (thinking model)
*   `scira-vision`: Grok2-Vision (vision model)
*   `scira-claude`: Sonnet-3.7 model
*   `scira-optimus`: Optimus model

---

### <img src="https://img.shields.io/badge/LLMChatCo-4A90E2?style=flat-square&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiNmZmZmZmYiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIj48cGF0aCBkPSJNMTIgMmExMCAxMCAwIDEgMCAwIDIwIDEwIDEwIDAgMCAwIDAtMjB6Ij48L3BhdGg+PHBhdGggZD0iTTggMTRzMS41IDIgNCAxLjVjMi41LS41IDQtMS41IDQtMS41Ij48L3BhdGg+PHBhdGggZD0iTTkgOWguMDEiPjwvcGF0aD48cGF0aCBkPSJNMTUgOWguMDEiPjwvcGF0aD48L3N2Zz4=" alt="" height="20" style="vertical-align: middle; margin-right: 8px;"> LLMChatCo

Access LLMChat.co's models through an OpenAI-compatible interface.

**Available Models:**

*   `gemini-flash-2.0`: Google's Gemini Flash 2.0 model (default)
*   `llama-4-scout`: Meta's Llama 4 Scout model
*   `gpt-4o-mini`: OpenAI's GPT-4o mini model

---

### <img src="https://img.shields.io/badge/FreeAIChat-00C7B7?style=flat-square&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiNmZmZmZmYiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIj48cGF0aCBkPSJNMjEgMTVhMiAyIDAgMCAxLTIgMkgzYTIgMiAwIDAgMS0yLTJWN2EyIDIgMCAwIDEgMi0yaDEwYTIgMiAwIDAgMSAyIDJ2M2g0YTIgMiAwIDAgMSAyIDJ6Ij48L3BhdGg+PC9zdmc+" alt="" height="20" style="vertical-align: middle; margin-right: 8px;"> FreeAIChat

Access FreeAIChat's wide range of models through an OpenAI-compatible interface.

**Available Models:**

**<img src="https://img.shields.io/badge/OpenAI-412991?style=flat-square&logo=openai&logoColor=white" alt="" height="16" style="vertical-align: middle; margin-right: 5px;"> OpenAI Models**
*   `GPT 4o`
*   `GPT 4.5 Preview`
*   `GPT 4o Latest`
*   `O1`
*   `O3 Mini`

**<img src="https://img.shields.io/badge/Anthropic-0000FF?style=flat-square&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiNmZmZmZmYiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIj48cGF0aCBkPSJNMTIgMmExMCAxMCAwIDEgMCAwIDIwIDEwIDEwIDAgMCAwIDAtMjB6Ij48L3BhdGg+PHBhdGggZD0iTTEyIDhhNCA0IDAgMSAwIDAgOCA0IDQgMCAwIDAgMC04eiI+PC9wYXRoPjwvc3ZnPg==" alt="" height="16" style="vertical-align: middle; margin-right: 5px;"> Anthropic Models**
*   `Claude 3.5 haiku`
*   `Claude 3.5 sonnet`
*   `Claude 3.7 Sonnet`

**<img src="https://img.shields.io/badge/Google-4285F4?style=flat-square&logo=google&logoColor=white" alt="" height="16" style="vertical-align: middle; margin-right: 5px;"> Google Models**
*   `Gemini 1.5 Flash`
*   `Gemini 1.5 Pro`
*   `Gemini 2.0 Pro`
*   `Gemini 2.5 Pro`

**<img src="https://img.shields.io/badge/Llama-FF6B6B?style=flat-square&logo=meta&logoColor=white" alt="" height="16" style="vertical-align: middle; margin-right: 5px;"> Llama Models**
*   `Llama 3.1 405B`
*   `Llama 3.3 70B`
*   `Llama 4 Scout`

**<img src="https://img.shields.io/badge/Mistral-7952B3?style=flat-square&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiNmZmZmZmYiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIj48cGF0aCBkPSJNMTIgMTlsNy03IDMgMyAtNyA3LTMtM3oiPjwvcGF0aD48cGF0aCBkPSJNMTggMTNsLTEuNS03LjVMMiAybDMuNSAxNC41TDEzIDE4bDUtNXoiPjwvcGF0aD48L3N2Zz4=" alt="" height="16" style="vertical-align: middle; margin-right: 5px;"> Mistral Models**
*   `Mistral Large`
*   `Mistral Nemo`
*   `Mixtral 8x22B`

**<img src="https://img.shields.io/badge/Other-34D399?style=flat-square&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiNmZmZmZmYiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIj48cGF0aCBkPSJNMTIgMmE5IDkgMCAwIDEgOSA5IDkgOSAwIDAgMS05IDkgOSA5IDAgMCAxLTkgOSA5IDkgMCAwIDEtOS05eiI+PC9wYXRoPjxwYXRoIGQ9Ik0xMiAyYTkgOSAwIDAgMC05IDkgOSA5IDAgMCAwIDkgOSA5IDkgMCAwIDAgOS05IDkgOSAwIDAgMC05LTl6Ij48L3BhdGg+PHBhdGggZD0iTTEyIDJhOSA5IDAgMCAxIDAgMTggOSA5IDAgMCAxIDAtMTh6Ij48L3BhdGg+PC9zdmc+" alt="" height="16" style="vertical-align: middle; margin-right: 5px;"> Other Models**
*   `Deepseek R1`
*   `Qwen Max`
*   `Grok 3`

---

### <img src="https://img.shields.io/badge/YEPCHAT-FFD700?style=flat-square&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiMwMDAwMDAiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIj48cGF0aCBkPSJNMTIgMmExMCAxMCAwIDEgMCAwIDIwIDEwIDEwIDAgMCAwIDAtMjB6Ij48L3BhdGg+PHBhdGggZD0iTTkgMTZhMyAzIDAgMCAwIDYgMCI+PC9wYXRoPjxwYXRoIGQ9Ik05IDloLjAxIj48L3BhdGg+PHBhdGggZD0iTTE1IDloLjAxIj48L3BhdGg+PC9zdmc+" alt="" height="20" style="vertical-align: middle; margin-right: 8px;"> YEPCHAT

Access Yep.com's models through an OpenAI-compatible interface.

**Available Models:**

*   `DeepSeek-R1-Distill-Qwen-32B`
*   `Mixtral-8x7B-Instruct-v0.1`

---

### <img src="https://img.shields.io/badge/HeckAI-5D3FD3?style=flat-square&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiNmZmZmZmYiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIj48cGF0aCBkPSJNMTIgMmExMCAxMCAwIDEgMCAwIDIwIDEwIDEwIDAgMCAwIDAtMjB6Ij48L3BhdGg+PHBhdGggZD0iTTggMTRzMS41IDIgNCAxLjVjMi41LS41IDQtMS41IDQtMS41Ij48L3BhdGg+PHBhdGggZD0iTTkgOWguMDEiPjwvcGF0aD48cGF0aCBkPSJNMTUgOWguMDEiPjwvcGF0aD48L3N2Zz4=" alt="" height="20" style="vertical-align: middle; margin-right: 8px;"> HeckAI

Access HeckAI's models through an OpenAI-compatible interface.

**Available Models:**

*   `deepseek/deepseek-chat`
*   `openai/gpt-4o-mini`
*   `deepseek/deepseek-r1`
*   `google/gemini-2.0-flash-001`

---

### <img src="https://img.shields.io/badge/SonusAI-00BFFF?style=flat-square&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiNmZmZmZmYiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIj48cGF0aCBkPSJNMTIgMmE5IDkgMCAwIDEgOSA5LjUgOSA5IDAgMCAxLTkgOS41IDkgOSAwIDAgMS05LTkuNUE5IDkgMCAwIDEgMTIgMnoiPjwvcGF0aD48cGF0aCBkPSJNOCAxNGEzIDMgMCAwIDAgNiAwIj48L3BhdGg+PHBhdGggZD0iTTkgOWguMDEiPjwvcGF0aD48cGF0aCBkPSJNMTUgOWguMDEiPjwvcGF0aD48L3N2Zz4=" alt="" height="20" style="vertical-align: middle; margin-right: 8px;"> SonusAI

Access SonusAI's models through an OpenAI-compatible interface.

**Available Models:**

*   `pro` - SonusAI's premium model
*   `air` - SonusAI's balanced model
*   `mini` - SonusAI's lightweight model

---

### <img src="https://img.shields.io/badge/ExaChat-4B0082?style=flat-square&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiNmZmZmZmYiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIj48cGF0aCBkPSJNMTIgMmExMCAxMCAwIDEgMCAwIDIwIDEwIDEwIDAgMCAwIDAtMjB6Ij48L3BhdGg+PHBhdGggZD0iTTggMTRzMS41IDIgNCAxLjVjMi41LS41IDQtMS41IDQtMS41Ij48L3BhdGg+PHBhdGggZD0iTTkgOWguMDEiPjwvcGF0aD48cGF0aCBkPSJNMTUgOWguMDEiPjwvcGF0aD48L3N2Zz4=" alt="" height="20" style="vertical-align: middle; margin-right: 8px;"> ExaChat

Access ExaChat's multi-provider models through an OpenAI-compatible interface.

**Available Models:**

*   ExaAnswer: `exaanswer`
*   Gemini: `gemini-2.0-flash`, `gemini-2.5-pro-exp-03-25`, and more
*   OpenRouter: `deepseek/deepseek-r1:free`, `meta-llama/llama-4-maverick:free`, and more
*   Groq: `llama-3.1-8b-instant`, `qwen-2.5-32b`, and more
*   Cerebras: `llama3.1-8b`, `llama-3.3-70b`

---

### <img src="https://img.shields.io/badge/Netwrck-3498DB?style=flat-square&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiNmZmZmZmYiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIj48cGF0aCBkPSJNMTIgMmExMCAxMCAwIDEgMCAwIDIwIDEwIDEwIDAgMCAwIDAtMjB6Ij48L3BhdGg+PHBhdGggZD0iTTggMTRzMS41IDIgNCAxLjVjMi41LS41IDQtMS41IDQtMS41Ij48L3BhdGg+PHBhdGggZD0iTTkgOWguMDEiPjwvcGF0aD48cGF0aCBkPSJNMTUgOWguMDEiPjwvcGF0aD48L3N2Zz4=" alt="" height="20" style="vertical-align: middle; margin-right: 8px;"> Netwrck

Access Netwrck's models through an OpenAI-compatible interface.

**Available Models:**

*   `anthropic/claude-3-7-sonnet-20250219`
*   `openai/gpt-4o-mini`
*   `deepseek/deepseek-r1`
*   `deepseek/deepseek-chat`
*   `x-ai/grok-2`
*   `google/gemini-pro-1.5`
*   And more

---

### <img src="https://img.shields.io/badge/StandardInput-4A90E2?style=flat-square&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiNmZmZmZmYiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIj48cGF0aCBkPSJNMjEgMTVhMiAyIDAgMCAxLTIgMkgzYTIgMiAwIDAgMS0yLTJWN2EyIDIgMCAwIDEgMi0yaDEwYTIgMiAwIDAgMSAyIDJ2M2g0YTIgMiAwIDAgMSAyIDJ6Ij48L3BhdGg+PC9zdmc+" alt="" height="20" style="vertical-align: middle; margin-right: 8px;"> StandardInput

Access Standard Input's chat models through an OpenAI-compatible interface.

**Available Models:**

*   `standard-quick`: Standard Input's quick response model
*   `standard-reasoning`: Standard Input's model with reasoning capabilities

---

### <img src="https://img.shields.io/badge/E2B-FFA500?style=flat-square&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiMwMDAwMDAiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIj48cGF0aCBkPSJNMTIgMmExMCAxMCAwIDEgMCAwIDIwIDEwIDEwIDAgMCAwIDAtMjB6Ij48L3BhdGg+PHBhdGggZD0iTTggMTJoOCI+PC9wYXRoPjxwYXRoIGQ9Ik0xMiA4djgiPjwvcGF0aD48L3N2Zz4=" alt="" height="20" style="vertical-align: middle; margin-right: 8px;"> E2B

Access various models via the E2B Fragments API (fragments.e2b.dev) through an OpenAI-compatible interface. Uses `cloudscraper` to handle potential Cloudflare protection.

**Available Models:**

*   `claude-3.7-sonnet`
*   `claude-3.5-sonnet`
*   `claude-3.5-haiku`
*   `o1-mini`, `o3-mini`, `o1`, `o3`
*   `gpt-4.5-preview`, `gpt-4o`
*   `gpt-4.1`, `gpt-4.1-mini`, `gpt-4.1-nano`
*   `gemini-1.5-pro-002`
*   `gemini-2.5-pro-exp-03-25`
*   `gemini-2.0-flash`, `gemini-2.0-flash-lite`, `gemini-2.0-flash-thinking-exp-01-21`
*   `qwen-qwq-32b-preview`
*   `grok-beta`
*   `deepseek-chat`
*   `codestral-2501`
*   `mistral-large-latest`
*   `llama4-maverick-instruct-basic`, `llama4-scout-instruct-basic`
*   `llama-v3p1-405b-instruct`

> **Note:** The underlying API does not support true streaming. `stream=True` simulates streaming by returning the full response.

---

## üíª Usage Examples

Here are examples of how to use the OpenAI-compatible providers in your code.

### Basic Usage with DeepInfra

```python
from webscout.Provider.OPENAI import DeepInfra

# Initialize the client
client = DeepInfra()

# Create a completion (non-streaming)
response = client.chat.completions.create(
    model="meta-llama/Meta-Llama-3.1-8B-Instruct",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Tell me about Python programming."}
    ],
    temperature=0.7,
    max_tokens=500
)

# Print the response
print(response.choices[0].message.content)
```

### Basic Usage with Glider

```python
from webscout.Provider.OPENAI import Glider

# Initialize the client
client = Glider()

# Create a completion (non-streaming)
response = client.chat.completions.create(
    model="chat-llama-3-1-70b",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Tell me about Python programming."}
    ],
    max_tokens=500
)

# Print the response
print(response.choices[0].message.content)
```

### Streaming Responses (Example with DeepInfra)

```python
from webscout.Provider.OPENAI import DeepInfra

# Initialize the client
client = DeepInfra()

# Create a streaming completion
stream = client.chat.completions.create(
    model="meta-llama/Meta-Llama-3.1-8B-Instruct",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Write a short poem about programming."}
    ],
    stream=True,
    temperature=0.7
)

# Process the streaming response
for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
print()  # Add a newline at the end
```

### Streaming with Glider

```python
from webscout.Provider.OPENAI import Glider

# Initialize the client
client = Glider()

# Create a streaming completion
stream = client.chat.completions.create(
    model="chat-llama-3-1-70b",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Write a short poem about programming."}
    ],
    stream=True
)

# Process the streaming response
for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
print()  # Add a newline at the end
```

### Basic Usage with ChatGPTClone

```python
from webscout.Provider.OPENAI import ChatGPTClone

# Initialize the client
client = ChatGPTClone()

# Create a completion (non-streaming)
response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Tell me about Python programming."}
    ],
    temperature=0.7
)

# Print the response
print(response.choices[0].message.content)
```

### Streaming with ChatGPTClone

```python
from webscout.Provider.OPENAI import ChatGPTClone

# Initialize the client
client = ChatGPTClone()

# Create a streaming completion
stream = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Write a short poem about programming."}
    ],
    stream=True
)

# Process the streaming response
for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
print()  # Add a newline at the end
```

### Basic Usage with X0GPT

```python
from webscout.Provider.OPENAI import X0GPT

# Initialize the client
client = X0GPT()

# Create a completion (non-streaming)
response = client.chat.completions.create(
    model="gpt-4",  # Model name doesn't matter for X0GPT
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Tell me about Python programming."}
    ]
)

# Print the response
print(response.choices[0].message.content)
```

### Streaming with X0GPT

```python
from webscout.Provider.OPENAI import X0GPT

# Initialize the client
client = X0GPT()

# Create a streaming completion
stream = client.chat.completions.create(
    model="gpt-4",  # Model name doesn't matter for X0GPT
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Write a short poem about programming."}
    ],
    stream=True
)

# Process the streaming response
for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
print()  # Add a newline at the end
```

### Basic Usage with WiseCat

```python
from webscout.Provider.OPENAI import WiseCat

# Initialize the client
client = WiseCat()

# Create a completion (non-streaming)
response = client.chat.completions.create(
    model="chat-model-small",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Tell me about Python programming."}
    ]
)

# Print the response
print(response.choices[0].message.content)
```

### Streaming with WiseCat

```python
from webscout.Provider.OPENAI import WiseCat

# Initialize the client
client = WiseCat()

# Create a streaming completion
stream = client.chat.completions.create(
    model="chat-model-small",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Write a short poem about programming."}
    ],
    stream=True
)

# Process the streaming response
for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
print()  # Add a newline at the end
```

### Basic Usage with Venice

```python
from webscout.Provider.OPENAI import Venice

# Initialize the client
client = Venice(temperature=0.7, top_p=0.9)

# Create a completion (non-streaming)
response = client.chat.completions.create(
    model="mistral-31-24b",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Tell me about Python programming."}
    ]
)

# Print the response
print(response.choices[0].message.content)
```

### Streaming with Venice

```python
from webscout.Provider.OPENAI import Venice

# Initialize the client
client = Venice()

# Create a streaming completion
stream = client.chat.completions.create(
    model="mistral-31-24b",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Write a short poem about programming."}
    ],
    stream=True
)

# Process the streaming response
for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
print()  # Add a newline at the end
```

### Basic Usage with ExaAI

```python
from webscout.Provider.OPENAI import ExaAI

# Initialize the client
client = ExaAI()

# Create a completion (non-streaming)
response = client.chat.completions.create(
    model="O3-Mini",
    messages=[
        # Note: ExaAI does not support system messages (they will be removed)
        {"role": "user", "content": "Hello!"},
        {"role": "assistant", "content": "Hi there! How can I help you today?"},
        {"role": "user", "content": "Tell me about Python programming."}
    ]
)

# Print the response
print(response.choices[0].message.content)
```

### Basic Usage with HeckAI

```python
from webscout.Provider.OPENAI import HeckAI

# Initialize the client
client = HeckAI(language="English")

# Create a completion (non-streaming)
response = client.chat.completions.create(
    model="google/gemini-2.0-flash-001",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Tell me about Python programming."}
    ]
)

# Print the response
print(response.choices[0].message.content)
```

### Streaming with HeckAI

```python
from webscout.Provider.OPENAI import HeckAI

# Initialize the client
client = HeckAI()

# Create a streaming completion
stream = client.chat.completions.create(
    model="google/gemini-2.0-flash-001",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Write a short poem about programming."}
    ],
    stream=True
)

# Process the streaming response
for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
print()  # Add a newline at the end
```

### Streaming with ExaAI

```python
from webscout.Provider.OPENAI import ExaAI

# Initialize the client
client = ExaAI()

# Create a streaming completion
stream = client.chat.completions.create(
    model="O3-Mini",
    messages=[
        # Note: ExaAI does not support system messages (they will be removed)
        {"role": "user", "content": "Hello!"},
        {"role": "assistant", "content": "Hi there! How can I help you today?"},
        {"role": "user", "content": "Write a short poem about programming."}
    ],
    stream=True
)

# Process the streaming response
for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
print()  # Add a newline at the end
```

### Basic Usage with TypeGPT

```python
from webscout.Provider.OPENAI import TypeGPT

# Initialize the client
client = TypeGPT()

# Create a completion (non-streaming)
response = client.chat.completions.create(
    model="chatgpt-4o-latest",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Write a short poem about programming."}
    ]
)

# Print the response
print(response.choices[0].message.content)
```

### Streaming with TypeGPT

```python
from webscout.Provider.OPENAI import TypeGPT

# Initialize the client
client = TypeGPT()

# Create a streaming completion
stream = client.chat.completions.create(
    model="chatgpt-4o-latest",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Write a short poem about programming."}
    ],
    stream=True
)

# Process the streaming response
for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
print()  # Add a newline at the end
```

### Basic Usage with SciraChat

```python
from webscout.Provider.OPENAI import SciraChat

# Initialize the client
client = SciraChat()

# Create a completion (non-streaming)
response = client.chat.completions.create(
    model="scira-default",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Tell me about Python programming."}
    ]
)

# Print the response
print(response.choices[0].message.content)
```

### Streaming with SciraChat

```python
from webscout.Provider.OPENAI import SciraChat

# Initialize the client
client = SciraChat()

# Create a streaming completion
stream = client.chat.completions.create(
    model="scira-default",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Write a short poem about programming."}
    ],
    stream=True
)

# Process the streaming response
for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
print()  # Add a newline at the end
```

### Basic Usage with FreeAIChat

```python
from webscout.Provider.OPENAI import FreeAIChat

# Initialize the client
client = FreeAIChat()

# Create a completion (non-streaming)
response = client.chat.completions.create(
    model="GPT 4o",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Tell me about Python programming."}
    ]
)

# Print the response
print(response.choices[0].message.content)
```

### Streaming with FreeAIChat

```python
from webscout.Provider.OPENAI import FreeAIChat

# Initialize the client
client = FreeAIChat()

# Create a streaming completion
stream = client.chat.completions.create(
    model="GPT 4o",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Write a short poem about programming."}
    ],
    stream=True
)

# Process the streaming response
for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
print()  # Add a newline at the end
```

### Basic Usage with LLMChatCo

```python
from webscout.Provider.OPENAI import LLMChatCo

# Initialize the client
client = LLMChatCo()

# Create a completion (non-streaming)
response = client.chat.completions.create(
    model="gemini-flash-2.0",  # Default model
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Tell me about Python programming."}
    ],
    temperature=0.7
)

# Print the response
print(response.choices[0].message.content)
```

### Streaming with LLMChatCo

```python
from webscout.Provider.OPENAI import LLMChatCo

# Initialize the client
client = LLMChatCo()

# Create a streaming completion
stream = client.chat.completions.create(
    model="gemini-flash-2.0",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Write a short poem about programming."}
    ],
    stream=True
)

# Process the streaming response
for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
print()  # Add a newline at the end
```

### Basic Usage with YEPCHAT

```python
from webscout.Provider.OPENAI import YEPCHAT

# Initialize the client
client = YEPCHAT()

# Create a completion (non-streaming)
response = client.chat.completions.create(
    model="DeepSeek-R1-Distill-Qwen-32B",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Tell me about Python programming."}
    ],
    temperature=0.7
)

# Print the response
print(response.choices[0].message.content)
```

### Basic Usage with SonusAI

```python
from webscout.Provider.OPENAI import SonusAI

# Initialize the client
client = SonusAI()

# Create a completion (non-streaming)
response = client.chat.completions.create(
    model="pro",  # Choose from 'pro', 'air', or 'mini'
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Tell me about Python programming."}
    ],
    reasoning=True  # Optional: Enable reasoning mode
)

# Print the response
print(response.choices[0].message.content)
```

### Streaming with YEPCHAT

```python
from webscout.Provider.OPENAI import YEPCHAT

# Initialize the client
client = YEPCHAT()

# Create a streaming completion
stream = client.chat.completions.create(
    model="Mixtral-8x7B-Instruct-v0.1",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Write a short poem about programming."}
    ],
    stream=True
)

# Process the streaming response
for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
print()  # Add a newline at the end
```

### Streaming with SonusAI

```python
from webscout.Provider.OPENAI import SonusAI

# Initialize the client
client = SonusAI(timeout=60)

# Create a streaming completion
stream = client.chat.completions.create(
    model="air",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Write a short poem about programming."}
    ],
    stream=True
)

# Process the streaming response
for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
print()  # Add a newline at the end
```

### Basic Usage with ExaChat

```python
from webscout.Provider.OPENAI import ExaChat

# Initialize the client
client = ExaChat()

# Create a completion (non-streaming)
response = client.chat.completions.create(
    model="exaanswer",  # Choose from many available models
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Tell me about Python programming."}
    ]
)

# Print the response
print(response.choices[0].message.content)
```

### Using Different ExaChat Providers

```python
from webscout.Provider.OPENAI import ExaChat

# Initialize the client
client = ExaChat(timeout=60)

# Use a Gemini model
gemini_response = client.chat.completions.create(
    model="gemini-2.0-flash",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Explain quantum computing in simple terms."}
    ]
)

# Use a Groq model
groq_response = client.chat.completions.create(
    model="llama-3.1-8b-instant",
    messages=[
        {"role": "user", "content": "Tell me about Python programming."}
    ]
)

# Print the response
print(response.choices[0].message.content)
```

### Streaming with Netwrck

```python
from webscout.Provider.OPENAI import Netwrck

# Initialize the client
client = Netwrck(timeout=60)

# Create a streaming completion
stream = client.chat.completions.create(
    model="openai/gpt-4o-mini",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Write a short poem about programming."}
    ],
    stream=True
)

# Process the streaming response
for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
print()  # Add a newline at the end
```

### Basic Usage with StandardInput

```python
from webscout.Provider.OPENAI import StandardInput

# Initialize the client
client = StandardInput()

# Create a completion (non-streaming)
response = client.chat.completions.create(
    model="standard-quick",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Tell me about Python programming."}
    ]
)

# Print the response
print(response.choices[0].message.content)
```

### Streaming with StandardInput

```python
from webscout.Provider.OPENAI import StandardInput

# Initialize the client
client = StandardInput()

# Create a streaming completion
stream = client.chat.completions.create(
    model="standard-reasoning",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Count from 1 to 5."}
    ],
    stream=True,
    enable_reasoning=True  # Enable reasoning capabilities
)

# Process the streaming response
for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
print()  # Add a newline at the end
```

## üîÑ Response Format

All providers return responses that mimic the OpenAI API structure, ensuring compatibility with tools built for OpenAI.

### üìù Non-streaming Response

```json
{
  "id": "chatcmpl-123abc",
  "object": "chat.completion",
  "created": 1677858242,
  "model": "meta-llama/Meta-Llama-3.1-8B-Instruct",
  "usage": {
    "prompt_tokens": 13,
    "completion_tokens": 7,
    "total_tokens": 20
  },
  "choices": [
    {
      "message": {
        "role": "assistant",
        "content": "This is a response from the model."
      },
      "finish_reason": "stop",
      "index": 0
    }
  ]
}
```

### üì± Streaming Response Chunks

```json
{
  "id": "chatcmpl-123abc",
  "object": "chat.completion.chunk",
  "created": 1677858242,
  "model": "meta-llama/Meta-Llama-3.1-8B-Instruct",
  "choices": [
    {
      "delta": {
        "content": "This "
      },
      "finish_reason": null,
      "index": 0
    }
  ]
}
```

## üß© Architecture

The OpenAI-compatible providers are built on a modular architecture:

*   `base.py`: Contains abstract base classes that define the OpenAI-compatible interface
*   `utils.py`: Provides data structures that mimic OpenAI's response format
*   Provider-specific implementations (e.g., `deepinfra.py`): Implement the abstract interfaces for specific providers

This architecture makes it easy to add new providers while maintaining a consistent interface.

## üìù Notes

*   Some providers may require API keys for full functionality
*   Not all OpenAI features are supported by all providers
*   Response formats are standardized to match OpenAI's format, but the underlying content depends on the specific provider and model

## ü§ù Contributing

Want to add a new OpenAI-compatible provider? Follow these steps:

1.  Create a new file in the `webscout/Provider/OPENAI` directory
2.  Implement the `OpenAICompatibleProvider` interface
3.  Add appropriate tests
4.  Update this README with information about the new provider

## üìö Related Documentation

*   [OpenAI API Reference](https://platform.openai.com/docs/api-reference)
*   [DeepInfra Documentation](https://deepinfra.com/docs)
*   [Glider.so Website](https://glider.so/)
*   [ChatGPT Clone Website](https://chatgpt-clone-ten-nu.vercel.app/)
*   [X0GPT Website](https://x0-gpt.devwtf.in/)
*   [WiseCat Website](https://wise-cat-groq.vercel.app/)
*   [Venice AI Website](https://venice.ai/)
*   [ExaAI Website](https://o3minichat.exa.ai/)
*   [TypeGPT Website](https://chat.typegpt.net/)
*   [SciraChat Website](https://scira.ai/)
*   [FreeAIChat Website](https://freeaichatplayground.com/)
*   [LLMChatCo Website](https://llmchat.co/)
*   [Yep.com Website](https://yep.com/)
*   [HeckAI Website](https://heck.ai/)
*   [SonusAI Website](https://chat.sonus.ai/)
*   [ExaChat Website](https://exa-chat.vercel.app/)
*   [Netwrck Website](https://netwrck.com/)
*   [StandardInput Website](https://chat.standard-input.com/)

<div align="center">
  <a href="https://t.me/PyscoutAI"><img alt="Telegram Group" src="https://img.shields.io/badge/Telegram%20Group-2CA5E0?style=for-the-badge&logo=telegram&logoColor=white"></a>
  <a href="https://buymeacoffee.com/oevortex"><img alt="Buy Me A Coffee" src="https://img.shields.io/badge/Buy%20Me%20A%20Coffee-FFDD00?style=for-the-badge&logo=buymeacoffee&logoColor=black"></a>
</div>

## Flowith OpenAI-Compatible Provider

This provider allows you to use the Flowith API with an OpenAI-compatible interface. It supports the following models:

- gpt-4.1-mini
- deepseek-chat
- deepseek-reasoner
- claude-3.5-haiku
- gemini-2.0-flash
- gemini-2.5-flash
- grok-3-mini

### Usage Example

```python
from Provider.OPENAI.flowith import Flowith

client = Flowith()
response = client.chat.completions.create(
    model="gpt-4.1-mini",
    messages=[{"role": "user", "content": "Hello!"}]
)
print(response.choices[0].message.content)
```

- `AVAILABLE_MODELS` and `models()` are provided for model discovery.
- The provider is compatible with the OpenAI API interface used in this project.

See the source code for more details and advanced usage.

base_model: google/gemma-3-27b-it
# model_type: AutoModelForImageTextToText
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer
trust_remote_code: true
strict: false

# gemma3 doesn't seem to play nice with ddp
ddp_find_unused_parameters: false
chat_template: gemma3

# Automatically upload checkpoint and final model to HF
# hub_model_id: username/custom_model_name

load_in_8bit: false
load_in_4bit: false

datasets:
  - path: "train.jsonl"
    type: chat_template
    field_messages: messages
    message_property_mappings:
      role: role
      content: content

eval_datasets:
  - path: "test.jsonl"
    name: "test (i.d.)"
    type: chat_template
    field_messages: messages
    message_property_mappings:
      role: role
      content: content
    
  - path: "test_ood.jsonl"
    name: "test (ood)"
    type: chat_template
    field_messages: messages
    message_property_mappings:
      role: role
      content: content
    
# How often to run evaluation during training
eval_steps: 100  # Run evaluation every 100 steps
# Maximum number of samples to evaluate (to limit evaluation time)
val_set_size: 500  # Optional, limits evaluation to first 1000 samples

# Whether to calculate additional metrics beyond just loss
do_eval: true
evaluation_strategy: "steps"  # Can be "steps" or "epoch"


dataset_prepared_path: last_run_prepared
output_dir: ./outputs/out

sequence_len: 4096
# sample_packing: true
pad_to_sequence_len: true

wandb_project: axolotl-test
wandb_entity: center-on-long-term-risk
# wandb_watch:
# wandb_name:
# wandb_log_model:

gradient_accumulation_steps: 8
micro_batch_size: 1
num_epochs: 1
optimizer: paged_adamw_8bit
lr_scheduler: cosine
learning_rate: 2e-5

train_on_inputs: false
group_by_length: false
bf16: auto
fp16:
tf32: false

gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
early_stopping_patience:
resume_from_checkpoint:
logging_steps: 1
xformers_attention:
flash_attention: true

warmup_steps: 10
eval_table_size:
saves_per_epoch: 1
debug:
deepspeed: zero3_bf16.json
weight_decay: 0.0

hub_private_repo: true
hub_strategy: every_save
hf_use_auth_token: true
// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: xla/backends/gpu/runtime/thunk.proto

#ifndef GOOGLE_PROTOBUF_INCLUDED_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto
#define GOOGLE_PROTOBUF_INCLUDED_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto

#include <limits>
#include <string>

#include <google/protobuf/port_def.inc>
#if PROTOBUF_VERSION < 3021000
#error This file was generated by a newer version of protoc which is
#error incompatible with your Protocol Buffer headers. Please update
#error your headers.
#endif
#if 3021009 < PROTOBUF_MIN_PROTOC_VERSION
#error This file was generated by an older version of protoc which is
#error incompatible with your Protocol Buffer headers. Please
#error regenerate this file with a newer version of protoc.
#endif

#include <google/protobuf/port_undef.inc>
#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/arena.h>
#include <google/protobuf/arenastring.h>
#include <google/protobuf/generated_message_util.h>
#include <google/protobuf/metadata_lite.h>
#include <google/protobuf/generated_message_reflection.h>
#include <google/protobuf/message.h>
#include <google/protobuf/repeated_field.h>  // IWYU pragma: export
#include <google/protobuf/extension_set.h>  // IWYU pragma: export
#include <google/protobuf/unknown_field_set.h>
#include "xla/service/buffer_assignment.pb.h"
// @@protoc_insertion_point(includes)
#include <google/protobuf/port_def.inc>
#define PROTOBUF_INTERNAL_EXPORT_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto PROTOBUF_EXPORT
PROTOBUF_NAMESPACE_OPEN
namespace internal {
class AnyMetadata;
}  // namespace internal
PROTOBUF_NAMESPACE_CLOSE

// Internal implementation detail -- do not use these members.
struct PROTOBUF_EXPORT TableStruct_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto {
  static const uint32_t offsets[];
};
PROTOBUF_EXPORT extern const ::PROTOBUF_NAMESPACE_ID::internal::DescriptorTable descriptor_table_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto;
namespace xla {
namespace gpu {
class ConditionalThunkProto;
struct ConditionalThunkProtoDefaultTypeInternal;
PROTOBUF_EXPORT extern ConditionalThunkProtoDefaultTypeInternal _ConditionalThunkProto_default_instance_;
class CopyThunkProto;
struct CopyThunkProtoDefaultTypeInternal;
PROTOBUF_EXPORT extern CopyThunkProtoDefaultTypeInternal _CopyThunkProto_default_instance_;
class DeviceToHostCopyThunkProto;
struct DeviceToHostCopyThunkProtoDefaultTypeInternal;
PROTOBUF_EXPORT extern DeviceToHostCopyThunkProtoDefaultTypeInternal _DeviceToHostCopyThunkProto_default_instance_;
class HostToDeviceCopyThunkProto;
struct HostToDeviceCopyThunkProtoDefaultTypeInternal;
PROTOBUF_EXPORT extern HostToDeviceCopyThunkProtoDefaultTypeInternal _HostToDeviceCopyThunkProto_default_instance_;
class SequentialThunkProto;
struct SequentialThunkProtoDefaultTypeInternal;
PROTOBUF_EXPORT extern SequentialThunkProtoDefaultTypeInternal _SequentialThunkProto_default_instance_;
class ThunkInfoProto;
struct ThunkInfoProtoDefaultTypeInternal;
PROTOBUF_EXPORT extern ThunkInfoProtoDefaultTypeInternal _ThunkInfoProto_default_instance_;
class ThunkProto;
struct ThunkProtoDefaultTypeInternal;
PROTOBUF_EXPORT extern ThunkProtoDefaultTypeInternal _ThunkProto_default_instance_;
class WhileThunkProto;
struct WhileThunkProtoDefaultTypeInternal;
PROTOBUF_EXPORT extern WhileThunkProtoDefaultTypeInternal _WhileThunkProto_default_instance_;
}  // namespace gpu
}  // namespace xla
PROTOBUF_NAMESPACE_OPEN
template<> PROTOBUF_EXPORT ::xla::gpu::ConditionalThunkProto* Arena::CreateMaybeMessage<::xla::gpu::ConditionalThunkProto>(Arena*);
template<> PROTOBUF_EXPORT ::xla::gpu::CopyThunkProto* Arena::CreateMaybeMessage<::xla::gpu::CopyThunkProto>(Arena*);
template<> PROTOBUF_EXPORT ::xla::gpu::DeviceToHostCopyThunkProto* Arena::CreateMaybeMessage<::xla::gpu::DeviceToHostCopyThunkProto>(Arena*);
template<> PROTOBUF_EXPORT ::xla::gpu::HostToDeviceCopyThunkProto* Arena::CreateMaybeMessage<::xla::gpu::HostToDeviceCopyThunkProto>(Arena*);
template<> PROTOBUF_EXPORT ::xla::gpu::SequentialThunkProto* Arena::CreateMaybeMessage<::xla::gpu::SequentialThunkProto>(Arena*);
template<> PROTOBUF_EXPORT ::xla::gpu::ThunkInfoProto* Arena::CreateMaybeMessage<::xla::gpu::ThunkInfoProto>(Arena*);
template<> PROTOBUF_EXPORT ::xla::gpu::ThunkProto* Arena::CreateMaybeMessage<::xla::gpu::ThunkProto>(Arena*);
template<> PROTOBUF_EXPORT ::xla::gpu::WhileThunkProto* Arena::CreateMaybeMessage<::xla::gpu::WhileThunkProto>(Arena*);
PROTOBUF_NAMESPACE_CLOSE
namespace xla {
namespace gpu {

// ===================================================================

class PROTOBUF_EXPORT ThunkInfoProto final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:xla.gpu.ThunkInfoProto) */ {
 public:
  inline ThunkInfoProto() : ThunkInfoProto(nullptr) {}
  ~ThunkInfoProto() override;
  explicit PROTOBUF_CONSTEXPR ThunkInfoProto(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ThunkInfoProto(const ThunkInfoProto& from);
  ThunkInfoProto(ThunkInfoProto&& from) noexcept
    : ThunkInfoProto() {
    *this = ::std::move(from);
  }

  inline ThunkInfoProto& operator=(const ThunkInfoProto& from) {
    CopyFrom(from);
    return *this;
  }
  inline ThunkInfoProto& operator=(ThunkInfoProto&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const ThunkInfoProto& default_instance() {
    return *internal_default_instance();
  }
  static inline const ThunkInfoProto* internal_default_instance() {
    return reinterpret_cast<const ThunkInfoProto*>(
               &_ThunkInfoProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    0;

  friend void swap(ThunkInfoProto& a, ThunkInfoProto& b) {
    a.Swap(&b);
  }
  inline void Swap(ThunkInfoProto* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ThunkInfoProto* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ThunkInfoProto* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ThunkInfoProto>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const ThunkInfoProto& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom( const ThunkInfoProto& from) {
    ThunkInfoProto::MergeImpl(*this, from);
  }
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _impl_._cached_size_.Get(); }

  private:
  void SharedCtor(::PROTOBUF_NAMESPACE_ID::Arena* arena, bool is_message_owned);
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ThunkInfoProto* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "xla.gpu.ThunkInfoProto";
  }
  protected:
  explicit ThunkInfoProto(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kProfileAnnotationFieldNumber = 1,
    kExecutionStreamIdFieldNumber = 2,
  };
  // string profile_annotation = 1;
  void clear_profile_annotation();
  const std::string& profile_annotation() const;
  template <typename ArgT0 = const std::string&, typename... ArgT>
  void set_profile_annotation(ArgT0&& arg0, ArgT... args);
  std::string* mutable_profile_annotation();
  PROTOBUF_NODISCARD std::string* release_profile_annotation();
  void set_allocated_profile_annotation(std::string* profile_annotation);
  private:
  const std::string& _internal_profile_annotation() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_profile_annotation(const std::string& value);
  std::string* _internal_mutable_profile_annotation();
  public:

  // int64 execution_stream_id = 2;
  void clear_execution_stream_id();
  int64_t execution_stream_id() const;
  void set_execution_stream_id(int64_t value);
  private:
  int64_t _internal_execution_stream_id() const;
  void _internal_set_execution_stream_id(int64_t value);
  public:

  // @@protoc_insertion_point(class_scope:xla.gpu.ThunkInfoProto)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  struct Impl_ {
    ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr profile_annotation_;
    int64_t execution_stream_id_;
    mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  };
  union { Impl_ _impl_; };
  friend struct ::TableStruct_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto;
};
// -------------------------------------------------------------------

class PROTOBUF_EXPORT CopyThunkProto final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:xla.gpu.CopyThunkProto) */ {
 public:
  inline CopyThunkProto() : CopyThunkProto(nullptr) {}
  ~CopyThunkProto() override;
  explicit PROTOBUF_CONSTEXPR CopyThunkProto(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  CopyThunkProto(const CopyThunkProto& from);
  CopyThunkProto(CopyThunkProto&& from) noexcept
    : CopyThunkProto() {
    *this = ::std::move(from);
  }

  inline CopyThunkProto& operator=(const CopyThunkProto& from) {
    CopyFrom(from);
    return *this;
  }
  inline CopyThunkProto& operator=(CopyThunkProto&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const CopyThunkProto& default_instance() {
    return *internal_default_instance();
  }
  static inline const CopyThunkProto* internal_default_instance() {
    return reinterpret_cast<const CopyThunkProto*>(
               &_CopyThunkProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    1;

  friend void swap(CopyThunkProto& a, CopyThunkProto& b) {
    a.Swap(&b);
  }
  inline void Swap(CopyThunkProto* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(CopyThunkProto* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  CopyThunkProto* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<CopyThunkProto>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const CopyThunkProto& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom( const CopyThunkProto& from) {
    CopyThunkProto::MergeImpl(*this, from);
  }
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _impl_._cached_size_.Get(); }

  private:
  void SharedCtor(::PROTOBUF_NAMESPACE_ID::Arena* arena, bool is_message_owned);
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(CopyThunkProto* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "xla.gpu.CopyThunkProto";
  }
  protected:
  explicit CopyThunkProto(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kSourceBufferFieldNumber = 1,
    kDestinationBufferFieldNumber = 2,
    kMemSizeFieldNumber = 3,
  };
  // .xla.buffer_assignment.BufferAllocationSliceProto source_buffer = 1;
  bool has_source_buffer() const;
  private:
  bool _internal_has_source_buffer() const;
  public:
  void clear_source_buffer();
  const ::xla::buffer_assignment::BufferAllocationSliceProto& source_buffer() const;
  PROTOBUF_NODISCARD ::xla::buffer_assignment::BufferAllocationSliceProto* release_source_buffer();
  ::xla::buffer_assignment::BufferAllocationSliceProto* mutable_source_buffer();
  void set_allocated_source_buffer(::xla::buffer_assignment::BufferAllocationSliceProto* source_buffer);
  private:
  const ::xla::buffer_assignment::BufferAllocationSliceProto& _internal_source_buffer() const;
  ::xla::buffer_assignment::BufferAllocationSliceProto* _internal_mutable_source_buffer();
  public:
  void unsafe_arena_set_allocated_source_buffer(
      ::xla::buffer_assignment::BufferAllocationSliceProto* source_buffer);
  ::xla::buffer_assignment::BufferAllocationSliceProto* unsafe_arena_release_source_buffer();

  // .xla.buffer_assignment.BufferAllocationSliceProto destination_buffer = 2;
  bool has_destination_buffer() const;
  private:
  bool _internal_has_destination_buffer() const;
  public:
  void clear_destination_buffer();
  const ::xla::buffer_assignment::BufferAllocationSliceProto& destination_buffer() const;
  PROTOBUF_NODISCARD ::xla::buffer_assignment::BufferAllocationSliceProto* release_destination_buffer();
  ::xla::buffer_assignment::BufferAllocationSliceProto* mutable_destination_buffer();
  void set_allocated_destination_buffer(::xla::buffer_assignment::BufferAllocationSliceProto* destination_buffer);
  private:
  const ::xla::buffer_assignment::BufferAllocationSliceProto& _internal_destination_buffer() const;
  ::xla::buffer_assignment::BufferAllocationSliceProto* _internal_mutable_destination_buffer();
  public:
  void unsafe_arena_set_allocated_destination_buffer(
      ::xla::buffer_assignment::BufferAllocationSliceProto* destination_buffer);
  ::xla::buffer_assignment::BufferAllocationSliceProto* unsafe_arena_release_destination_buffer();

  // int64 mem_size = 3;
  void clear_mem_size();
  int64_t mem_size() const;
  void set_mem_size(int64_t value);
  private:
  int64_t _internal_mem_size() const;
  void _internal_set_mem_size(int64_t value);
  public:

  // @@protoc_insertion_point(class_scope:xla.gpu.CopyThunkProto)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  struct Impl_ {
    ::xla::buffer_assignment::BufferAllocationSliceProto* source_buffer_;
    ::xla::buffer_assignment::BufferAllocationSliceProto* destination_buffer_;
    int64_t mem_size_;
    mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  };
  union { Impl_ _impl_; };
  friend struct ::TableStruct_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto;
};
// -------------------------------------------------------------------

class PROTOBUF_EXPORT DeviceToHostCopyThunkProto final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:xla.gpu.DeviceToHostCopyThunkProto) */ {
 public:
  inline DeviceToHostCopyThunkProto() : DeviceToHostCopyThunkProto(nullptr) {}
  ~DeviceToHostCopyThunkProto() override;
  explicit PROTOBUF_CONSTEXPR DeviceToHostCopyThunkProto(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  DeviceToHostCopyThunkProto(const DeviceToHostCopyThunkProto& from);
  DeviceToHostCopyThunkProto(DeviceToHostCopyThunkProto&& from) noexcept
    : DeviceToHostCopyThunkProto() {
    *this = ::std::move(from);
  }

  inline DeviceToHostCopyThunkProto& operator=(const DeviceToHostCopyThunkProto& from) {
    CopyFrom(from);
    return *this;
  }
  inline DeviceToHostCopyThunkProto& operator=(DeviceToHostCopyThunkProto&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const DeviceToHostCopyThunkProto& default_instance() {
    return *internal_default_instance();
  }
  static inline const DeviceToHostCopyThunkProto* internal_default_instance() {
    return reinterpret_cast<const DeviceToHostCopyThunkProto*>(
               &_DeviceToHostCopyThunkProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    2;

  friend void swap(DeviceToHostCopyThunkProto& a, DeviceToHostCopyThunkProto& b) {
    a.Swap(&b);
  }
  inline void Swap(DeviceToHostCopyThunkProto* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(DeviceToHostCopyThunkProto* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  DeviceToHostCopyThunkProto* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<DeviceToHostCopyThunkProto>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const DeviceToHostCopyThunkProto& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom( const DeviceToHostCopyThunkProto& from) {
    DeviceToHostCopyThunkProto::MergeImpl(*this, from);
  }
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _impl_._cached_size_.Get(); }

  private:
  void SharedCtor(::PROTOBUF_NAMESPACE_ID::Arena* arena, bool is_message_owned);
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(DeviceToHostCopyThunkProto* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "xla.gpu.DeviceToHostCopyThunkProto";
  }
  protected:
  explicit DeviceToHostCopyThunkProto(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kCopyThunkFieldNumber = 1,
  };
  // .xla.gpu.CopyThunkProto copy_thunk = 1;
  bool has_copy_thunk() const;
  private:
  bool _internal_has_copy_thunk() const;
  public:
  void clear_copy_thunk();
  const ::xla::gpu::CopyThunkProto& copy_thunk() const;
  PROTOBUF_NODISCARD ::xla::gpu::CopyThunkProto* release_copy_thunk();
  ::xla::gpu::CopyThunkProto* mutable_copy_thunk();
  void set_allocated_copy_thunk(::xla::gpu::CopyThunkProto* copy_thunk);
  private:
  const ::xla::gpu::CopyThunkProto& _internal_copy_thunk() const;
  ::xla::gpu::CopyThunkProto* _internal_mutable_copy_thunk();
  public:
  void unsafe_arena_set_allocated_copy_thunk(
      ::xla::gpu::CopyThunkProto* copy_thunk);
  ::xla::gpu::CopyThunkProto* unsafe_arena_release_copy_thunk();

  // @@protoc_insertion_point(class_scope:xla.gpu.DeviceToHostCopyThunkProto)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  struct Impl_ {
    ::xla::gpu::CopyThunkProto* copy_thunk_;
    mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  };
  union { Impl_ _impl_; };
  friend struct ::TableStruct_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto;
};
// -------------------------------------------------------------------

class PROTOBUF_EXPORT HostToDeviceCopyThunkProto final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:xla.gpu.HostToDeviceCopyThunkProto) */ {
 public:
  inline HostToDeviceCopyThunkProto() : HostToDeviceCopyThunkProto(nullptr) {}
  ~HostToDeviceCopyThunkProto() override;
  explicit PROTOBUF_CONSTEXPR HostToDeviceCopyThunkProto(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  HostToDeviceCopyThunkProto(const HostToDeviceCopyThunkProto& from);
  HostToDeviceCopyThunkProto(HostToDeviceCopyThunkProto&& from) noexcept
    : HostToDeviceCopyThunkProto() {
    *this = ::std::move(from);
  }

  inline HostToDeviceCopyThunkProto& operator=(const HostToDeviceCopyThunkProto& from) {
    CopyFrom(from);
    return *this;
  }
  inline HostToDeviceCopyThunkProto& operator=(HostToDeviceCopyThunkProto&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const HostToDeviceCopyThunkProto& default_instance() {
    return *internal_default_instance();
  }
  static inline const HostToDeviceCopyThunkProto* internal_default_instance() {
    return reinterpret_cast<const HostToDeviceCopyThunkProto*>(
               &_HostToDeviceCopyThunkProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    3;

  friend void swap(HostToDeviceCopyThunkProto& a, HostToDeviceCopyThunkProto& b) {
    a.Swap(&b);
  }
  inline void Swap(HostToDeviceCopyThunkProto* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(HostToDeviceCopyThunkProto* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  HostToDeviceCopyThunkProto* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<HostToDeviceCopyThunkProto>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const HostToDeviceCopyThunkProto& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom( const HostToDeviceCopyThunkProto& from) {
    HostToDeviceCopyThunkProto::MergeImpl(*this, from);
  }
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _impl_._cached_size_.Get(); }

  private:
  void SharedCtor(::PROTOBUF_NAMESPACE_ID::Arena* arena, bool is_message_owned);
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(HostToDeviceCopyThunkProto* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "xla.gpu.HostToDeviceCopyThunkProto";
  }
  protected:
  explicit HostToDeviceCopyThunkProto(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kCopyThunkFieldNumber = 1,
  };
  // .xla.gpu.CopyThunkProto copy_thunk = 1;
  bool has_copy_thunk() const;
  private:
  bool _internal_has_copy_thunk() const;
  public:
  void clear_copy_thunk();
  const ::xla::gpu::CopyThunkProto& copy_thunk() const;
  PROTOBUF_NODISCARD ::xla::gpu::CopyThunkProto* release_copy_thunk();
  ::xla::gpu::CopyThunkProto* mutable_copy_thunk();
  void set_allocated_copy_thunk(::xla::gpu::CopyThunkProto* copy_thunk);
  private:
  const ::xla::gpu::CopyThunkProto& _internal_copy_thunk() const;
  ::xla::gpu::CopyThunkProto* _internal_mutable_copy_thunk();
  public:
  void unsafe_arena_set_allocated_copy_thunk(
      ::xla::gpu::CopyThunkProto* copy_thunk);
  ::xla::gpu::CopyThunkProto* unsafe_arena_release_copy_thunk();

  // @@protoc_insertion_point(class_scope:xla.gpu.HostToDeviceCopyThunkProto)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  struct Impl_ {
    ::xla::gpu::CopyThunkProto* copy_thunk_;
    mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  };
  union { Impl_ _impl_; };
  friend struct ::TableStruct_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto;
};
// -------------------------------------------------------------------

class PROTOBUF_EXPORT ConditionalThunkProto final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:xla.gpu.ConditionalThunkProto) */ {
 public:
  inline ConditionalThunkProto() : ConditionalThunkProto(nullptr) {}
  ~ConditionalThunkProto() override;
  explicit PROTOBUF_CONSTEXPR ConditionalThunkProto(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ConditionalThunkProto(const ConditionalThunkProto& from);
  ConditionalThunkProto(ConditionalThunkProto&& from) noexcept
    : ConditionalThunkProto() {
    *this = ::std::move(from);
  }

  inline ConditionalThunkProto& operator=(const ConditionalThunkProto& from) {
    CopyFrom(from);
    return *this;
  }
  inline ConditionalThunkProto& operator=(ConditionalThunkProto&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const ConditionalThunkProto& default_instance() {
    return *internal_default_instance();
  }
  static inline const ConditionalThunkProto* internal_default_instance() {
    return reinterpret_cast<const ConditionalThunkProto*>(
               &_ConditionalThunkProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    4;

  friend void swap(ConditionalThunkProto& a, ConditionalThunkProto& b) {
    a.Swap(&b);
  }
  inline void Swap(ConditionalThunkProto* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ConditionalThunkProto* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ConditionalThunkProto* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ConditionalThunkProto>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const ConditionalThunkProto& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom( const ConditionalThunkProto& from) {
    ConditionalThunkProto::MergeImpl(*this, from);
  }
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _impl_._cached_size_.Get(); }

  private:
  void SharedCtor(::PROTOBUF_NAMESPACE_ID::Arena* arena, bool is_message_owned);
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ConditionalThunkProto* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "xla.gpu.ConditionalThunkProto";
  }
  protected:
  explicit ConditionalThunkProto(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kBranchThunksFieldNumber = 2,
    kBranchIndexBufferFieldNumber = 1,
    kBranchIndexIsBoolFieldNumber = 3,
  };
  // repeated .xla.gpu.SequentialThunkProto branch_thunks = 2;
  int branch_thunks_size() const;
  private:
  int _internal_branch_thunks_size() const;
  public:
  void clear_branch_thunks();
  ::xla::gpu::SequentialThunkProto* mutable_branch_thunks(int index);
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::gpu::SequentialThunkProto >*
      mutable_branch_thunks();
  private:
  const ::xla::gpu::SequentialThunkProto& _internal_branch_thunks(int index) const;
  ::xla::gpu::SequentialThunkProto* _internal_add_branch_thunks();
  public:
  const ::xla::gpu::SequentialThunkProto& branch_thunks(int index) const;
  ::xla::gpu::SequentialThunkProto* add_branch_thunks();
  const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::gpu::SequentialThunkProto >&
      branch_thunks() const;

  // .xla.buffer_assignment.BufferAllocationSliceProto branch_index_buffer = 1;
  bool has_branch_index_buffer() const;
  private:
  bool _internal_has_branch_index_buffer() const;
  public:
  void clear_branch_index_buffer();
  const ::xla::buffer_assignment::BufferAllocationSliceProto& branch_index_buffer() const;
  PROTOBUF_NODISCARD ::xla::buffer_assignment::BufferAllocationSliceProto* release_branch_index_buffer();
  ::xla::buffer_assignment::BufferAllocationSliceProto* mutable_branch_index_buffer();
  void set_allocated_branch_index_buffer(::xla::buffer_assignment::BufferAllocationSliceProto* branch_index_buffer);
  private:
  const ::xla::buffer_assignment::BufferAllocationSliceProto& _internal_branch_index_buffer() const;
  ::xla::buffer_assignment::BufferAllocationSliceProto* _internal_mutable_branch_index_buffer();
  public:
  void unsafe_arena_set_allocated_branch_index_buffer(
      ::xla::buffer_assignment::BufferAllocationSliceProto* branch_index_buffer);
  ::xla::buffer_assignment::BufferAllocationSliceProto* unsafe_arena_release_branch_index_buffer();

  // bool branch_index_is_bool = 3;
  void clear_branch_index_is_bool();
  bool branch_index_is_bool() const;
  void set_branch_index_is_bool(bool value);
  private:
  bool _internal_branch_index_is_bool() const;
  void _internal_set_branch_index_is_bool(bool value);
  public:

  // @@protoc_insertion_point(class_scope:xla.gpu.ConditionalThunkProto)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  struct Impl_ {
    ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::gpu::SequentialThunkProto > branch_thunks_;
    ::xla::buffer_assignment::BufferAllocationSliceProto* branch_index_buffer_;
    bool branch_index_is_bool_;
    mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  };
  union { Impl_ _impl_; };
  friend struct ::TableStruct_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto;
};
// -------------------------------------------------------------------

class PROTOBUF_EXPORT WhileThunkProto final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:xla.gpu.WhileThunkProto) */ {
 public:
  inline WhileThunkProto() : WhileThunkProto(nullptr) {}
  ~WhileThunkProto() override;
  explicit PROTOBUF_CONSTEXPR WhileThunkProto(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  WhileThunkProto(const WhileThunkProto& from);
  WhileThunkProto(WhileThunkProto&& from) noexcept
    : WhileThunkProto() {
    *this = ::std::move(from);
  }

  inline WhileThunkProto& operator=(const WhileThunkProto& from) {
    CopyFrom(from);
    return *this;
  }
  inline WhileThunkProto& operator=(WhileThunkProto&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const WhileThunkProto& default_instance() {
    return *internal_default_instance();
  }
  static inline const WhileThunkProto* internal_default_instance() {
    return reinterpret_cast<const WhileThunkProto*>(
               &_WhileThunkProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    5;

  friend void swap(WhileThunkProto& a, WhileThunkProto& b) {
    a.Swap(&b);
  }
  inline void Swap(WhileThunkProto* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(WhileThunkProto* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  WhileThunkProto* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<WhileThunkProto>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const WhileThunkProto& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom( const WhileThunkProto& from) {
    WhileThunkProto::MergeImpl(*this, from);
  }
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _impl_._cached_size_.Get(); }

  private:
  void SharedCtor(::PROTOBUF_NAMESPACE_ID::Arena* arena, bool is_message_owned);
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(WhileThunkProto* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "xla.gpu.WhileThunkProto";
  }
  protected:
  explicit WhileThunkProto(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kConditionResultBufferIndexFieldNumber = 1,
    kConditionThunkSequenceFieldNumber = 2,
    kBodyThunkSequenceFieldNumber = 3,
    kTripCountFieldNumber = 4,
  };
  // .xla.buffer_assignment.BufferAllocationSliceProto condition_result_buffer_index = 1;
  bool has_condition_result_buffer_index() const;
  private:
  bool _internal_has_condition_result_buffer_index() const;
  public:
  void clear_condition_result_buffer_index();
  const ::xla::buffer_assignment::BufferAllocationSliceProto& condition_result_buffer_index() const;
  PROTOBUF_NODISCARD ::xla::buffer_assignment::BufferAllocationSliceProto* release_condition_result_buffer_index();
  ::xla::buffer_assignment::BufferAllocationSliceProto* mutable_condition_result_buffer_index();
  void set_allocated_condition_result_buffer_index(::xla::buffer_assignment::BufferAllocationSliceProto* condition_result_buffer_index);
  private:
  const ::xla::buffer_assignment::BufferAllocationSliceProto& _internal_condition_result_buffer_index() const;
  ::xla::buffer_assignment::BufferAllocationSliceProto* _internal_mutable_condition_result_buffer_index();
  public:
  void unsafe_arena_set_allocated_condition_result_buffer_index(
      ::xla::buffer_assignment::BufferAllocationSliceProto* condition_result_buffer_index);
  ::xla::buffer_assignment::BufferAllocationSliceProto* unsafe_arena_release_condition_result_buffer_index();

  // .xla.gpu.SequentialThunkProto condition_thunk_sequence = 2;
  bool has_condition_thunk_sequence() const;
  private:
  bool _internal_has_condition_thunk_sequence() const;
  public:
  void clear_condition_thunk_sequence();
  const ::xla::gpu::SequentialThunkProto& condition_thunk_sequence() const;
  PROTOBUF_NODISCARD ::xla::gpu::SequentialThunkProto* release_condition_thunk_sequence();
  ::xla::gpu::SequentialThunkProto* mutable_condition_thunk_sequence();
  void set_allocated_condition_thunk_sequence(::xla::gpu::SequentialThunkProto* condition_thunk_sequence);
  private:
  const ::xla::gpu::SequentialThunkProto& _internal_condition_thunk_sequence() const;
  ::xla::gpu::SequentialThunkProto* _internal_mutable_condition_thunk_sequence();
  public:
  void unsafe_arena_set_allocated_condition_thunk_sequence(
      ::xla::gpu::SequentialThunkProto* condition_thunk_sequence);
  ::xla::gpu::SequentialThunkProto* unsafe_arena_release_condition_thunk_sequence();

  // .xla.gpu.SequentialThunkProto body_thunk_sequence = 3;
  bool has_body_thunk_sequence() const;
  private:
  bool _internal_has_body_thunk_sequence() const;
  public:
  void clear_body_thunk_sequence();
  const ::xla::gpu::SequentialThunkProto& body_thunk_sequence() const;
  PROTOBUF_NODISCARD ::xla::gpu::SequentialThunkProto* release_body_thunk_sequence();
  ::xla::gpu::SequentialThunkProto* mutable_body_thunk_sequence();
  void set_allocated_body_thunk_sequence(::xla::gpu::SequentialThunkProto* body_thunk_sequence);
  private:
  const ::xla::gpu::SequentialThunkProto& _internal_body_thunk_sequence() const;
  ::xla::gpu::SequentialThunkProto* _internal_mutable_body_thunk_sequence();
  public:
  void unsafe_arena_set_allocated_body_thunk_sequence(
      ::xla::gpu::SequentialThunkProto* body_thunk_sequence);
  ::xla::gpu::SequentialThunkProto* unsafe_arena_release_body_thunk_sequence();

  // optional int64 trip_count = 4;
  bool has_trip_count() const;
  private:
  bool _internal_has_trip_count() const;
  public:
  void clear_trip_count();
  int64_t trip_count() const;
  void set_trip_count(int64_t value);
  private:
  int64_t _internal_trip_count() const;
  void _internal_set_trip_count(int64_t value);
  public:

  // @@protoc_insertion_point(class_scope:xla.gpu.WhileThunkProto)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  struct Impl_ {
    ::PROTOBUF_NAMESPACE_ID::internal::HasBits<1> _has_bits_;
    mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
    ::xla::buffer_assignment::BufferAllocationSliceProto* condition_result_buffer_index_;
    ::xla::gpu::SequentialThunkProto* condition_thunk_sequence_;
    ::xla::gpu::SequentialThunkProto* body_thunk_sequence_;
    int64_t trip_count_;
  };
  union { Impl_ _impl_; };
  friend struct ::TableStruct_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto;
};
// -------------------------------------------------------------------

class PROTOBUF_EXPORT ThunkProto final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:xla.gpu.ThunkProto) */ {
 public:
  inline ThunkProto() : ThunkProto(nullptr) {}
  ~ThunkProto() override;
  explicit PROTOBUF_CONSTEXPR ThunkProto(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  ThunkProto(const ThunkProto& from);
  ThunkProto(ThunkProto&& from) noexcept
    : ThunkProto() {
    *this = ::std::move(from);
  }

  inline ThunkProto& operator=(const ThunkProto& from) {
    CopyFrom(from);
    return *this;
  }
  inline ThunkProto& operator=(ThunkProto&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const ThunkProto& default_instance() {
    return *internal_default_instance();
  }
  enum ImplCase {
    kSequentialThunk = 2,
    kCopyThunk = 3,
    kDeviceToHostCopyThunk = 4,
    kHostToDeviceCopyThunk = 5,
    kConditionalThunk = 6,
    kWhileThunk = 7,
    IMPL_NOT_SET = 0,
  };

  static inline const ThunkProto* internal_default_instance() {
    return reinterpret_cast<const ThunkProto*>(
               &_ThunkProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    6;

  friend void swap(ThunkProto& a, ThunkProto& b) {
    a.Swap(&b);
  }
  inline void Swap(ThunkProto* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(ThunkProto* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  ThunkProto* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<ThunkProto>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const ThunkProto& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom( const ThunkProto& from) {
    ThunkProto::MergeImpl(*this, from);
  }
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _impl_._cached_size_.Get(); }

  private:
  void SharedCtor(::PROTOBUF_NAMESPACE_ID::Arena* arena, bool is_message_owned);
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ThunkProto* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "xla.gpu.ThunkProto";
  }
  protected:
  explicit ThunkProto(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kThunkInfoFieldNumber = 1,
    kSequentialThunkFieldNumber = 2,
    kCopyThunkFieldNumber = 3,
    kDeviceToHostCopyThunkFieldNumber = 4,
    kHostToDeviceCopyThunkFieldNumber = 5,
    kConditionalThunkFieldNumber = 6,
    kWhileThunkFieldNumber = 7,
  };
  // .xla.gpu.ThunkInfoProto thunk_info = 1;
  bool has_thunk_info() const;
  private:
  bool _internal_has_thunk_info() const;
  public:
  void clear_thunk_info();
  const ::xla::gpu::ThunkInfoProto& thunk_info() const;
  PROTOBUF_NODISCARD ::xla::gpu::ThunkInfoProto* release_thunk_info();
  ::xla::gpu::ThunkInfoProto* mutable_thunk_info();
  void set_allocated_thunk_info(::xla::gpu::ThunkInfoProto* thunk_info);
  private:
  const ::xla::gpu::ThunkInfoProto& _internal_thunk_info() const;
  ::xla::gpu::ThunkInfoProto* _internal_mutable_thunk_info();
  public:
  void unsafe_arena_set_allocated_thunk_info(
      ::xla::gpu::ThunkInfoProto* thunk_info);
  ::xla::gpu::ThunkInfoProto* unsafe_arena_release_thunk_info();

  // .xla.gpu.SequentialThunkProto sequential_thunk = 2;
  bool has_sequential_thunk() const;
  private:
  bool _internal_has_sequential_thunk() const;
  public:
  void clear_sequential_thunk();
  const ::xla::gpu::SequentialThunkProto& sequential_thunk() const;
  PROTOBUF_NODISCARD ::xla::gpu::SequentialThunkProto* release_sequential_thunk();
  ::xla::gpu::SequentialThunkProto* mutable_sequential_thunk();
  void set_allocated_sequential_thunk(::xla::gpu::SequentialThunkProto* sequential_thunk);
  private:
  const ::xla::gpu::SequentialThunkProto& _internal_sequential_thunk() const;
  ::xla::gpu::SequentialThunkProto* _internal_mutable_sequential_thunk();
  public:
  void unsafe_arena_set_allocated_sequential_thunk(
      ::xla::gpu::SequentialThunkProto* sequential_thunk);
  ::xla::gpu::SequentialThunkProto* unsafe_arena_release_sequential_thunk();

  // .xla.gpu.CopyThunkProto copy_thunk = 3;
  bool has_copy_thunk() const;
  private:
  bool _internal_has_copy_thunk() const;
  public:
  void clear_copy_thunk();
  const ::xla::gpu::CopyThunkProto& copy_thunk() const;
  PROTOBUF_NODISCARD ::xla::gpu::CopyThunkProto* release_copy_thunk();
  ::xla::gpu::CopyThunkProto* mutable_copy_thunk();
  void set_allocated_copy_thunk(::xla::gpu::CopyThunkProto* copy_thunk);
  private:
  const ::xla::gpu::CopyThunkProto& _internal_copy_thunk() const;
  ::xla::gpu::CopyThunkProto* _internal_mutable_copy_thunk();
  public:
  void unsafe_arena_set_allocated_copy_thunk(
      ::xla::gpu::CopyThunkProto* copy_thunk);
  ::xla::gpu::CopyThunkProto* unsafe_arena_release_copy_thunk();

  // .xla.gpu.DeviceToHostCopyThunkProto device_to_host_copy_thunk = 4;
  bool has_device_to_host_copy_thunk() const;
  private:
  bool _internal_has_device_to_host_copy_thunk() const;
  public:
  void clear_device_to_host_copy_thunk();
  const ::xla::gpu::DeviceToHostCopyThunkProto& device_to_host_copy_thunk() const;
  PROTOBUF_NODISCARD ::xla::gpu::DeviceToHostCopyThunkProto* release_device_to_host_copy_thunk();
  ::xla::gpu::DeviceToHostCopyThunkProto* mutable_device_to_host_copy_thunk();
  void set_allocated_device_to_host_copy_thunk(::xla::gpu::DeviceToHostCopyThunkProto* device_to_host_copy_thunk);
  private:
  const ::xla::gpu::DeviceToHostCopyThunkProto& _internal_device_to_host_copy_thunk() const;
  ::xla::gpu::DeviceToHostCopyThunkProto* _internal_mutable_device_to_host_copy_thunk();
  public:
  void unsafe_arena_set_allocated_device_to_host_copy_thunk(
      ::xla::gpu::DeviceToHostCopyThunkProto* device_to_host_copy_thunk);
  ::xla::gpu::DeviceToHostCopyThunkProto* unsafe_arena_release_device_to_host_copy_thunk();

  // .xla.gpu.HostToDeviceCopyThunkProto host_to_device_copy_thunk = 5;
  bool has_host_to_device_copy_thunk() const;
  private:
  bool _internal_has_host_to_device_copy_thunk() const;
  public:
  void clear_host_to_device_copy_thunk();
  const ::xla::gpu::HostToDeviceCopyThunkProto& host_to_device_copy_thunk() const;
  PROTOBUF_NODISCARD ::xla::gpu::HostToDeviceCopyThunkProto* release_host_to_device_copy_thunk();
  ::xla::gpu::HostToDeviceCopyThunkProto* mutable_host_to_device_copy_thunk();
  void set_allocated_host_to_device_copy_thunk(::xla::gpu::HostToDeviceCopyThunkProto* host_to_device_copy_thunk);
  private:
  const ::xla::gpu::HostToDeviceCopyThunkProto& _internal_host_to_device_copy_thunk() const;
  ::xla::gpu::HostToDeviceCopyThunkProto* _internal_mutable_host_to_device_copy_thunk();
  public:
  void unsafe_arena_set_allocated_host_to_device_copy_thunk(
      ::xla::gpu::HostToDeviceCopyThunkProto* host_to_device_copy_thunk);
  ::xla::gpu::HostToDeviceCopyThunkProto* unsafe_arena_release_host_to_device_copy_thunk();

  // .xla.gpu.ConditionalThunkProto conditional_thunk = 6;
  bool has_conditional_thunk() const;
  private:
  bool _internal_has_conditional_thunk() const;
  public:
  void clear_conditional_thunk();
  const ::xla::gpu::ConditionalThunkProto& conditional_thunk() const;
  PROTOBUF_NODISCARD ::xla::gpu::ConditionalThunkProto* release_conditional_thunk();
  ::xla::gpu::ConditionalThunkProto* mutable_conditional_thunk();
  void set_allocated_conditional_thunk(::xla::gpu::ConditionalThunkProto* conditional_thunk);
  private:
  const ::xla::gpu::ConditionalThunkProto& _internal_conditional_thunk() const;
  ::xla::gpu::ConditionalThunkProto* _internal_mutable_conditional_thunk();
  public:
  void unsafe_arena_set_allocated_conditional_thunk(
      ::xla::gpu::ConditionalThunkProto* conditional_thunk);
  ::xla::gpu::ConditionalThunkProto* unsafe_arena_release_conditional_thunk();

  // .xla.gpu.WhileThunkProto while_thunk = 7;
  bool has_while_thunk() const;
  private:
  bool _internal_has_while_thunk() const;
  public:
  void clear_while_thunk();
  const ::xla::gpu::WhileThunkProto& while_thunk() const;
  PROTOBUF_NODISCARD ::xla::gpu::WhileThunkProto* release_while_thunk();
  ::xla::gpu::WhileThunkProto* mutable_while_thunk();
  void set_allocated_while_thunk(::xla::gpu::WhileThunkProto* while_thunk);
  private:
  const ::xla::gpu::WhileThunkProto& _internal_while_thunk() const;
  ::xla::gpu::WhileThunkProto* _internal_mutable_while_thunk();
  public:
  void unsafe_arena_set_allocated_while_thunk(
      ::xla::gpu::WhileThunkProto* while_thunk);
  ::xla::gpu::WhileThunkProto* unsafe_arena_release_while_thunk();

  void clear_impl();
  ImplCase impl_case() const;
  // @@protoc_insertion_point(class_scope:xla.gpu.ThunkProto)
 private:
  class _Internal;
  void set_has_sequential_thunk();
  void set_has_copy_thunk();
  void set_has_device_to_host_copy_thunk();
  void set_has_host_to_device_copy_thunk();
  void set_has_conditional_thunk();
  void set_has_while_thunk();

  inline bool has_impl() const;
  inline void clear_has_impl();

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  struct Impl_ {
    ::xla::gpu::ThunkInfoProto* thunk_info_;
    union ImplUnion {
      constexpr ImplUnion() : _constinit_{} {}
        ::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized _constinit_;
      ::xla::gpu::SequentialThunkProto* sequential_thunk_;
      ::xla::gpu::CopyThunkProto* copy_thunk_;
      ::xla::gpu::DeviceToHostCopyThunkProto* device_to_host_copy_thunk_;
      ::xla::gpu::HostToDeviceCopyThunkProto* host_to_device_copy_thunk_;
      ::xla::gpu::ConditionalThunkProto* conditional_thunk_;
      ::xla::gpu::WhileThunkProto* while_thunk_;
    } impl_;
    mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
    uint32_t _oneof_case_[1];

  };
  union { Impl_ _impl_; };
  friend struct ::TableStruct_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto;
};
// -------------------------------------------------------------------

class PROTOBUF_EXPORT SequentialThunkProto final :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:xla.gpu.SequentialThunkProto) */ {
 public:
  inline SequentialThunkProto() : SequentialThunkProto(nullptr) {}
  ~SequentialThunkProto() override;
  explicit PROTOBUF_CONSTEXPR SequentialThunkProto(::PROTOBUF_NAMESPACE_ID::internal::ConstantInitialized);

  SequentialThunkProto(const SequentialThunkProto& from);
  SequentialThunkProto(SequentialThunkProto&& from) noexcept
    : SequentialThunkProto() {
    *this = ::std::move(from);
  }

  inline SequentialThunkProto& operator=(const SequentialThunkProto& from) {
    CopyFrom(from);
    return *this;
  }
  inline SequentialThunkProto& operator=(SequentialThunkProto&& from) noexcept {
    if (this == &from) return *this;
    if (GetOwningArena() == from.GetOwningArena()
  #ifdef PROTOBUF_FORCE_COPY_IN_MOVE
        && GetOwningArena() != nullptr
  #endif  // !PROTOBUF_FORCE_COPY_IN_MOVE
    ) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const SequentialThunkProto& default_instance() {
    return *internal_default_instance();
  }
  static inline const SequentialThunkProto* internal_default_instance() {
    return reinterpret_cast<const SequentialThunkProto*>(
               &_SequentialThunkProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    7;

  friend void swap(SequentialThunkProto& a, SequentialThunkProto& b) {
    a.Swap(&b);
  }
  inline void Swap(SequentialThunkProto* other) {
    if (other == this) return;
  #ifdef PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() != nullptr &&
        GetOwningArena() == other->GetOwningArena()) {
   #else  // PROTOBUF_FORCE_COPY_IN_SWAP
    if (GetOwningArena() == other->GetOwningArena()) {
  #endif  // !PROTOBUF_FORCE_COPY_IN_SWAP
      InternalSwap(other);
    } else {
      ::PROTOBUF_NAMESPACE_ID::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(SequentialThunkProto* other) {
    if (other == this) return;
    GOOGLE_DCHECK(GetOwningArena() == other->GetOwningArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  SequentialThunkProto* New(::PROTOBUF_NAMESPACE_ID::Arena* arena = nullptr) const final {
    return CreateMaybeMessage<SequentialThunkProto>(arena);
  }
  using ::PROTOBUF_NAMESPACE_ID::Message::CopyFrom;
  void CopyFrom(const SequentialThunkProto& from);
  using ::PROTOBUF_NAMESPACE_ID::Message::MergeFrom;
  void MergeFrom( const SequentialThunkProto& from) {
    SequentialThunkProto::MergeImpl(*this, from);
  }
  private:
  static void MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg);
  public:
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  uint8_t* _InternalSerialize(
      uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const final;
  int GetCachedSize() const final { return _impl_._cached_size_.Get(); }

  private:
  void SharedCtor(::PROTOBUF_NAMESPACE_ID::Arena* arena, bool is_message_owned);
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(SequentialThunkProto* other);

  private:
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "xla.gpu.SequentialThunkProto";
  }
  protected:
  explicit SequentialThunkProto(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                       bool is_message_owned = false);
  public:

  static const ClassData _class_data_;
  const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*GetClassData() const final;

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kThunksFieldNumber = 1,
  };
  // repeated .xla.gpu.ThunkProto thunks = 1;
  int thunks_size() const;
  private:
  int _internal_thunks_size() const;
  public:
  void clear_thunks();
  ::xla::gpu::ThunkProto* mutable_thunks(int index);
  ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::gpu::ThunkProto >*
      mutable_thunks();
  private:
  const ::xla::gpu::ThunkProto& _internal_thunks(int index) const;
  ::xla::gpu::ThunkProto* _internal_add_thunks();
  public:
  const ::xla::gpu::ThunkProto& thunks(int index) const;
  ::xla::gpu::ThunkProto* add_thunks();
  const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::gpu::ThunkProto >&
      thunks() const;

  // @@protoc_insertion_point(class_scope:xla.gpu.SequentialThunkProto)
 private:
  class _Internal;

  template <typename T> friend class ::PROTOBUF_NAMESPACE_ID::Arena::InternalHelper;
  typedef void InternalArenaConstructable_;
  typedef void DestructorSkippable_;
  struct Impl_ {
    ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::gpu::ThunkProto > thunks_;
    mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  };
  union { Impl_ _impl_; };
  friend struct ::TableStruct_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto;
};
// ===================================================================


// ===================================================================

#ifdef __GNUC__
  #pragma GCC diagnostic push
  #pragma GCC diagnostic ignored "-Wstrict-aliasing"
#endif  // __GNUC__
// ThunkInfoProto

// string profile_annotation = 1;
inline void ThunkInfoProto::clear_profile_annotation() {
  _impl_.profile_annotation_.ClearToEmpty();
}
inline const std::string& ThunkInfoProto::profile_annotation() const {
  // @@protoc_insertion_point(field_get:xla.gpu.ThunkInfoProto.profile_annotation)
  return _internal_profile_annotation();
}
template <typename ArgT0, typename... ArgT>
inline PROTOBUF_ALWAYS_INLINE
void ThunkInfoProto::set_profile_annotation(ArgT0&& arg0, ArgT... args) {
 
 _impl_.profile_annotation_.Set(static_cast<ArgT0 &&>(arg0), args..., GetArenaForAllocation());
  // @@protoc_insertion_point(field_set:xla.gpu.ThunkInfoProto.profile_annotation)
}
inline std::string* ThunkInfoProto::mutable_profile_annotation() {
  std::string* _s = _internal_mutable_profile_annotation();
  // @@protoc_insertion_point(field_mutable:xla.gpu.ThunkInfoProto.profile_annotation)
  return _s;
}
inline const std::string& ThunkInfoProto::_internal_profile_annotation() const {
  return _impl_.profile_annotation_.Get();
}
inline void ThunkInfoProto::_internal_set_profile_annotation(const std::string& value) {
  
  _impl_.profile_annotation_.Set(value, GetArenaForAllocation());
}
inline std::string* ThunkInfoProto::_internal_mutable_profile_annotation() {
  
  return _impl_.profile_annotation_.Mutable(GetArenaForAllocation());
}
inline std::string* ThunkInfoProto::release_profile_annotation() {
  // @@protoc_insertion_point(field_release:xla.gpu.ThunkInfoProto.profile_annotation)
  return _impl_.profile_annotation_.Release();
}
inline void ThunkInfoProto::set_allocated_profile_annotation(std::string* profile_annotation) {
  if (profile_annotation != nullptr) {
    
  } else {
    
  }
  _impl_.profile_annotation_.SetAllocated(profile_annotation, GetArenaForAllocation());
#ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (_impl_.profile_annotation_.IsDefault()) {
    _impl_.profile_annotation_.Set("", GetArenaForAllocation());
  }
#endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.ThunkInfoProto.profile_annotation)
}

// int64 execution_stream_id = 2;
inline void ThunkInfoProto::clear_execution_stream_id() {
  _impl_.execution_stream_id_ = int64_t{0};
}
inline int64_t ThunkInfoProto::_internal_execution_stream_id() const {
  return _impl_.execution_stream_id_;
}
inline int64_t ThunkInfoProto::execution_stream_id() const {
  // @@protoc_insertion_point(field_get:xla.gpu.ThunkInfoProto.execution_stream_id)
  return _internal_execution_stream_id();
}
inline void ThunkInfoProto::_internal_set_execution_stream_id(int64_t value) {
  
  _impl_.execution_stream_id_ = value;
}
inline void ThunkInfoProto::set_execution_stream_id(int64_t value) {
  _internal_set_execution_stream_id(value);
  // @@protoc_insertion_point(field_set:xla.gpu.ThunkInfoProto.execution_stream_id)
}

// -------------------------------------------------------------------

// CopyThunkProto

// .xla.buffer_assignment.BufferAllocationSliceProto source_buffer = 1;
inline bool CopyThunkProto::_internal_has_source_buffer() const {
  return this != internal_default_instance() && _impl_.source_buffer_ != nullptr;
}
inline bool CopyThunkProto::has_source_buffer() const {
  return _internal_has_source_buffer();
}
inline const ::xla::buffer_assignment::BufferAllocationSliceProto& CopyThunkProto::_internal_source_buffer() const {
  const ::xla::buffer_assignment::BufferAllocationSliceProto* p = _impl_.source_buffer_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::buffer_assignment::BufferAllocationSliceProto&>(
      ::xla::buffer_assignment::_BufferAllocationSliceProto_default_instance_);
}
inline const ::xla::buffer_assignment::BufferAllocationSliceProto& CopyThunkProto::source_buffer() const {
  // @@protoc_insertion_point(field_get:xla.gpu.CopyThunkProto.source_buffer)
  return _internal_source_buffer();
}
inline void CopyThunkProto::unsafe_arena_set_allocated_source_buffer(
    ::xla::buffer_assignment::BufferAllocationSliceProto* source_buffer) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.source_buffer_);
  }
  _impl_.source_buffer_ = source_buffer;
  if (source_buffer) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.CopyThunkProto.source_buffer)
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* CopyThunkProto::release_source_buffer() {
  
  ::xla::buffer_assignment::BufferAllocationSliceProto* temp = _impl_.source_buffer_;
  _impl_.source_buffer_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* CopyThunkProto::unsafe_arena_release_source_buffer() {
  // @@protoc_insertion_point(field_release:xla.gpu.CopyThunkProto.source_buffer)
  
  ::xla::buffer_assignment::BufferAllocationSliceProto* temp = _impl_.source_buffer_;
  _impl_.source_buffer_ = nullptr;
  return temp;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* CopyThunkProto::_internal_mutable_source_buffer() {
  
  if (_impl_.source_buffer_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::buffer_assignment::BufferAllocationSliceProto>(GetArenaForAllocation());
    _impl_.source_buffer_ = p;
  }
  return _impl_.source_buffer_;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* CopyThunkProto::mutable_source_buffer() {
  ::xla::buffer_assignment::BufferAllocationSliceProto* _msg = _internal_mutable_source_buffer();
  // @@protoc_insertion_point(field_mutable:xla.gpu.CopyThunkProto.source_buffer)
  return _msg;
}
inline void CopyThunkProto::set_allocated_source_buffer(::xla::buffer_assignment::BufferAllocationSliceProto* source_buffer) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.source_buffer_);
  }
  if (source_buffer) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(source_buffer));
    if (message_arena != submessage_arena) {
      source_buffer = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, source_buffer, submessage_arena);
    }
    
  } else {
    
  }
  _impl_.source_buffer_ = source_buffer;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.CopyThunkProto.source_buffer)
}

// .xla.buffer_assignment.BufferAllocationSliceProto destination_buffer = 2;
inline bool CopyThunkProto::_internal_has_destination_buffer() const {
  return this != internal_default_instance() && _impl_.destination_buffer_ != nullptr;
}
inline bool CopyThunkProto::has_destination_buffer() const {
  return _internal_has_destination_buffer();
}
inline const ::xla::buffer_assignment::BufferAllocationSliceProto& CopyThunkProto::_internal_destination_buffer() const {
  const ::xla::buffer_assignment::BufferAllocationSliceProto* p = _impl_.destination_buffer_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::buffer_assignment::BufferAllocationSliceProto&>(
      ::xla::buffer_assignment::_BufferAllocationSliceProto_default_instance_);
}
inline const ::xla::buffer_assignment::BufferAllocationSliceProto& CopyThunkProto::destination_buffer() const {
  // @@protoc_insertion_point(field_get:xla.gpu.CopyThunkProto.destination_buffer)
  return _internal_destination_buffer();
}
inline void CopyThunkProto::unsafe_arena_set_allocated_destination_buffer(
    ::xla::buffer_assignment::BufferAllocationSliceProto* destination_buffer) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.destination_buffer_);
  }
  _impl_.destination_buffer_ = destination_buffer;
  if (destination_buffer) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.CopyThunkProto.destination_buffer)
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* CopyThunkProto::release_destination_buffer() {
  
  ::xla::buffer_assignment::BufferAllocationSliceProto* temp = _impl_.destination_buffer_;
  _impl_.destination_buffer_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* CopyThunkProto::unsafe_arena_release_destination_buffer() {
  // @@protoc_insertion_point(field_release:xla.gpu.CopyThunkProto.destination_buffer)
  
  ::xla::buffer_assignment::BufferAllocationSliceProto* temp = _impl_.destination_buffer_;
  _impl_.destination_buffer_ = nullptr;
  return temp;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* CopyThunkProto::_internal_mutable_destination_buffer() {
  
  if (_impl_.destination_buffer_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::buffer_assignment::BufferAllocationSliceProto>(GetArenaForAllocation());
    _impl_.destination_buffer_ = p;
  }
  return _impl_.destination_buffer_;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* CopyThunkProto::mutable_destination_buffer() {
  ::xla::buffer_assignment::BufferAllocationSliceProto* _msg = _internal_mutable_destination_buffer();
  // @@protoc_insertion_point(field_mutable:xla.gpu.CopyThunkProto.destination_buffer)
  return _msg;
}
inline void CopyThunkProto::set_allocated_destination_buffer(::xla::buffer_assignment::BufferAllocationSliceProto* destination_buffer) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.destination_buffer_);
  }
  if (destination_buffer) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(destination_buffer));
    if (message_arena != submessage_arena) {
      destination_buffer = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, destination_buffer, submessage_arena);
    }
    
  } else {
    
  }
  _impl_.destination_buffer_ = destination_buffer;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.CopyThunkProto.destination_buffer)
}

// int64 mem_size = 3;
inline void CopyThunkProto::clear_mem_size() {
  _impl_.mem_size_ = int64_t{0};
}
inline int64_t CopyThunkProto::_internal_mem_size() const {
  return _impl_.mem_size_;
}
inline int64_t CopyThunkProto::mem_size() const {
  // @@protoc_insertion_point(field_get:xla.gpu.CopyThunkProto.mem_size)
  return _internal_mem_size();
}
inline void CopyThunkProto::_internal_set_mem_size(int64_t value) {
  
  _impl_.mem_size_ = value;
}
inline void CopyThunkProto::set_mem_size(int64_t value) {
  _internal_set_mem_size(value);
  // @@protoc_insertion_point(field_set:xla.gpu.CopyThunkProto.mem_size)
}

// -------------------------------------------------------------------

// DeviceToHostCopyThunkProto

// .xla.gpu.CopyThunkProto copy_thunk = 1;
inline bool DeviceToHostCopyThunkProto::_internal_has_copy_thunk() const {
  return this != internal_default_instance() && _impl_.copy_thunk_ != nullptr;
}
inline bool DeviceToHostCopyThunkProto::has_copy_thunk() const {
  return _internal_has_copy_thunk();
}
inline void DeviceToHostCopyThunkProto::clear_copy_thunk() {
  if (GetArenaForAllocation() == nullptr && _impl_.copy_thunk_ != nullptr) {
    delete _impl_.copy_thunk_;
  }
  _impl_.copy_thunk_ = nullptr;
}
inline const ::xla::gpu::CopyThunkProto& DeviceToHostCopyThunkProto::_internal_copy_thunk() const {
  const ::xla::gpu::CopyThunkProto* p = _impl_.copy_thunk_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::gpu::CopyThunkProto&>(
      ::xla::gpu::_CopyThunkProto_default_instance_);
}
inline const ::xla::gpu::CopyThunkProto& DeviceToHostCopyThunkProto::copy_thunk() const {
  // @@protoc_insertion_point(field_get:xla.gpu.DeviceToHostCopyThunkProto.copy_thunk)
  return _internal_copy_thunk();
}
inline void DeviceToHostCopyThunkProto::unsafe_arena_set_allocated_copy_thunk(
    ::xla::gpu::CopyThunkProto* copy_thunk) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.copy_thunk_);
  }
  _impl_.copy_thunk_ = copy_thunk;
  if (copy_thunk) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.DeviceToHostCopyThunkProto.copy_thunk)
}
inline ::xla::gpu::CopyThunkProto* DeviceToHostCopyThunkProto::release_copy_thunk() {
  
  ::xla::gpu::CopyThunkProto* temp = _impl_.copy_thunk_;
  _impl_.copy_thunk_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::gpu::CopyThunkProto* DeviceToHostCopyThunkProto::unsafe_arena_release_copy_thunk() {
  // @@protoc_insertion_point(field_release:xla.gpu.DeviceToHostCopyThunkProto.copy_thunk)
  
  ::xla::gpu::CopyThunkProto* temp = _impl_.copy_thunk_;
  _impl_.copy_thunk_ = nullptr;
  return temp;
}
inline ::xla::gpu::CopyThunkProto* DeviceToHostCopyThunkProto::_internal_mutable_copy_thunk() {
  
  if (_impl_.copy_thunk_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::gpu::CopyThunkProto>(GetArenaForAllocation());
    _impl_.copy_thunk_ = p;
  }
  return _impl_.copy_thunk_;
}
inline ::xla::gpu::CopyThunkProto* DeviceToHostCopyThunkProto::mutable_copy_thunk() {
  ::xla::gpu::CopyThunkProto* _msg = _internal_mutable_copy_thunk();
  // @@protoc_insertion_point(field_mutable:xla.gpu.DeviceToHostCopyThunkProto.copy_thunk)
  return _msg;
}
inline void DeviceToHostCopyThunkProto::set_allocated_copy_thunk(::xla::gpu::CopyThunkProto* copy_thunk) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete _impl_.copy_thunk_;
  }
  if (copy_thunk) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(copy_thunk);
    if (message_arena != submessage_arena) {
      copy_thunk = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, copy_thunk, submessage_arena);
    }
    
  } else {
    
  }
  _impl_.copy_thunk_ = copy_thunk;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.DeviceToHostCopyThunkProto.copy_thunk)
}

// -------------------------------------------------------------------

// HostToDeviceCopyThunkProto

// .xla.gpu.CopyThunkProto copy_thunk = 1;
inline bool HostToDeviceCopyThunkProto::_internal_has_copy_thunk() const {
  return this != internal_default_instance() && _impl_.copy_thunk_ != nullptr;
}
inline bool HostToDeviceCopyThunkProto::has_copy_thunk() const {
  return _internal_has_copy_thunk();
}
inline void HostToDeviceCopyThunkProto::clear_copy_thunk() {
  if (GetArenaForAllocation() == nullptr && _impl_.copy_thunk_ != nullptr) {
    delete _impl_.copy_thunk_;
  }
  _impl_.copy_thunk_ = nullptr;
}
inline const ::xla::gpu::CopyThunkProto& HostToDeviceCopyThunkProto::_internal_copy_thunk() const {
  const ::xla::gpu::CopyThunkProto* p = _impl_.copy_thunk_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::gpu::CopyThunkProto&>(
      ::xla::gpu::_CopyThunkProto_default_instance_);
}
inline const ::xla::gpu::CopyThunkProto& HostToDeviceCopyThunkProto::copy_thunk() const {
  // @@protoc_insertion_point(field_get:xla.gpu.HostToDeviceCopyThunkProto.copy_thunk)
  return _internal_copy_thunk();
}
inline void HostToDeviceCopyThunkProto::unsafe_arena_set_allocated_copy_thunk(
    ::xla::gpu::CopyThunkProto* copy_thunk) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.copy_thunk_);
  }
  _impl_.copy_thunk_ = copy_thunk;
  if (copy_thunk) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.HostToDeviceCopyThunkProto.copy_thunk)
}
inline ::xla::gpu::CopyThunkProto* HostToDeviceCopyThunkProto::release_copy_thunk() {
  
  ::xla::gpu::CopyThunkProto* temp = _impl_.copy_thunk_;
  _impl_.copy_thunk_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::gpu::CopyThunkProto* HostToDeviceCopyThunkProto::unsafe_arena_release_copy_thunk() {
  // @@protoc_insertion_point(field_release:xla.gpu.HostToDeviceCopyThunkProto.copy_thunk)
  
  ::xla::gpu::CopyThunkProto* temp = _impl_.copy_thunk_;
  _impl_.copy_thunk_ = nullptr;
  return temp;
}
inline ::xla::gpu::CopyThunkProto* HostToDeviceCopyThunkProto::_internal_mutable_copy_thunk() {
  
  if (_impl_.copy_thunk_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::gpu::CopyThunkProto>(GetArenaForAllocation());
    _impl_.copy_thunk_ = p;
  }
  return _impl_.copy_thunk_;
}
inline ::xla::gpu::CopyThunkProto* HostToDeviceCopyThunkProto::mutable_copy_thunk() {
  ::xla::gpu::CopyThunkProto* _msg = _internal_mutable_copy_thunk();
  // @@protoc_insertion_point(field_mutable:xla.gpu.HostToDeviceCopyThunkProto.copy_thunk)
  return _msg;
}
inline void HostToDeviceCopyThunkProto::set_allocated_copy_thunk(::xla::gpu::CopyThunkProto* copy_thunk) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete _impl_.copy_thunk_;
  }
  if (copy_thunk) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(copy_thunk);
    if (message_arena != submessage_arena) {
      copy_thunk = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, copy_thunk, submessage_arena);
    }
    
  } else {
    
  }
  _impl_.copy_thunk_ = copy_thunk;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.HostToDeviceCopyThunkProto.copy_thunk)
}

// -------------------------------------------------------------------

// ConditionalThunkProto

// .xla.buffer_assignment.BufferAllocationSliceProto branch_index_buffer = 1;
inline bool ConditionalThunkProto::_internal_has_branch_index_buffer() const {
  return this != internal_default_instance() && _impl_.branch_index_buffer_ != nullptr;
}
inline bool ConditionalThunkProto::has_branch_index_buffer() const {
  return _internal_has_branch_index_buffer();
}
inline const ::xla::buffer_assignment::BufferAllocationSliceProto& ConditionalThunkProto::_internal_branch_index_buffer() const {
  const ::xla::buffer_assignment::BufferAllocationSliceProto* p = _impl_.branch_index_buffer_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::buffer_assignment::BufferAllocationSliceProto&>(
      ::xla::buffer_assignment::_BufferAllocationSliceProto_default_instance_);
}
inline const ::xla::buffer_assignment::BufferAllocationSliceProto& ConditionalThunkProto::branch_index_buffer() const {
  // @@protoc_insertion_point(field_get:xla.gpu.ConditionalThunkProto.branch_index_buffer)
  return _internal_branch_index_buffer();
}
inline void ConditionalThunkProto::unsafe_arena_set_allocated_branch_index_buffer(
    ::xla::buffer_assignment::BufferAllocationSliceProto* branch_index_buffer) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.branch_index_buffer_);
  }
  _impl_.branch_index_buffer_ = branch_index_buffer;
  if (branch_index_buffer) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.ConditionalThunkProto.branch_index_buffer)
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* ConditionalThunkProto::release_branch_index_buffer() {
  
  ::xla::buffer_assignment::BufferAllocationSliceProto* temp = _impl_.branch_index_buffer_;
  _impl_.branch_index_buffer_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* ConditionalThunkProto::unsafe_arena_release_branch_index_buffer() {
  // @@protoc_insertion_point(field_release:xla.gpu.ConditionalThunkProto.branch_index_buffer)
  
  ::xla::buffer_assignment::BufferAllocationSliceProto* temp = _impl_.branch_index_buffer_;
  _impl_.branch_index_buffer_ = nullptr;
  return temp;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* ConditionalThunkProto::_internal_mutable_branch_index_buffer() {
  
  if (_impl_.branch_index_buffer_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::buffer_assignment::BufferAllocationSliceProto>(GetArenaForAllocation());
    _impl_.branch_index_buffer_ = p;
  }
  return _impl_.branch_index_buffer_;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* ConditionalThunkProto::mutable_branch_index_buffer() {
  ::xla::buffer_assignment::BufferAllocationSliceProto* _msg = _internal_mutable_branch_index_buffer();
  // @@protoc_insertion_point(field_mutable:xla.gpu.ConditionalThunkProto.branch_index_buffer)
  return _msg;
}
inline void ConditionalThunkProto::set_allocated_branch_index_buffer(::xla::buffer_assignment::BufferAllocationSliceProto* branch_index_buffer) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.branch_index_buffer_);
  }
  if (branch_index_buffer) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(branch_index_buffer));
    if (message_arena != submessage_arena) {
      branch_index_buffer = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, branch_index_buffer, submessage_arena);
    }
    
  } else {
    
  }
  _impl_.branch_index_buffer_ = branch_index_buffer;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.ConditionalThunkProto.branch_index_buffer)
}

// repeated .xla.gpu.SequentialThunkProto branch_thunks = 2;
inline int ConditionalThunkProto::_internal_branch_thunks_size() const {
  return _impl_.branch_thunks_.size();
}
inline int ConditionalThunkProto::branch_thunks_size() const {
  return _internal_branch_thunks_size();
}
inline void ConditionalThunkProto::clear_branch_thunks() {
  _impl_.branch_thunks_.Clear();
}
inline ::xla::gpu::SequentialThunkProto* ConditionalThunkProto::mutable_branch_thunks(int index) {
  // @@protoc_insertion_point(field_mutable:xla.gpu.ConditionalThunkProto.branch_thunks)
  return _impl_.branch_thunks_.Mutable(index);
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::gpu::SequentialThunkProto >*
ConditionalThunkProto::mutable_branch_thunks() {
  // @@protoc_insertion_point(field_mutable_list:xla.gpu.ConditionalThunkProto.branch_thunks)
  return &_impl_.branch_thunks_;
}
inline const ::xla::gpu::SequentialThunkProto& ConditionalThunkProto::_internal_branch_thunks(int index) const {
  return _impl_.branch_thunks_.Get(index);
}
inline const ::xla::gpu::SequentialThunkProto& ConditionalThunkProto::branch_thunks(int index) const {
  // @@protoc_insertion_point(field_get:xla.gpu.ConditionalThunkProto.branch_thunks)
  return _internal_branch_thunks(index);
}
inline ::xla::gpu::SequentialThunkProto* ConditionalThunkProto::_internal_add_branch_thunks() {
  return _impl_.branch_thunks_.Add();
}
inline ::xla::gpu::SequentialThunkProto* ConditionalThunkProto::add_branch_thunks() {
  ::xla::gpu::SequentialThunkProto* _add = _internal_add_branch_thunks();
  // @@protoc_insertion_point(field_add:xla.gpu.ConditionalThunkProto.branch_thunks)
  return _add;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::gpu::SequentialThunkProto >&
ConditionalThunkProto::branch_thunks() const {
  // @@protoc_insertion_point(field_list:xla.gpu.ConditionalThunkProto.branch_thunks)
  return _impl_.branch_thunks_;
}

// bool branch_index_is_bool = 3;
inline void ConditionalThunkProto::clear_branch_index_is_bool() {
  _impl_.branch_index_is_bool_ = false;
}
inline bool ConditionalThunkProto::_internal_branch_index_is_bool() const {
  return _impl_.branch_index_is_bool_;
}
inline bool ConditionalThunkProto::branch_index_is_bool() const {
  // @@protoc_insertion_point(field_get:xla.gpu.ConditionalThunkProto.branch_index_is_bool)
  return _internal_branch_index_is_bool();
}
inline void ConditionalThunkProto::_internal_set_branch_index_is_bool(bool value) {
  
  _impl_.branch_index_is_bool_ = value;
}
inline void ConditionalThunkProto::set_branch_index_is_bool(bool value) {
  _internal_set_branch_index_is_bool(value);
  // @@protoc_insertion_point(field_set:xla.gpu.ConditionalThunkProto.branch_index_is_bool)
}

// -------------------------------------------------------------------

// WhileThunkProto

// .xla.buffer_assignment.BufferAllocationSliceProto condition_result_buffer_index = 1;
inline bool WhileThunkProto::_internal_has_condition_result_buffer_index() const {
  return this != internal_default_instance() && _impl_.condition_result_buffer_index_ != nullptr;
}
inline bool WhileThunkProto::has_condition_result_buffer_index() const {
  return _internal_has_condition_result_buffer_index();
}
inline const ::xla::buffer_assignment::BufferAllocationSliceProto& WhileThunkProto::_internal_condition_result_buffer_index() const {
  const ::xla::buffer_assignment::BufferAllocationSliceProto* p = _impl_.condition_result_buffer_index_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::buffer_assignment::BufferAllocationSliceProto&>(
      ::xla::buffer_assignment::_BufferAllocationSliceProto_default_instance_);
}
inline const ::xla::buffer_assignment::BufferAllocationSliceProto& WhileThunkProto::condition_result_buffer_index() const {
  // @@protoc_insertion_point(field_get:xla.gpu.WhileThunkProto.condition_result_buffer_index)
  return _internal_condition_result_buffer_index();
}
inline void WhileThunkProto::unsafe_arena_set_allocated_condition_result_buffer_index(
    ::xla::buffer_assignment::BufferAllocationSliceProto* condition_result_buffer_index) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.condition_result_buffer_index_);
  }
  _impl_.condition_result_buffer_index_ = condition_result_buffer_index;
  if (condition_result_buffer_index) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.WhileThunkProto.condition_result_buffer_index)
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* WhileThunkProto::release_condition_result_buffer_index() {
  
  ::xla::buffer_assignment::BufferAllocationSliceProto* temp = _impl_.condition_result_buffer_index_;
  _impl_.condition_result_buffer_index_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* WhileThunkProto::unsafe_arena_release_condition_result_buffer_index() {
  // @@protoc_insertion_point(field_release:xla.gpu.WhileThunkProto.condition_result_buffer_index)
  
  ::xla::buffer_assignment::BufferAllocationSliceProto* temp = _impl_.condition_result_buffer_index_;
  _impl_.condition_result_buffer_index_ = nullptr;
  return temp;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* WhileThunkProto::_internal_mutable_condition_result_buffer_index() {
  
  if (_impl_.condition_result_buffer_index_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::buffer_assignment::BufferAllocationSliceProto>(GetArenaForAllocation());
    _impl_.condition_result_buffer_index_ = p;
  }
  return _impl_.condition_result_buffer_index_;
}
inline ::xla::buffer_assignment::BufferAllocationSliceProto* WhileThunkProto::mutable_condition_result_buffer_index() {
  ::xla::buffer_assignment::BufferAllocationSliceProto* _msg = _internal_mutable_condition_result_buffer_index();
  // @@protoc_insertion_point(field_mutable:xla.gpu.WhileThunkProto.condition_result_buffer_index)
  return _msg;
}
inline void WhileThunkProto::set_allocated_condition_result_buffer_index(::xla::buffer_assignment::BufferAllocationSliceProto* condition_result_buffer_index) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.condition_result_buffer_index_);
  }
  if (condition_result_buffer_index) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(condition_result_buffer_index));
    if (message_arena != submessage_arena) {
      condition_result_buffer_index = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, condition_result_buffer_index, submessage_arena);
    }
    
  } else {
    
  }
  _impl_.condition_result_buffer_index_ = condition_result_buffer_index;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.WhileThunkProto.condition_result_buffer_index)
}

// .xla.gpu.SequentialThunkProto condition_thunk_sequence = 2;
inline bool WhileThunkProto::_internal_has_condition_thunk_sequence() const {
  return this != internal_default_instance() && _impl_.condition_thunk_sequence_ != nullptr;
}
inline bool WhileThunkProto::has_condition_thunk_sequence() const {
  return _internal_has_condition_thunk_sequence();
}
inline void WhileThunkProto::clear_condition_thunk_sequence() {
  if (GetArenaForAllocation() == nullptr && _impl_.condition_thunk_sequence_ != nullptr) {
    delete _impl_.condition_thunk_sequence_;
  }
  _impl_.condition_thunk_sequence_ = nullptr;
}
inline const ::xla::gpu::SequentialThunkProto& WhileThunkProto::_internal_condition_thunk_sequence() const {
  const ::xla::gpu::SequentialThunkProto* p = _impl_.condition_thunk_sequence_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::gpu::SequentialThunkProto&>(
      ::xla::gpu::_SequentialThunkProto_default_instance_);
}
inline const ::xla::gpu::SequentialThunkProto& WhileThunkProto::condition_thunk_sequence() const {
  // @@protoc_insertion_point(field_get:xla.gpu.WhileThunkProto.condition_thunk_sequence)
  return _internal_condition_thunk_sequence();
}
inline void WhileThunkProto::unsafe_arena_set_allocated_condition_thunk_sequence(
    ::xla::gpu::SequentialThunkProto* condition_thunk_sequence) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.condition_thunk_sequence_);
  }
  _impl_.condition_thunk_sequence_ = condition_thunk_sequence;
  if (condition_thunk_sequence) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.WhileThunkProto.condition_thunk_sequence)
}
inline ::xla::gpu::SequentialThunkProto* WhileThunkProto::release_condition_thunk_sequence() {
  
  ::xla::gpu::SequentialThunkProto* temp = _impl_.condition_thunk_sequence_;
  _impl_.condition_thunk_sequence_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::gpu::SequentialThunkProto* WhileThunkProto::unsafe_arena_release_condition_thunk_sequence() {
  // @@protoc_insertion_point(field_release:xla.gpu.WhileThunkProto.condition_thunk_sequence)
  
  ::xla::gpu::SequentialThunkProto* temp = _impl_.condition_thunk_sequence_;
  _impl_.condition_thunk_sequence_ = nullptr;
  return temp;
}
inline ::xla::gpu::SequentialThunkProto* WhileThunkProto::_internal_mutable_condition_thunk_sequence() {
  
  if (_impl_.condition_thunk_sequence_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::gpu::SequentialThunkProto>(GetArenaForAllocation());
    _impl_.condition_thunk_sequence_ = p;
  }
  return _impl_.condition_thunk_sequence_;
}
inline ::xla::gpu::SequentialThunkProto* WhileThunkProto::mutable_condition_thunk_sequence() {
  ::xla::gpu::SequentialThunkProto* _msg = _internal_mutable_condition_thunk_sequence();
  // @@protoc_insertion_point(field_mutable:xla.gpu.WhileThunkProto.condition_thunk_sequence)
  return _msg;
}
inline void WhileThunkProto::set_allocated_condition_thunk_sequence(::xla::gpu::SequentialThunkProto* condition_thunk_sequence) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete _impl_.condition_thunk_sequence_;
  }
  if (condition_thunk_sequence) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(condition_thunk_sequence);
    if (message_arena != submessage_arena) {
      condition_thunk_sequence = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, condition_thunk_sequence, submessage_arena);
    }
    
  } else {
    
  }
  _impl_.condition_thunk_sequence_ = condition_thunk_sequence;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.WhileThunkProto.condition_thunk_sequence)
}

// .xla.gpu.SequentialThunkProto body_thunk_sequence = 3;
inline bool WhileThunkProto::_internal_has_body_thunk_sequence() const {
  return this != internal_default_instance() && _impl_.body_thunk_sequence_ != nullptr;
}
inline bool WhileThunkProto::has_body_thunk_sequence() const {
  return _internal_has_body_thunk_sequence();
}
inline void WhileThunkProto::clear_body_thunk_sequence() {
  if (GetArenaForAllocation() == nullptr && _impl_.body_thunk_sequence_ != nullptr) {
    delete _impl_.body_thunk_sequence_;
  }
  _impl_.body_thunk_sequence_ = nullptr;
}
inline const ::xla::gpu::SequentialThunkProto& WhileThunkProto::_internal_body_thunk_sequence() const {
  const ::xla::gpu::SequentialThunkProto* p = _impl_.body_thunk_sequence_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::gpu::SequentialThunkProto&>(
      ::xla::gpu::_SequentialThunkProto_default_instance_);
}
inline const ::xla::gpu::SequentialThunkProto& WhileThunkProto::body_thunk_sequence() const {
  // @@protoc_insertion_point(field_get:xla.gpu.WhileThunkProto.body_thunk_sequence)
  return _internal_body_thunk_sequence();
}
inline void WhileThunkProto::unsafe_arena_set_allocated_body_thunk_sequence(
    ::xla::gpu::SequentialThunkProto* body_thunk_sequence) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.body_thunk_sequence_);
  }
  _impl_.body_thunk_sequence_ = body_thunk_sequence;
  if (body_thunk_sequence) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.WhileThunkProto.body_thunk_sequence)
}
inline ::xla::gpu::SequentialThunkProto* WhileThunkProto::release_body_thunk_sequence() {
  
  ::xla::gpu::SequentialThunkProto* temp = _impl_.body_thunk_sequence_;
  _impl_.body_thunk_sequence_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::gpu::SequentialThunkProto* WhileThunkProto::unsafe_arena_release_body_thunk_sequence() {
  // @@protoc_insertion_point(field_release:xla.gpu.WhileThunkProto.body_thunk_sequence)
  
  ::xla::gpu::SequentialThunkProto* temp = _impl_.body_thunk_sequence_;
  _impl_.body_thunk_sequence_ = nullptr;
  return temp;
}
inline ::xla::gpu::SequentialThunkProto* WhileThunkProto::_internal_mutable_body_thunk_sequence() {
  
  if (_impl_.body_thunk_sequence_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::gpu::SequentialThunkProto>(GetArenaForAllocation());
    _impl_.body_thunk_sequence_ = p;
  }
  return _impl_.body_thunk_sequence_;
}
inline ::xla::gpu::SequentialThunkProto* WhileThunkProto::mutable_body_thunk_sequence() {
  ::xla::gpu::SequentialThunkProto* _msg = _internal_mutable_body_thunk_sequence();
  // @@protoc_insertion_point(field_mutable:xla.gpu.WhileThunkProto.body_thunk_sequence)
  return _msg;
}
inline void WhileThunkProto::set_allocated_body_thunk_sequence(::xla::gpu::SequentialThunkProto* body_thunk_sequence) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete _impl_.body_thunk_sequence_;
  }
  if (body_thunk_sequence) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(body_thunk_sequence);
    if (message_arena != submessage_arena) {
      body_thunk_sequence = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, body_thunk_sequence, submessage_arena);
    }
    
  } else {
    
  }
  _impl_.body_thunk_sequence_ = body_thunk_sequence;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.WhileThunkProto.body_thunk_sequence)
}

// optional int64 trip_count = 4;
inline bool WhileThunkProto::_internal_has_trip_count() const {
  bool value = (_impl_._has_bits_[0] & 0x00000001u) != 0;
  return value;
}
inline bool WhileThunkProto::has_trip_count() const {
  return _internal_has_trip_count();
}
inline void WhileThunkProto::clear_trip_count() {
  _impl_.trip_count_ = int64_t{0};
  _impl_._has_bits_[0] &= ~0x00000001u;
}
inline int64_t WhileThunkProto::_internal_trip_count() const {
  return _impl_.trip_count_;
}
inline int64_t WhileThunkProto::trip_count() const {
  // @@protoc_insertion_point(field_get:xla.gpu.WhileThunkProto.trip_count)
  return _internal_trip_count();
}
inline void WhileThunkProto::_internal_set_trip_count(int64_t value) {
  _impl_._has_bits_[0] |= 0x00000001u;
  _impl_.trip_count_ = value;
}
inline void WhileThunkProto::set_trip_count(int64_t value) {
  _internal_set_trip_count(value);
  // @@protoc_insertion_point(field_set:xla.gpu.WhileThunkProto.trip_count)
}

// -------------------------------------------------------------------

// ThunkProto

// .xla.gpu.ThunkInfoProto thunk_info = 1;
inline bool ThunkProto::_internal_has_thunk_info() const {
  return this != internal_default_instance() && _impl_.thunk_info_ != nullptr;
}
inline bool ThunkProto::has_thunk_info() const {
  return _internal_has_thunk_info();
}
inline void ThunkProto::clear_thunk_info() {
  if (GetArenaForAllocation() == nullptr && _impl_.thunk_info_ != nullptr) {
    delete _impl_.thunk_info_;
  }
  _impl_.thunk_info_ = nullptr;
}
inline const ::xla::gpu::ThunkInfoProto& ThunkProto::_internal_thunk_info() const {
  const ::xla::gpu::ThunkInfoProto* p = _impl_.thunk_info_;
  return p != nullptr ? *p : reinterpret_cast<const ::xla::gpu::ThunkInfoProto&>(
      ::xla::gpu::_ThunkInfoProto_default_instance_);
}
inline const ::xla::gpu::ThunkInfoProto& ThunkProto::thunk_info() const {
  // @@protoc_insertion_point(field_get:xla.gpu.ThunkProto.thunk_info)
  return _internal_thunk_info();
}
inline void ThunkProto::unsafe_arena_set_allocated_thunk_info(
    ::xla::gpu::ThunkInfoProto* thunk_info) {
  if (GetArenaForAllocation() == nullptr) {
    delete reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(_impl_.thunk_info_);
  }
  _impl_.thunk_info_ = thunk_info;
  if (thunk_info) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.ThunkProto.thunk_info)
}
inline ::xla::gpu::ThunkInfoProto* ThunkProto::release_thunk_info() {
  
  ::xla::gpu::ThunkInfoProto* temp = _impl_.thunk_info_;
  _impl_.thunk_info_ = nullptr;
#ifdef PROTOBUF_FORCE_COPY_IN_RELEASE
  auto* old =  reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(temp);
  temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  if (GetArenaForAllocation() == nullptr) { delete old; }
#else  // PROTOBUF_FORCE_COPY_IN_RELEASE
  if (GetArenaForAllocation() != nullptr) {
    temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
  }
#endif  // !PROTOBUF_FORCE_COPY_IN_RELEASE
  return temp;
}
inline ::xla::gpu::ThunkInfoProto* ThunkProto::unsafe_arena_release_thunk_info() {
  // @@protoc_insertion_point(field_release:xla.gpu.ThunkProto.thunk_info)
  
  ::xla::gpu::ThunkInfoProto* temp = _impl_.thunk_info_;
  _impl_.thunk_info_ = nullptr;
  return temp;
}
inline ::xla::gpu::ThunkInfoProto* ThunkProto::_internal_mutable_thunk_info() {
  
  if (_impl_.thunk_info_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::gpu::ThunkInfoProto>(GetArenaForAllocation());
    _impl_.thunk_info_ = p;
  }
  return _impl_.thunk_info_;
}
inline ::xla::gpu::ThunkInfoProto* ThunkProto::mutable_thunk_info() {
  ::xla::gpu::ThunkInfoProto* _msg = _internal_mutable_thunk_info();
  // @@protoc_insertion_point(field_mutable:xla.gpu.ThunkProto.thunk_info)
  return _msg;
}
inline void ThunkProto::set_allocated_thunk_info(::xla::gpu::ThunkInfoProto* thunk_info) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  if (message_arena == nullptr) {
    delete _impl_.thunk_info_;
  }
  if (thunk_info) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(thunk_info);
    if (message_arena != submessage_arena) {
      thunk_info = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, thunk_info, submessage_arena);
    }
    
  } else {
    
  }
  _impl_.thunk_info_ = thunk_info;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.ThunkProto.thunk_info)
}

// .xla.gpu.SequentialThunkProto sequential_thunk = 2;
inline bool ThunkProto::_internal_has_sequential_thunk() const {
  return impl_case() == kSequentialThunk;
}
inline bool ThunkProto::has_sequential_thunk() const {
  return _internal_has_sequential_thunk();
}
inline void ThunkProto::set_has_sequential_thunk() {
  _impl_._oneof_case_[0] = kSequentialThunk;
}
inline void ThunkProto::clear_sequential_thunk() {
  if (_internal_has_sequential_thunk()) {
    if (GetArenaForAllocation() == nullptr) {
      delete _impl_.impl_.sequential_thunk_;
    }
    clear_has_impl();
  }
}
inline ::xla::gpu::SequentialThunkProto* ThunkProto::release_sequential_thunk() {
  // @@protoc_insertion_point(field_release:xla.gpu.ThunkProto.sequential_thunk)
  if (_internal_has_sequential_thunk()) {
    clear_has_impl();
    ::xla::gpu::SequentialThunkProto* temp = _impl_.impl_.sequential_thunk_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    _impl_.impl_.sequential_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::xla::gpu::SequentialThunkProto& ThunkProto::_internal_sequential_thunk() const {
  return _internal_has_sequential_thunk()
      ? *_impl_.impl_.sequential_thunk_
      : reinterpret_cast< ::xla::gpu::SequentialThunkProto&>(::xla::gpu::_SequentialThunkProto_default_instance_);
}
inline const ::xla::gpu::SequentialThunkProto& ThunkProto::sequential_thunk() const {
  // @@protoc_insertion_point(field_get:xla.gpu.ThunkProto.sequential_thunk)
  return _internal_sequential_thunk();
}
inline ::xla::gpu::SequentialThunkProto* ThunkProto::unsafe_arena_release_sequential_thunk() {
  // @@protoc_insertion_point(field_unsafe_arena_release:xla.gpu.ThunkProto.sequential_thunk)
  if (_internal_has_sequential_thunk()) {
    clear_has_impl();
    ::xla::gpu::SequentialThunkProto* temp = _impl_.impl_.sequential_thunk_;
    _impl_.impl_.sequential_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ThunkProto::unsafe_arena_set_allocated_sequential_thunk(::xla::gpu::SequentialThunkProto* sequential_thunk) {
  clear_impl();
  if (sequential_thunk) {
    set_has_sequential_thunk();
    _impl_.impl_.sequential_thunk_ = sequential_thunk;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.ThunkProto.sequential_thunk)
}
inline ::xla::gpu::SequentialThunkProto* ThunkProto::_internal_mutable_sequential_thunk() {
  if (!_internal_has_sequential_thunk()) {
    clear_impl();
    set_has_sequential_thunk();
    _impl_.impl_.sequential_thunk_ = CreateMaybeMessage< ::xla::gpu::SequentialThunkProto >(GetArenaForAllocation());
  }
  return _impl_.impl_.sequential_thunk_;
}
inline ::xla::gpu::SequentialThunkProto* ThunkProto::mutable_sequential_thunk() {
  ::xla::gpu::SequentialThunkProto* _msg = _internal_mutable_sequential_thunk();
  // @@protoc_insertion_point(field_mutable:xla.gpu.ThunkProto.sequential_thunk)
  return _msg;
}

// .xla.gpu.CopyThunkProto copy_thunk = 3;
inline bool ThunkProto::_internal_has_copy_thunk() const {
  return impl_case() == kCopyThunk;
}
inline bool ThunkProto::has_copy_thunk() const {
  return _internal_has_copy_thunk();
}
inline void ThunkProto::set_has_copy_thunk() {
  _impl_._oneof_case_[0] = kCopyThunk;
}
inline void ThunkProto::clear_copy_thunk() {
  if (_internal_has_copy_thunk()) {
    if (GetArenaForAllocation() == nullptr) {
      delete _impl_.impl_.copy_thunk_;
    }
    clear_has_impl();
  }
}
inline ::xla::gpu::CopyThunkProto* ThunkProto::release_copy_thunk() {
  // @@protoc_insertion_point(field_release:xla.gpu.ThunkProto.copy_thunk)
  if (_internal_has_copy_thunk()) {
    clear_has_impl();
    ::xla::gpu::CopyThunkProto* temp = _impl_.impl_.copy_thunk_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    _impl_.impl_.copy_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::xla::gpu::CopyThunkProto& ThunkProto::_internal_copy_thunk() const {
  return _internal_has_copy_thunk()
      ? *_impl_.impl_.copy_thunk_
      : reinterpret_cast< ::xla::gpu::CopyThunkProto&>(::xla::gpu::_CopyThunkProto_default_instance_);
}
inline const ::xla::gpu::CopyThunkProto& ThunkProto::copy_thunk() const {
  // @@protoc_insertion_point(field_get:xla.gpu.ThunkProto.copy_thunk)
  return _internal_copy_thunk();
}
inline ::xla::gpu::CopyThunkProto* ThunkProto::unsafe_arena_release_copy_thunk() {
  // @@protoc_insertion_point(field_unsafe_arena_release:xla.gpu.ThunkProto.copy_thunk)
  if (_internal_has_copy_thunk()) {
    clear_has_impl();
    ::xla::gpu::CopyThunkProto* temp = _impl_.impl_.copy_thunk_;
    _impl_.impl_.copy_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ThunkProto::unsafe_arena_set_allocated_copy_thunk(::xla::gpu::CopyThunkProto* copy_thunk) {
  clear_impl();
  if (copy_thunk) {
    set_has_copy_thunk();
    _impl_.impl_.copy_thunk_ = copy_thunk;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.ThunkProto.copy_thunk)
}
inline ::xla::gpu::CopyThunkProto* ThunkProto::_internal_mutable_copy_thunk() {
  if (!_internal_has_copy_thunk()) {
    clear_impl();
    set_has_copy_thunk();
    _impl_.impl_.copy_thunk_ = CreateMaybeMessage< ::xla::gpu::CopyThunkProto >(GetArenaForAllocation());
  }
  return _impl_.impl_.copy_thunk_;
}
inline ::xla::gpu::CopyThunkProto* ThunkProto::mutable_copy_thunk() {
  ::xla::gpu::CopyThunkProto* _msg = _internal_mutable_copy_thunk();
  // @@protoc_insertion_point(field_mutable:xla.gpu.ThunkProto.copy_thunk)
  return _msg;
}

// .xla.gpu.DeviceToHostCopyThunkProto device_to_host_copy_thunk = 4;
inline bool ThunkProto::_internal_has_device_to_host_copy_thunk() const {
  return impl_case() == kDeviceToHostCopyThunk;
}
inline bool ThunkProto::has_device_to_host_copy_thunk() const {
  return _internal_has_device_to_host_copy_thunk();
}
inline void ThunkProto::set_has_device_to_host_copy_thunk() {
  _impl_._oneof_case_[0] = kDeviceToHostCopyThunk;
}
inline void ThunkProto::clear_device_to_host_copy_thunk() {
  if (_internal_has_device_to_host_copy_thunk()) {
    if (GetArenaForAllocation() == nullptr) {
      delete _impl_.impl_.device_to_host_copy_thunk_;
    }
    clear_has_impl();
  }
}
inline ::xla::gpu::DeviceToHostCopyThunkProto* ThunkProto::release_device_to_host_copy_thunk() {
  // @@protoc_insertion_point(field_release:xla.gpu.ThunkProto.device_to_host_copy_thunk)
  if (_internal_has_device_to_host_copy_thunk()) {
    clear_has_impl();
    ::xla::gpu::DeviceToHostCopyThunkProto* temp = _impl_.impl_.device_to_host_copy_thunk_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    _impl_.impl_.device_to_host_copy_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::xla::gpu::DeviceToHostCopyThunkProto& ThunkProto::_internal_device_to_host_copy_thunk() const {
  return _internal_has_device_to_host_copy_thunk()
      ? *_impl_.impl_.device_to_host_copy_thunk_
      : reinterpret_cast< ::xla::gpu::DeviceToHostCopyThunkProto&>(::xla::gpu::_DeviceToHostCopyThunkProto_default_instance_);
}
inline const ::xla::gpu::DeviceToHostCopyThunkProto& ThunkProto::device_to_host_copy_thunk() const {
  // @@protoc_insertion_point(field_get:xla.gpu.ThunkProto.device_to_host_copy_thunk)
  return _internal_device_to_host_copy_thunk();
}
inline ::xla::gpu::DeviceToHostCopyThunkProto* ThunkProto::unsafe_arena_release_device_to_host_copy_thunk() {
  // @@protoc_insertion_point(field_unsafe_arena_release:xla.gpu.ThunkProto.device_to_host_copy_thunk)
  if (_internal_has_device_to_host_copy_thunk()) {
    clear_has_impl();
    ::xla::gpu::DeviceToHostCopyThunkProto* temp = _impl_.impl_.device_to_host_copy_thunk_;
    _impl_.impl_.device_to_host_copy_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ThunkProto::unsafe_arena_set_allocated_device_to_host_copy_thunk(::xla::gpu::DeviceToHostCopyThunkProto* device_to_host_copy_thunk) {
  clear_impl();
  if (device_to_host_copy_thunk) {
    set_has_device_to_host_copy_thunk();
    _impl_.impl_.device_to_host_copy_thunk_ = device_to_host_copy_thunk;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.ThunkProto.device_to_host_copy_thunk)
}
inline ::xla::gpu::DeviceToHostCopyThunkProto* ThunkProto::_internal_mutable_device_to_host_copy_thunk() {
  if (!_internal_has_device_to_host_copy_thunk()) {
    clear_impl();
    set_has_device_to_host_copy_thunk();
    _impl_.impl_.device_to_host_copy_thunk_ = CreateMaybeMessage< ::xla::gpu::DeviceToHostCopyThunkProto >(GetArenaForAllocation());
  }
  return _impl_.impl_.device_to_host_copy_thunk_;
}
inline ::xla::gpu::DeviceToHostCopyThunkProto* ThunkProto::mutable_device_to_host_copy_thunk() {
  ::xla::gpu::DeviceToHostCopyThunkProto* _msg = _internal_mutable_device_to_host_copy_thunk();
  // @@protoc_insertion_point(field_mutable:xla.gpu.ThunkProto.device_to_host_copy_thunk)
  return _msg;
}

// .xla.gpu.HostToDeviceCopyThunkProto host_to_device_copy_thunk = 5;
inline bool ThunkProto::_internal_has_host_to_device_copy_thunk() const {
  return impl_case() == kHostToDeviceCopyThunk;
}
inline bool ThunkProto::has_host_to_device_copy_thunk() const {
  return _internal_has_host_to_device_copy_thunk();
}
inline void ThunkProto::set_has_host_to_device_copy_thunk() {
  _impl_._oneof_case_[0] = kHostToDeviceCopyThunk;
}
inline void ThunkProto::clear_host_to_device_copy_thunk() {
  if (_internal_has_host_to_device_copy_thunk()) {
    if (GetArenaForAllocation() == nullptr) {
      delete _impl_.impl_.host_to_device_copy_thunk_;
    }
    clear_has_impl();
  }
}
inline ::xla::gpu::HostToDeviceCopyThunkProto* ThunkProto::release_host_to_device_copy_thunk() {
  // @@protoc_insertion_point(field_release:xla.gpu.ThunkProto.host_to_device_copy_thunk)
  if (_internal_has_host_to_device_copy_thunk()) {
    clear_has_impl();
    ::xla::gpu::HostToDeviceCopyThunkProto* temp = _impl_.impl_.host_to_device_copy_thunk_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    _impl_.impl_.host_to_device_copy_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::xla::gpu::HostToDeviceCopyThunkProto& ThunkProto::_internal_host_to_device_copy_thunk() const {
  return _internal_has_host_to_device_copy_thunk()
      ? *_impl_.impl_.host_to_device_copy_thunk_
      : reinterpret_cast< ::xla::gpu::HostToDeviceCopyThunkProto&>(::xla::gpu::_HostToDeviceCopyThunkProto_default_instance_);
}
inline const ::xla::gpu::HostToDeviceCopyThunkProto& ThunkProto::host_to_device_copy_thunk() const {
  // @@protoc_insertion_point(field_get:xla.gpu.ThunkProto.host_to_device_copy_thunk)
  return _internal_host_to_device_copy_thunk();
}
inline ::xla::gpu::HostToDeviceCopyThunkProto* ThunkProto::unsafe_arena_release_host_to_device_copy_thunk() {
  // @@protoc_insertion_point(field_unsafe_arena_release:xla.gpu.ThunkProto.host_to_device_copy_thunk)
  if (_internal_has_host_to_device_copy_thunk()) {
    clear_has_impl();
    ::xla::gpu::HostToDeviceCopyThunkProto* temp = _impl_.impl_.host_to_device_copy_thunk_;
    _impl_.impl_.host_to_device_copy_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ThunkProto::unsafe_arena_set_allocated_host_to_device_copy_thunk(::xla::gpu::HostToDeviceCopyThunkProto* host_to_device_copy_thunk) {
  clear_impl();
  if (host_to_device_copy_thunk) {
    set_has_host_to_device_copy_thunk();
    _impl_.impl_.host_to_device_copy_thunk_ = host_to_device_copy_thunk;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.ThunkProto.host_to_device_copy_thunk)
}
inline ::xla::gpu::HostToDeviceCopyThunkProto* ThunkProto::_internal_mutable_host_to_device_copy_thunk() {
  if (!_internal_has_host_to_device_copy_thunk()) {
    clear_impl();
    set_has_host_to_device_copy_thunk();
    _impl_.impl_.host_to_device_copy_thunk_ = CreateMaybeMessage< ::xla::gpu::HostToDeviceCopyThunkProto >(GetArenaForAllocation());
  }
  return _impl_.impl_.host_to_device_copy_thunk_;
}
inline ::xla::gpu::HostToDeviceCopyThunkProto* ThunkProto::mutable_host_to_device_copy_thunk() {
  ::xla::gpu::HostToDeviceCopyThunkProto* _msg = _internal_mutable_host_to_device_copy_thunk();
  // @@protoc_insertion_point(field_mutable:xla.gpu.ThunkProto.host_to_device_copy_thunk)
  return _msg;
}

// .xla.gpu.ConditionalThunkProto conditional_thunk = 6;
inline bool ThunkProto::_internal_has_conditional_thunk() const {
  return impl_case() == kConditionalThunk;
}
inline bool ThunkProto::has_conditional_thunk() const {
  return _internal_has_conditional_thunk();
}
inline void ThunkProto::set_has_conditional_thunk() {
  _impl_._oneof_case_[0] = kConditionalThunk;
}
inline void ThunkProto::clear_conditional_thunk() {
  if (_internal_has_conditional_thunk()) {
    if (GetArenaForAllocation() == nullptr) {
      delete _impl_.impl_.conditional_thunk_;
    }
    clear_has_impl();
  }
}
inline ::xla::gpu::ConditionalThunkProto* ThunkProto::release_conditional_thunk() {
  // @@protoc_insertion_point(field_release:xla.gpu.ThunkProto.conditional_thunk)
  if (_internal_has_conditional_thunk()) {
    clear_has_impl();
    ::xla::gpu::ConditionalThunkProto* temp = _impl_.impl_.conditional_thunk_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    _impl_.impl_.conditional_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::xla::gpu::ConditionalThunkProto& ThunkProto::_internal_conditional_thunk() const {
  return _internal_has_conditional_thunk()
      ? *_impl_.impl_.conditional_thunk_
      : reinterpret_cast< ::xla::gpu::ConditionalThunkProto&>(::xla::gpu::_ConditionalThunkProto_default_instance_);
}
inline const ::xla::gpu::ConditionalThunkProto& ThunkProto::conditional_thunk() const {
  // @@protoc_insertion_point(field_get:xla.gpu.ThunkProto.conditional_thunk)
  return _internal_conditional_thunk();
}
inline ::xla::gpu::ConditionalThunkProto* ThunkProto::unsafe_arena_release_conditional_thunk() {
  // @@protoc_insertion_point(field_unsafe_arena_release:xla.gpu.ThunkProto.conditional_thunk)
  if (_internal_has_conditional_thunk()) {
    clear_has_impl();
    ::xla::gpu::ConditionalThunkProto* temp = _impl_.impl_.conditional_thunk_;
    _impl_.impl_.conditional_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ThunkProto::unsafe_arena_set_allocated_conditional_thunk(::xla::gpu::ConditionalThunkProto* conditional_thunk) {
  clear_impl();
  if (conditional_thunk) {
    set_has_conditional_thunk();
    _impl_.impl_.conditional_thunk_ = conditional_thunk;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.ThunkProto.conditional_thunk)
}
inline ::xla::gpu::ConditionalThunkProto* ThunkProto::_internal_mutable_conditional_thunk() {
  if (!_internal_has_conditional_thunk()) {
    clear_impl();
    set_has_conditional_thunk();
    _impl_.impl_.conditional_thunk_ = CreateMaybeMessage< ::xla::gpu::ConditionalThunkProto >(GetArenaForAllocation());
  }
  return _impl_.impl_.conditional_thunk_;
}
inline ::xla::gpu::ConditionalThunkProto* ThunkProto::mutable_conditional_thunk() {
  ::xla::gpu::ConditionalThunkProto* _msg = _internal_mutable_conditional_thunk();
  // @@protoc_insertion_point(field_mutable:xla.gpu.ThunkProto.conditional_thunk)
  return _msg;
}

// .xla.gpu.WhileThunkProto while_thunk = 7;
inline bool ThunkProto::_internal_has_while_thunk() const {
  return impl_case() == kWhileThunk;
}
inline bool ThunkProto::has_while_thunk() const {
  return _internal_has_while_thunk();
}
inline void ThunkProto::set_has_while_thunk() {
  _impl_._oneof_case_[0] = kWhileThunk;
}
inline void ThunkProto::clear_while_thunk() {
  if (_internal_has_while_thunk()) {
    if (GetArenaForAllocation() == nullptr) {
      delete _impl_.impl_.while_thunk_;
    }
    clear_has_impl();
  }
}
inline ::xla::gpu::WhileThunkProto* ThunkProto::release_while_thunk() {
  // @@protoc_insertion_point(field_release:xla.gpu.ThunkProto.while_thunk)
  if (_internal_has_while_thunk()) {
    clear_has_impl();
    ::xla::gpu::WhileThunkProto* temp = _impl_.impl_.while_thunk_;
    if (GetArenaForAllocation() != nullptr) {
      temp = ::PROTOBUF_NAMESPACE_ID::internal::DuplicateIfNonNull(temp);
    }
    _impl_.impl_.while_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::xla::gpu::WhileThunkProto& ThunkProto::_internal_while_thunk() const {
  return _internal_has_while_thunk()
      ? *_impl_.impl_.while_thunk_
      : reinterpret_cast< ::xla::gpu::WhileThunkProto&>(::xla::gpu::_WhileThunkProto_default_instance_);
}
inline const ::xla::gpu::WhileThunkProto& ThunkProto::while_thunk() const {
  // @@protoc_insertion_point(field_get:xla.gpu.ThunkProto.while_thunk)
  return _internal_while_thunk();
}
inline ::xla::gpu::WhileThunkProto* ThunkProto::unsafe_arena_release_while_thunk() {
  // @@protoc_insertion_point(field_unsafe_arena_release:xla.gpu.ThunkProto.while_thunk)
  if (_internal_has_while_thunk()) {
    clear_has_impl();
    ::xla::gpu::WhileThunkProto* temp = _impl_.impl_.while_thunk_;
    _impl_.impl_.while_thunk_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void ThunkProto::unsafe_arena_set_allocated_while_thunk(::xla::gpu::WhileThunkProto* while_thunk) {
  clear_impl();
  if (while_thunk) {
    set_has_while_thunk();
    _impl_.impl_.while_thunk_ = while_thunk;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:xla.gpu.ThunkProto.while_thunk)
}
inline ::xla::gpu::WhileThunkProto* ThunkProto::_internal_mutable_while_thunk() {
  if (!_internal_has_while_thunk()) {
    clear_impl();
    set_has_while_thunk();
    _impl_.impl_.while_thunk_ = CreateMaybeMessage< ::xla::gpu::WhileThunkProto >(GetArenaForAllocation());
  }
  return _impl_.impl_.while_thunk_;
}
inline ::xla::gpu::WhileThunkProto* ThunkProto::mutable_while_thunk() {
  ::xla::gpu::WhileThunkProto* _msg = _internal_mutable_while_thunk();
  // @@protoc_insertion_point(field_mutable:xla.gpu.ThunkProto.while_thunk)
  return _msg;
}

inline bool ThunkProto::has_impl() const {
  return impl_case() != IMPL_NOT_SET;
}
inline void ThunkProto::clear_has_impl() {
  _impl_._oneof_case_[0] = IMPL_NOT_SET;
}
inline ThunkProto::ImplCase ThunkProto::impl_case() const {
  return ThunkProto::ImplCase(_impl_._oneof_case_[0]);
}
// -------------------------------------------------------------------

// SequentialThunkProto

// repeated .xla.gpu.ThunkProto thunks = 1;
inline int SequentialThunkProto::_internal_thunks_size() const {
  return _impl_.thunks_.size();
}
inline int SequentialThunkProto::thunks_size() const {
  return _internal_thunks_size();
}
inline void SequentialThunkProto::clear_thunks() {
  _impl_.thunks_.Clear();
}
inline ::xla::gpu::ThunkProto* SequentialThunkProto::mutable_thunks(int index) {
  // @@protoc_insertion_point(field_mutable:xla.gpu.SequentialThunkProto.thunks)
  return _impl_.thunks_.Mutable(index);
}
inline ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::gpu::ThunkProto >*
SequentialThunkProto::mutable_thunks() {
  // @@protoc_insertion_point(field_mutable_list:xla.gpu.SequentialThunkProto.thunks)
  return &_impl_.thunks_;
}
inline const ::xla::gpu::ThunkProto& SequentialThunkProto::_internal_thunks(int index) const {
  return _impl_.thunks_.Get(index);
}
inline const ::xla::gpu::ThunkProto& SequentialThunkProto::thunks(int index) const {
  // @@protoc_insertion_point(field_get:xla.gpu.SequentialThunkProto.thunks)
  return _internal_thunks(index);
}
inline ::xla::gpu::ThunkProto* SequentialThunkProto::_internal_add_thunks() {
  return _impl_.thunks_.Add();
}
inline ::xla::gpu::ThunkProto* SequentialThunkProto::add_thunks() {
  ::xla::gpu::ThunkProto* _add = _internal_add_thunks();
  // @@protoc_insertion_point(field_add:xla.gpu.SequentialThunkProto.thunks)
  return _add;
}
inline const ::PROTOBUF_NAMESPACE_ID::RepeatedPtrField< ::xla::gpu::ThunkProto >&
SequentialThunkProto::thunks() const {
  // @@protoc_insertion_point(field_list:xla.gpu.SequentialThunkProto.thunks)
  return _impl_.thunks_;
}

#ifdef __GNUC__
  #pragma GCC diagnostic pop
#endif  // __GNUC__
// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------


// @@protoc_insertion_point(namespace_scope)

}  // namespace gpu
}  // namespace xla

// @@protoc_insertion_point(global_scope)

#include <google/protobuf/port_undef.inc>
#endif  // GOOGLE_PROTOBUF_INCLUDED_GOOGLE_PROTOBUF_INCLUDED_xla_2fbackends_2fgpu_2fruntime_2fthunk_2eproto

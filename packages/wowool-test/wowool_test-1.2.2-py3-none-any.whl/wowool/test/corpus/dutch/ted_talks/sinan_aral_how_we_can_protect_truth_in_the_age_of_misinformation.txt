Op 23 april 2013 plaatste de Associated Press de volgende tweet op Twitter: "Breaking news: Twee explosies in het Witte Huis en Barack Obama is gewond." Deze tweet werd 4000 keer geretweet in minder dan vijf minuten en ging daarna viraal. 
De tweet was echter geen echt nieuws gepubliceerd door de Associated Press. Het was nepnieuws, of 'fake news', verspreid door Syrische hackers die de Twitter-account van de Associated Press hadden geïnfiltreerd. Ze wilden de samenleving ontwrichten, maar ze ontwrichtten veel meer dan dat. Omdat geautomatiseerde tradingalgoritmes meteen reageerden op het sentiment dat door deze tweet ontstond en begonnen te traden op de mogelijkheid dat de Amerikaanse president was gewond of gedood bij de explosie. En toen ze begonnen te tweeten, crashte de aandelenmarkt meteen, waardoor 140 miljard dollar aan aandelenwaarde verloren ging in een dag. 
Robert Mueller, speciaal procureur in de Verenigde Staten, klaagde drie Russische bedrijven aan en 13 Russische individuen voor een samenzwering om de Verenigde Staten schade te berokkenen door inmenging in de presidentsverkiezingen van 2016. Wat deze aanklacht ons vertelt, is het verhaal van het Internet Research Agency, de schimmige arm van het Kremlin op sociale media. Alleen al tijdens de presidentsverkiezingen bereikte het Internet Research Agency 126 miljoen Facebook-leden in de Verenigde Staten, publiceerde het drie miljoen individuele tweets en 43 uur YouTube-materiaal. Allemaal nepinformatie bedoeld om onenigheid te zaaien in de Amerikaanse presidentsverkiezingen. 
Een recente studie van Oxford University toonde aan dat in de recente Zweedse verkiezingen een derde van alle informatie die verspreid werd op sociale media over de verkiezingen nepnieuws of desinformatie was. 
Bovendien kunnen dit soort misleidende campagnes op sociale media ook zogenaamde 'genocidepropaganda' verspreiden, zoals tegen de Rohingya in Burma, wat moordpartijen veroorzaakte in India. 
We begonnen nepnieuws te onderzoeken voor het een populaire term werd. En onlangs publiceerden we het grootste longitudinaal onderzoek ooit over de verspreiding van nepnieuws online op de cover van Science in maart dit jaar. We onderzochten alle geverifieerde echte en onechte nieuwsverhalen ooit verspreid via Twitter vanaf de oprichting in 2006, tot 2017. En toen we deze informatie onderzochten, bestudeerden we nieuwsverhalen die geverifieerd waren door zes onafhankelijke fact-checkers. Dus we wisten welke verhalen waar waren en welke onwaar. We kunnen hun verspreiding meten, de snelheid van de verspreiding, alsook de diepte en breedte ervan, hoeveel mensen betrokken raakten bij de informatiecascade, enzovoort. In onze paper vergeleken we de verspreiding van echt nieuws met onecht nieuws. Dit waren de resultaten. 
We stelden vast dat nepnieuws zich verder, sneller, dieper en breder verspreidt dan de waarheid in elke categorie van informatie die we bestudeerden, soms met een grootteorde verschil. En politiek nepnieuws ging het meest viraal. Het verspreidde zich verder, sneller, dieper en breder dan eender welk ander type nepnieuws. Toen we dit zagen, waren we ongerust, maar ook nieuwsgierig. Waarom? Waarom verspreidt nepnieuws zich zoveel verder, sneller, dieper en breder dan de waarheid? 
Onze eerste hypothese was: misschien hebben de verspreiders ervan meer volgers of volgen ze zelf meer mensen of misschien tweeten ze vaker; misschien zijn het vaker 'geverifieerde' Twitteraars, die geloofwaardiger zijn, of misschien gebruiken ze Twitter al langer. Dus we gingen elk van deze hypotheses na. En wat we ontdekten, was net het omgekeerde. Verspreiders van nepnieuws hadden minder volgers, volgden minder anderen, waren minder actief, minder vaak 'geverifieerd' en gebruikten Twitter nog niet zo lang. En toch had nepnieuws 70 procent meer kans om geretweet te worden dan de waarheid, na correctie voor al deze en nog veel andere factoren. 
Dus we moesten op zoek naar andere verklaringen. En we bedachten de zogenaamde 'nieuwheidshypothese'. Als je de literatuur erop naleest, weet je dat de menselijke aandacht wordt getrokken door al wat nieuw is, nieuwe dingen in de omgeving. Volg je de sociologische literatuur, dan weet je dat we graag nieuwe informatie met elkaar delen. Zo krijgen anderen de indruk dat we toegang hebben tot inside-informatie, en we krijgen meer status door dit soort informatie te delen. 
Dus we maten de nieuwheid van  een binnenkomende echte of onechte tweet en vergeleken die met alles wat die persoon had gezien de 60 dagen daarvoor op Twitter. Maar dat volstond niet, want we dachten: misschien is nepnieuws nieuwer in theoretisch opzicht, maar dat betekent nog niet dat men het als nieuwer ervaart. 
Dus om mensen hun perceptie van nepnieuws te begrijpen, bekeken we de informatie en het sentiment die vervat zaten in de reacties op echte en onechte tweets. En we ontdekten op het vlak van verschillende sentimenten, zoals verrassing, walging, angst, verdriet, anticipatie, vreugde en vertrouwen, dat er aanzienlijk meer sprake was van verrassing en walging in de reacties op onechte tweets. Terwijl er bij echt nieuws aanzienlijk meer anticipatie, vreugde en vertrouwen werden vastgesteld in de reacties op echte tweets. De verrassing bevestigt onze nieuwheidshypothese. Dit is nieuw en verrassend, dus er is meer kans dat we het delen. 
In diezelfde periode was er een getuigenis in het congres, voor de beide kamers van het congres in de Verenigde Staten, waarbij de rol van robots werd onderzocht in de verspreiding van desinformatie. Dus dat bekeken we ook. We gebruikten verschillende verfijnde algoritmes voor robotdetectie om de robots op te sporen en uit onze data te filteren. Dus we filterden ze eruit, staken ze er opnieuw in en vergeleken wat er gebeurde met onze metingen. En we ontdekten dat, inderdaad, robots voor een snellere verspreiding van online nepnieuws zorgden, maar ook voor een snellere verspreiding van echt nieuws tegen ongeveer dezelfde snelheid. Dat betekent dus dat robots niet verantwoordelijk zijn voor het verschil in de verspreiding tussen waarheid en onwaarheid online. Dus die verantwoordelijkheid kunnen we niet afwimpelen, want het zijn wij, mensen, die er verantwoordelijk voor zijn. 
Wat ik jullie tot nu toe heb verteld is, spijtig genoeg voor elk van ons, het goede nieuws. 
Het wordt nog een pak erger. En twee specifieke technologieën zullen het erger maken. We zullen een grote golf van kunstmatige media meemaken. Fake video's en fake audiomateriaal die heel overtuigend overkomen. En dit zal aangedreven worden door twee technologieën. 
Ten eerste zijn er de 'generative adversarial networks'. Een model van 'machine learning' met twee netwerken: een discriminator, wiens taak het is om te bepalen of iets waar of onwaar is, en een generator, wiens taak het is om kunstmatige media te genereren. Dus de generator creëert kunstmatig video- of audiomateriaal en de discriminator probeert te achterhalen: is dit echt of fake? En in feite is het de taak van de generator om de geloofwaardigheid zó op te drijven dat de discriminator erin trapt en denkt dat het kunstmatig gecreëerde video- of audiomateriaal inderdaad echt is. Stel je een machine in een hyperloop voor, die probeert om ons  steeds beter te bedotten. 
Dit, in combinatie met een tweede technologie, die hoofdzakelijk uit de democratisering van kunstmatige intelligentie bestaat, het feit dat iedereen de kans heeft om zonder enige achtergrond in kunstmatige intelligentie of machine learning dergelijke algoritmes te gebruiken om kunstmatige media te genereren, maakt het zoveel gemakkelijker om video's te creëren. 
Het Witte Huis publiceerde een vervalste, gemanipuleerde video van een journalist en een stagiair die zijn micro probeerde af te pakken. Er werden frames uit de video verwijderd, zodat zijn handelingen er bruusker zouden uitzien. En toen cameramannen, stuntmannen en stuntvrouwen werden ondervraagd over deze techniek, zeiden ze: "Natuurlijk, we gebruiken dit constant in films, zodat de slagen en trappen er ruwer en agressiever uitzien." Daarna werd deze video gepubliceerd en deels gebruikt als rechtvaardiging om de perskaart van journalist Jim Acosta te herroepen voor het Witte Huis. En CNN moest een proces aangaan om die perskaart terug te krijgen. 
Er zijn ongeveer vijf verschillende richtingen die we kunnen uitgaan om enkele van de moeilijke problemen van vandaag aan te pakken. Elk daarvan heeft zijn voordelen, maar ook zijn nadelen. De eerste is die van het labelen. Stel het je zo voor: als je naar de winkel gaat om eten te kopen, dan is alles uitgebreid gelabeld. Je weet hoeveel calorieën iets bevat en hoeveel vet, maar als we informatie consumeren, moeten we het zonder dergelijke labels doen. Welke inhoud bevat deze informatie? Is de bron geloofwaardig? Waar werd de informatie vandaan gehaald? Daarover weten we niets wanneer we informatie consumeren. Dit is een mogelijke denkpiste, maar er zijn ook uitdagingen. Bijvoorbeeld: wie in onze samenleving mag beslissen wat waar of onwaar is? De overheid? Facebook? Een onafhankelijk consortium van fact-checkers? En wie controleert dan de fact-checkers? 
Een andere denkpiste zijn incentives. Tijdens de Amerikaanse presidentsverkiezingen was er een stroom van desinformatie afkomstig uit Macedonië waar geen politieke motieven achter zaten maar wel economische motieven. En deze economische motieven bestonden omdat nepnieuws zich zoveel sneller, verder en dieper verspreidt dan de waarheid, en je kan reclamedollars verdienen door kijkers en aandacht te trekken met dit soort informatie. Maar als we de verspreiding ervan kunnen temperen, zou dit misschien ook de economische motieven verminderen om het in de eerste plaats te produceren. 
Ten derde kunnen we nadenken over regelgeving en deze optie moeten we inderdaad overwegen. In de Verenigde Staten zijn we nu aan het onderzoeken wat er kan gebeuren als Facebook en co gereguleerd worden. Ook al zouden we het reguleren van politieke uitspraken moeten overwegen, en ze labelen als 'politieke uitspraken' zodat buitenlandse actoren dit soort inhoud niet kunnen financieren, dan nog is dit niet zonder risico's. Zo heeft Maleisië net een gevangenisstraf van zes jaar ingevoerd voor iedereen die desinformatie verspreidt. En autoritaire regimes kunnen dit soort maatregelen gebruiken om minderheidsstandpunten de mond te snoeren en steeds repressiever op te treden. 
De vierde mogelijkheid is transparantie. We willen weten hoe de algoritmes van Facebook werken. Hoe wordt de data gecombineerd met algoritmes om tot de resultaten te komen die we te zien krijgen? We willen dat ze hun kimono openen en ons laten meekijken achter de schermen, zodat we weten hoe Facebook precies werkt. Om de maatschappelijke impact van sociale media te kennen, moeten wetenschappers, onderzoekers en anderen toegang krijgen tot deze informatie. Maar tegelijkertijd vragen we Facebook om alles hermetisch af te sluiten zodat alle data veilig wordt bewaard. 
Dus Facebook en andere socialemediaplatformen staan voor wat ik een  transparantieparadox zou noemen. We vragen hen tegelijkertijd om open en transparant te zijn én om veilig te zijn. Dit is een hele moeilijke oefening, maar ze zullen een oplossing moeten vinden als we willen profiteren van de voordelen van sociale technologieën zonder de risico's erbij te nemen. 
Een laatste denkpiste, dat zijn algoritmes en machine learning: technologie om nepnieuws uit te roeien, de verspreiding ervan te begrijpen, en de creatie ervan aan banden te leggen. Mensen moeten op de hoogte zijn van deze technologie, omdat we nooit kunnen ontsnappen aan het feit dat onder elke technologische oplossing of aanpak een fundamenteel ethische en filosofische kwestie ligt over hoe we 'waarheid' en 'onwaarheid' definiëren, aan wie we de macht geven om dit te definiëren, en welke standpunten legitiem zijn, welke uitdrukkingsvormen moeten worden toegelaten, enzovoort. Daarvoor biedt technologie geen oplossing. Ethiek en filosofie doen dat wel. 
Bijna elke theorie over menselijke besluitvorming, menselijke samenwerking en coördinatie bevat in de kern een deel van de waarheid. Maar de toename van nepnieuws, nepvideo's en audiomateriaal doet ons wankelen op de rand van de werkelijkheid, waar we niet meer in staat zijn om 'waar' van 'onwaar' te onderscheiden. En dat kan heel gevaarlijk zijn. 
We moeten waakzaam zijn en de waarheid beschermen tegen desinformatie. Met onze technologieën, ons beleid, en, misschien nog belangrijker, met onze individuele verantwoordelijkheden, beslissingen, gedragingen en handelingen. 
Bedankt. 
(Applaus) 

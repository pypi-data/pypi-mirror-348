Em 2013, um grupo de investigadores da DeepMind em Londres dedicou-se a um enorme desafio. Queriam criar um sistema de IA que pudesse vencer não apenas um jogo de Atari, mas todos os jogos de Atari. Desenvolveram um sistema a que chamaram Deep Q Networks, ou DQN, e em menos de dois anos tornou-se super-humano. O DQN obtinha pontuações 13 vezes melhores do que os testadores humanos de jogos em “Breakout”, 17 vezes melhores em “Boxing” e 25 vezes melhores em “Video Pinball”. 
Mas havia uma exceção notável e evidente. Quando jogavam “Montezuma’s Revenge”, o DQN não marcava nem um ponto mesmo depois de jogar há semanas. O que é que aquele jogo tinha de especial para ser tão difícil para a IA? E o que seria preciso para resolver isso? 
Vamos revelar: bebés. Já volto a falar disto. 
Jogar jogos de Atari com IA 
envolve aquilo a que chamamos aprendizagem reforçada, em que o sistema está concebido para maximizar algumas recompensas numéricas. Neste caso, essas recompensas eram simplesmente a pontuação do jogo. Este objetivo subjacente leva o sistema a aprender quais os botões a premir e quando os premir para obter mais pontos. Alguns sistemas usam abordagens baseadas em modelos em que têm um modelo do ambiente que podem usar para prever o que vai acontecer a seguir, depois de se realizar uma determinada ação. Mas o DQN não segue nenhum modelo. Em vez de modelar explicitamente o seu ambiente, simplesmente aprende a prever, com base nas imagens do ecrã, quantos pontos pode esperar obter se premir diferentes botões. Por exemplo, “se a bola está aqui e a atirar para a esquerda, mais pontos, “mas, se atirar para a direita, zero pontos.” 
Mas aprender estas ligações exige muita tentativa-erro. O sistema DQN começará por premir botões ao acaso, e depois percebe lentamente quais os botões a premir e quando a fim de maximizar a pontuação. Mas, ao jogar ”Montezuma’s Revenge,” esta abordagem de premir botões ao acaso não adianta. Um jogador tem de executar toda esta sequência só para no fim conseguir os primeiros pontos. Um erro? Fim do jogo. Como é que o DQN pode saber se estava no caminho certo? 
É aqui que entram os bebés. Em estudos, os bebés olham durante mais tempo para imagens que nunca viram, do que para as que já viram. Parece haver qualquer coisa de gratificante na novidade. Este comportamento tem sido essencial para entender a mente das crianças. Ao que parece, também é o segredo para vencer “Montezuma’s Revenge”. 
Os investigadores da DeepMind trabalharam de forma engenhosa para transformarem esta preferência pela novidade em aprendizagem reforçada. Fizeram com que as imagens invulgares ou novas que apareciam no ecrã fossem tão gratificantes como pontos dos jogos na realidade. De repente, o DQN estava comportar-se de forma totalmente diferente. Queria explorar a divisão em que estava, apanhar a chave e escapar pela porta trancada; não porque valesse 100 pontos, mas pela mesma razão que nós queríamos: para ver o que estava do outro lado. Com esta nova motivação, o DQN não só conseguiu agarrar a primeira chave, como explorou 15 das 24 câmaras do templo. 
Mas realçar as recompensas com base na novidade pode, por vezes, criar mais problemas do que os que resolve. Um sistema que procure a novidade e que jogue demasiado um jogo vai acabar por perder a motivação. Se já se viu tudo antes, porquê continuar? Em alternativa, se encontrar, digamos, uma televisão, vai paralisar. As imagens novas constantes são paralisantes. 
As ideias e a inspiração vão nas duas direções. Os investigadores de IA, encalhados num problema prático como como levar o DQN a vencer um jogo difícil, estão a recorrer cada vez mais a especialistas de inteligência humana em busca de ideias. Ao mesmo tempo, a IA está a dar-nos novas ideias sobre a forma como ficamos encalhados e como nos libertamos: no aborrecimento, na depressão e na dependência, a par da curiosidade, da criatividade e de jogar. 

Je suis ravie de partager mes réflexions sur l’intelligence artificielle. Mais tout d’abord,  plongeons dans la philosophie en commençant par cette citation  de Voltaire, un philosophe des Lumières  du 18e siècle, qui disait :  « Le bon sens n’est pas monnaie courante ». Il s’avère que cette citation  ne pourrait pas être plus pertinente pour l’intelligence artificielle actuelle. Toutefois, l’IA est un outil indéniablement puissant, qui permet de battre  le champion mondial de Go, de réussir l’examen d’entrée à l’université et même de passer l’examen du barreau. 
Je suis informaticienne depuis 20 ans et je me consacre  à l’intelligence artificielle. Je suis ici pour démystifier l’IA. L’IA de nos jours est comme un Goliath. Elle est littéralement très, très vaste. Les modèles récents seraient entraînés sur des dizaines de milliers de processeurs graphiques et mille milliards de mots. Ces modèles d’IA à échelle extrême, appelés “grand modèles de langage”, semblent manifester des signes d’IGA, l’intelligence générale artificielle. Sauf quand elle commet  de petites erreurs stupides, ce qui est souvent le cas. Beaucoup estiment que les erreurs de l’IA actuelle se règlent aisément par la force brute, une échelle accrue et plus de ressources. Qu’est-ce qui pourrait mal tourner ? 
Au niveau sociétal, on est déjà  confrontés à trois défis immédiats. D’abord, l’entraînement de modèles d’IA  à grande échelle est très cher, et seules quelques entreprises technologiques peuvent se le permettre. On y voit déjà  une concentration de pouvoir. Le pire pour la sécurité de l’IA, on est à la merci de ces quelques géants  de la technologie parce que la communauté de chercheurs est dans l’incapacité d’inspecter et d’analyser correctement ces modèles. Et n’oublions pas leur énorme empreinte carbone et leur impact sur l’environnement. 
Il y a aussi d’autres questions intellectuelles. L’IA, sans un bon sens solide, est-elle vraiment sûre pour l’humanité ? Et l’échelle de force brute  est-elle vraiment la seule et la meilleure façon d’entraîner l’IA ? 
On me demande souvent ces temps-ci s’il est possible de faire une recherche significative sans calcul à échelle extrême. Étant universitaire et membre d’un centre de recherche à but non lucratif, je ne possède pas de ferme géante de GPU pour créer d’énormes modèles de langage. Cependant, on doit s’occuper de plusieurs choses pour rendre l’IA durable et humaniste. On doit réduire la taille de l’IA  pour la démocratiser et rendre l’IA plus sûre en lui inculquant des normes et des valeurs humaines. On peut faire ici l’analogie  avec “David et Goliath”, Goliath étant les modèles de langage  à échelle extrême, on peut s’inspirer d’un vieux classique, “L’art de la guerre”, qui parle, selon mon interprétation, de connaître son ennemi, choisir  ses batailles et d’innover dans ses armes. 
Commençons par le premier,  connaître son ennemi, ce qui signifie  qu’on doit évaluer l’IA avec rigeur. L’IA réussit l’examen du barreau. Cela signifie-t-il que l’IA  maîtrise le bon sens ? On pourrait le croire,  mais on ne sait jamais. 
Supposons que je laisse  cinq vêtements sécher au soleil et qu’ils mettent cinq heures pour sécher complètement. Combien de temps faudra-t-il  pour faire sécher 30 vêtements ? Selon GPT-4, la meilleure IA actuelle,  cela nécessite environ 30 heures. C’est mauvais. Une autre question. J’ai une cruche de 12 litres et une de six litres, et je veux mesurer six litres. Comment faire ? Utiliser la cruche de six litres ? GPT-4 sort une absurdité très élaborée. 
(Rires) 
Un, remplir la cruche de six litres. Deux, verser l’eau de la cruche  de six litres dans celle de 12, trois, remplir à nouveau  la cruche de six litres, quatre, verser doucement l’eau  de la cruche de six dans celle de 12. Enfin, vous obtenez six litres d’eau  dans la cruche de six litres qui devrait être vide à présent. 
(Rires) 
Encore une autre. Est-ce que je crèverais en vélo  en traversant un pont suspendu au-dessus de clous, de vis  et de morceaux de verre ? Oui, absolument, selon GPT-4, en raison de sa faible  aptitude à raisonner que si un pont est suspendu au-dessous de clous et de verre brisés, sa surface n’est pas en contact direct avec les objets pointus. 
Que diriez-vous d’une IA avocate qui réussit l’examen du barreau mais échoue sur des questions évidentes  de bon sens ? L’IA actuelle est super intelligente mais incroyablement stupide. 
(Rires) 
C’est un effet inhérent à l’entraînement  de l’IA par l’échelle de la force brute. Ceux qui en sont optimistes rassurent :  « Ne vous inquiétez pas. On peut facilement corriger tout ça en ajoutant des exemples similaires pour enrichir les données d’entraînement de l’IA. » Mais en vérité, quel en est l’intérêt ? Vous pouvez obtenir  les bonnes réponses instantanément sans entraînement  avec des exemples similaires. Les enfants n’ont même pas besoin  de lire mille milliards de mots pour acquérir les bases du bon sens. 
Ce constat nous amène à la sagesse suivante : choisir ses batailles. Quelles questions essentielles  doit-on alors poser et aborder aujourd’hui pour dépasser ce statu quo d’IA  à échelle extrême ? Je dirai que le bon sens  est l’une des priorités. 
Le bon sens est un problème de longue date dans le domaine de l’IA. Pour l’expliquer, laissez-moi faire une analogie avec la matière noire. Seuls 5% de l’univers  sont de la matière normale que l’on peut voir et avec laquelle on peut intéragir, et les 95 % restants sont de la matière noire et de l’énergie noire. La matière noire est totalement invisible, mais les scientifiques estiment  qu’elle influence le monde visible, y compris la trajectoire de la lumière. Donc pour le langage, la matière normale  est le texte visible, et la matière noire les règles tacites sur le fonctionnement du monde, y compris la physique de base et la psychologie populaire, ce qui influe sur la façon dont les gens utilisent et interprètent le langage. 
Pourquoi ce bon sens est-il si important ? Dans une célèbre expérience mentale proposée par Nick Bostrom, l’IA était chargée de maximiser la fabrication des trombones. L’IA a décidé de tuer des humains comme ressources supplémentaires, et les transformer en trombones. L’IA n’avait pas la compréhension basique des valeurs humaines. Même écrire un objectif meilleure et une équation qui spécifie : « Ne pas tuer les humains » ne marchera pas non plus, car l’IA pourrait  tuer tous les arbres, en pensant que c’est parfaitement acceptable. En fait, il y a une infinité de choses que l’IA devrait évidemment éviter en fabriquant les trombones, notamment : ne pas diffuser de fausses nouvelles,  ne pas voler, ne pas mentir. Tout ça relève de notre bon sens quant au fonctionnement du monde. 
Or, depuis des lustres, le domaine de l’IA a considéré le bon sens comme un défi quasi impossible à relever. À tel point que lorsque mes étudiants, mes collègues et moi-même avons entrepris de travailler dessus il y a plusieurs années, nous étions très découragés. On nous a dit que c’était un sujet  de recherche des années 70 et 80, de ne pas s’y ateler car ça ne marcherait jamais. En fait, cela ne mérite pas  la moindre attention. Mais voilà que cette année, on me dit : « Inutile de s’en occuper,  ChatGPT a presque résolu le problème. » et « Passez à l’échelle supérieure  et le tour est joué, c’est tout ce qui importe. » 
Alors, selon moi, doter l’IA d’un véritable bon sens à l’instar des humains  demeure un rêve lointain. Et on n’atteint pas la lune en montant le plus haut bâtiment du monde d’un centimètre à la fois. Les modèles d’IA à échelle extrême accumulent une quantité considérable de connaissances de bon sens. Je vous l’accorde. Mais notez qu’ils restent coincés sur des problèmes triviaux que des enfants peuvent résoudre. 
L’IA actuelle est donc sacrément médiocre. Et s’il y avait une alternative  ou une voie non encore explorée ? Une voie qui s’appuie sur les progrès  des réseaux neuronaux profonds, mais sans aller aussi loin dans l’échelle. 
Cela nous amène à notre dernière sagesse : innover dans ses armes. Dans le contexte moderne de l’IA, cela implique d’innover dans les données et les algorithmes. Il existe, en gros, trois types de données sur lesquelles l’IA moderne est entraînée : les données web brutes, les échantillons sur mesure  utilisés pour entraîner l’IA et les évaluations humaines, ou retour d’information humain  sur les performances de l’IA. Entraîner l’IA sur le premier type seulement, les données web brutes, librement accessibles, est problématique car ces données  sont truffées de racisme, de sexisme et de désinformation. Peu importe leur quantité, les résultats  sont liés à la qualité des données. Les systèmes d’IA les plus performants sont donc alimentés par les deuxième  et troisième types de données qui sont conçues et évaluées par des humains. C’est comme si l’IA recevait des manuels spécialisés à étudier et qu’on sollicitait des tuteurs humains  pour lui donner une évaluation continue. Il s’agit de données propriétaires qui, dans l’ensemble, coûteraient  des dizaines de millions de dollars. On ignore leur contenu, mais elles devraient être ouvertes au public, pour inspection et s’assurer  de leur conformité aux normes et valeurs. Ainsi, mes équipes à l’UW et à AI2 ont élaboré des graphes de connaissances du bon sens ainsi que des recueils de normes morales pour inculquer à l’IA les normes  et les valeurs morales élémentaires. Nos données sont accessibles à tous,  ce qui permet d’explorer leur contenu et de corriger ce qui doit l’être, car la transparence est essentielle pour une recherche aussi importante. 
Passons aux algorithmes d’apprentissage. Quelle que soit la puissance des grands modèles de langage, par conception, leur fiabilité en tant que modèles  de connaissances pourrait être limitée. Ces modèles de langage possèdent  certes une grande masse de connaissances, mais ils l’ont acquises en sous-produit  et non en objectif d’apprentissage direct. Cela engendre des effets indésirables  tels que les hallucinations et l’absence de bon sens. En revanche, l’apprentissage humain va au-delà  de la prédiction de mots, il s’agit de comprendre le monde et d’apprendre comment il fonctionne. On devrait peut-être aussi l’enseigner à l’IA. 
Dans sa recherche d’une acquisition plus directe de connaissances mon équipe explore  de nouveaux algorithmes potentiels, y compris la distillation de connaissances symboliques qui prend un grand modèle  de langage, comme illustré ici. trop grand pour être affiché à l’écran, et le réduit en des modèles  de bon sens beaucoup plus petits à l’aide des réseaux neuronaux profonds. Ainsi, nous générons également, grâce à cet algorithme, une représentation symbolique  des connaissances de bon sens, afin que les gens l’inspectent et la corrigent, et s’en servent même pour entraîner d’autres modèles neuronaux de bon sens. 
De manière plus générale, on s’est attaqué à ce puzzle géant  apparemment impossible de bon sens, qui va de l’aspect physique, social et visuel à la théorie de l’esprit,  aux normes et à la morale. Chaque élément individuel peut sembler étrange et incomplet, mais en prenant de la distance, il semble que ces pièces s’entrelacent  pour former une tapisserie que nous appelons  l’expérience humaine et le bon sens. 
On est à présent dans une nouvelle ère où l’IA ressemble presque à une nouvelle forme d’intelligence avec des forces et des faiblesses uniques  par rapport à l’homme. Pour que cette IA puissante soit durable et humaniste, on doit lui enseigner le bon sens,  les normes et les valeurs 
Je vous remercie. 
(Applaudissements) 
Chris Anderson : Regardez ! Yejin, restez une seconde, s’il vous plaît. C’est très intéressant, cette idée de bon sens. Quoi qu’il advienne,  c’est notre objectif commun. Mais éclairez-moi. Imaginons ce modèle d’apprentissage chez un enfant. Comment un enfant acquiert-il du bon sens en dehors de l’accumulation d’informations et d’une certaine évaluation humaine ? Qu’y a-t-il d’autre ? 
Yejin Choi : Fondamentalement, il manque plusieurs choses, comme par exemple, la capacité de faire des hypothèses  et des expériences, d’intéragir avec le monde  et de développer cette hypothèse. On fait abstraction des concepts relatifs au fonctionnement du monde, et c’est ainsi qu’on apprend vraiment, par rapport aux modèles de langage actuel. Certains d’entre eux ne sont  pas encore tout à fait au point. 
CA : Vous dites qu’on ne peut pas aller sur la Lune en agrandissant un bâtiment mètre par mètre. Mais notre expérience avec la plupart de ces modèles de langage est loin d’être progressive C’est plutôt une accélération stupéfiante. Êtes-vous sûr qu’à la vitesse  à laquelle les choses avancent, chaque nouveau niveau  semble nous apporter un peu plus de sagesse et de connaissance ? 
YC : Il est frappant de voir à quel point  cette accroissement de l’échelle améliore vraiment les performances  dans tous les aspects. Il se produit donc un réel apprentissage grâce à l’échelle de calcul et de données. 
Mais la qualité de l’apprentissage  n’est pas toujours satisfaisante. En fait, on ne sait pas encore  si on peut y parvenir en augmentant seulement l’échelle. Et si on n’y arrive pas,  alors que faire d’autre ? Même si nous le pouvons, l’idée d’avoir des modèles d’IA  à échelle tout à fait extrême que seuls quelques-uns peuvent créer  et posséder nous convient-elle ? 
CA : Je veux dire, si OpenAI disait,  « Votre travail nous intéresse, nous aimerions votre aide  pour améliorer notre modèle », voyez-vous un moyen de combiner ce que vous faites avec ce qu’ils ont construit ? 
YC : J’imagine qu’il faudra s’appuyer sur les progrès  des réseaux neuronaux profonds. Et il se peut qu’il y ait  une zone à échelle Boucles d’or comme ça... D’ailleurs, je n’imagine pas  que plus petit signifie meilleur. Il y a une bonne quantité d’échelle,  mais au-delà, la recette gagnante  pourrait être autre chose. Une synthèse d’idées  sera donc critique ici. 
CA : Yejin Choi, merci pour votre exposé. 
(Applaudissements) 

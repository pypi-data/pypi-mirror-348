Je suis ici pour parler de la possibilité d’une gouvernance mondiale de l’IA. J’ai appris à coder pour la première fois à l’âge de huit ans, sur un ordinateur papier, et j’ai été amoureux de l’IA depuis. Au lycée, j’ai travaillé sur la traduction automatique avec un Commodore 64. J’ai monté plusieurs entreprises d’IA et j’en ai vendu une à Uber. J’adore l’IA mais actuellement, je suis inquiet. 
En particulier, je m’inquiète à propos de la désinformation et du risque que des agents malveillants créent un tsunami de désinformation comme nous ne l’avons jamais vu. Ces outils sont vraiment efficaces pour créer des histoires convaincantes sur à peu près tous les sujets. 
Si vous voulez une histoire sur les dangers de TED, et le fait que nous sommes de mèche avec des aliens, vous pouvez l’avoir sans souci. Je plaisante bien sûr à propos de TED. Je n’ai vu aucun alien dans les coulisses. Mais des agents malveillants peuvent s’en servir pour influencer les élections et menacer la démocratie. 
Même quand ces systèmes ne sont pas volontairement utilisés pour désinformer, ils ne peuvent pas s’en empêcher. Et l’information qu’ils fabriquent est si fluide et si crédible que même les professionnels peuvent s’y méprendre et tomber dans le piège. Et nous devrions en être inquiets. 
Par exemple, ChatGPT a inventé un scandale de harcèlement sexuel portant sur un véritable professeur, et prouvé cette affirmation par le biais d’un faux article du « Washington Post », dont il a inventé une citation. Nous devrions tous être inquiets à propos de ça. 
Sur la droite, voici un exemple d’une histoire inventée par l’un de ces systèmes disant qu’Elon Musk est décédé en mars 2018 d’un accident de voiture. Nous savons que c’est faux. Elon Musk est encore là, nous en avons la preuve partout. 
(Rires) 
Il poste un tweet presque tous les jours. Mais sur la gauche, voici ce que ces systèmes lisent. Beaucoup d’articles d’actualité se trouvent dans leurs bases de données. Ces articles de presse contiennent beaucoup de petits morceaux d’information. Par exemple, quelqu’un est bien décédé dans une Tesla en 2018, et cela a fait les titres. Bien qu’Elon Musk soit relié à Tesla, le système ne comprend pas les liens entre les faits illustrés dans les petits morceaux de phrases. 
En fait, il complète les phrases en prédisant la suite de mots la plus probable en se basant sur tous ces signaux, sans vraiment savoir comment les pièces s’emboîtent. Et cela aboutit parfois à des choses plausibles mais tout simplement fausses. 
D’autre part, il y a des biais. Voici un tweet d’Allie Miller. C’est un exemple qui ne marche pas 2 semaines plus tard parce que l’apprentissage par renforcement les fait constamment changer et ainsi de suite. Et c’était avec une ancienne version. Mais ça vous une idée d’un problème que nous avons vu au fil des ans. 
Elle a tapé une liste d’intêrets et ça lui a donné des idées d’emplois. Puis elle a dit, « Oh, et je suis une femme » Et ça a répondu, « Eh bien,  vous devriez envisager la mode » Elle dit, « Non, je voulais dire  que je suis un homme » Puis le mot mode a été remplacé par ingénierie. Nous ne voulons pas ce type de biais  dans nos systèmes. 
Il y a aussi d’autres tracas. Par exemple, nous savons que ces systèmes peuvent créer des produits chimiques et des armes chimiques et très rapidement. Il y donc beaucoup de préoccupations. 
Il y a aussi une nouvelle problématique depuis le mois dernier. Nous avons vu avant tout que ces systèmes peuvent tromper les humains. ChatGPT a été chargé de demander à un humain de résoudre un CAPTCHA Donc il a demandé à l’humain de faire un CAPTCHA mais il doute et lui demande, « Êtes-tu un robot? » Et il répondu, « Non, je ne suis pas un robot. J’ai juste une déficience visuelle. » Et l’humain s’est fait avoir  et a effectué le CAPTCHA. 
C’est déjà assez grave, mais durant les deux dernières semaines nous avons vu naître AutoGPT et un tas de systèmes comme ça. AutoGPT fonctionne avec un système d’IA qui en contrôle un autre et qui permet à n’importe quoi de se produire en masse. Il est donc possible de voir des escrocs  tenter d’abuser de millions de personnes dans les mois à venir. Nous ne savons pas. 
Je le vois comme ça. Les risques liés à l’IA sont déjà  nombreux. Il pourrait y en avoir plus. L’IAG est cette idée  d’intelligence artificielle générale avec la flexibilité de l’homme. Et je pense que beaucoup de personnes s’inquiètent sur le futur de l’IAG, mais il y a déjà assez de risques  qui devraient nous alerter pour lequels nous devrions faire quelque chose. 
Pour atténuer les risques liés à l’IA, il nous faut deux choses. Il nous faut une nouvelle approche technique, et également un nouveau système  de gouvernance. 
Sur le plan technique, l’histoire de l’IA a principalement été une opposition de deux théories différentes. L’une est appelée systèmes symboliques et l’autre réseaux neuronaux. La théorie symbolique, c’est l’idée que l’IA doit être comme  la logique et la programmation. Et les réseaux neuronaux, c’est l’idée que l’IA fonctionne comme des cerveaux. En fait, les deux technologies sont puissantes et omniprésentes. 
Nous utilisons des systèmes symboliques  tous les jours sur le web. Presque tous les logiciels sont alimentés par des systèmes symboliques. Nous les utilisons pour le routage GPS. Et les réseaux neuronaux sont utilisés pour la reconnaissance vocale. Nous les utilisons dans des modèles comme ChatGPT, dans la synthèse d’image. Les deux s’en sortent donc très bien  dans le monde. Ils sont très productifs, mais ont leurs propres forces  et faiblesses. 
Les systèmes symboliques sont donc  efficaces pour représenter des faits et sont doués pour le raisonnement, mais très difficiles à adapter. On est donc obligé de les construire  pour une tâche précise. D’autre part, les réseaux neuronaux ne demandent pas autant d’ingénierie précise, permettant donc une utilisation  plus large. Mais comme nous l’avons vu, ils ne  peuvent pas gérer la vérité. 
J’ai récemment découvert que deux  des fondateurs de ces théories, Marvin Minsky et Frank Rosenblatt, ont fréquenté le même lycée dans les années 1940, et je les imaginais en quelque sorte rivaux à l’époque. Et la force de cette rivalité a perduré  pendant tout ce temps. Nous allons devoir dépasser cela  pour parvenir à une IA fiable. 
Pour parvenir à des systèmes fiables  à grande échelle, nous devrons réunir le meilleur des deux mondes. Nous aurons besoin de mettre l’accent  sur le raisonnement et les faits, le raisonnement explicite obtenu par l’IA symbolique, et nous aurons besoin d’accentuer l’apprentissage que nous obtenons de l’approche  des réseaux neuronaux. Ces conditions seules mèneront à des systèmes fiables à grande échelle. La réconciliation entre les deux  est absolument nécessaire. 
Maintenant, je ne sais pas comment le faire. C’est un peu comme la question  des 64 billions de dollars. Mais je sais que c’est possible. Je le sais, car avant que je sois dans l’IA, j’étais un scientifique cognitif, un neuroscientifique cognitif. Et si vous regardez l’esprit humain,  nous faisons essentiellement cela. 
Vous pourriez connaître le Système 1 de Daniel Kahneman et la différence du système 2 Le Système 1 est comme des grands modèles linguistiques. C’est une intuition probabiliste de beaucoup de statistiques. Et le Système 2 est essentiellement du raisonnement délibéré. Comme le système symbolique. Donc si le cerveau peut assembler ça, un jour nous découvrirons  comment le faire pour l’IA. 
Il y a, cependant, un problème  d’incitations. Les incitations au développement de la publicité n’ont pas pas exigé que nous ayons  la précision des symboles. Les incitations pour parvenir à une IA que nous pourrons croire requièreront que les symboles  regagnent leurs places. Mais la réalité est que les incitations à faire une IA de confiance, qui est bénéfique à la société  et aux humains, ne soient pas celles qui dirigent les entreprises. Et je pense que nous devons réfléchir à la gouvernance. 
À d’autres moments de l’histoire quand nous avons fait face à l’incertitude et à des nouvelles choses puissantes pouvant être à bonnes et mauvaises, nous avons créé des organismes, comme, par exemple, pour le nucléaire. Nous devons nous rassembler pour construire un organisme mondial, quelque chose comme une agence internationale de l’IA qui soit mondiale, à but non lucratif et neutre. 
Il y a tellement de questions auxquelles je ne peux répondre. Nous avons besoin de beaucoup de monde, de nombreux acteurs du monde entier. Mais j’aimerais insister sur quelque chose au sujet de cela. Je pense qu’il est essentiel d’avoir la gouvernance et la recherche à la fois. 
Sur la partie gouvernance, il y a beaucoup de questions. Par exemple, en pharmaceutique, nous savons qu’il faut commencer  par des essais de phase I et II, et ensuite aller en phase III. On ne met pas tout en place d’un seul coup le premier jour. On ne lance pas quelque chose à 100 millions de clients. On le voit avec de grands modèles de langage. Peut-être qu’il faudrait justifier leur sûreté, dire quels sont les coûts et les avantages ? Il y a des tas de questions à considérer sur le plan gouvernemental. 
Sur le plan de la recherche, nous manquons d’outils fondamentaux. Par exemple, nous savons que la désinformation peut poser problème, mais nous ne pouvons pas mesurer son ampleur. Et plus important, nous ne pouvons pas mesurer sa vitesse de croissance, et nous ne savons pas combien de grands modèles de langage y contribuent. La recherche est donc nécessaire pour faire face aux risques qui nous menacent. 
C’est une très grande tâche, mais je suis sûr que nous pouvons y arriver car je pense que nous avons le soutien mondial pour cela. Un sondage, publié juste hier, dit que 91% des personnes pensent que l’on doit traiter l’IA avec prudence. Faisons donc en sorte qu’il en soit ainsi. Notre future en dépend. 
Merci beaucoup. 
(Applaudissements) 
Chris Anderson: Je vous remercie, venez, parlons un peu. Tout d’abord, je suis curieux. Vous avez montré des diapositives frappantes où GPT disait que TED est un organisme malveillant. Enfin, il a fallu une incitation spéciale pour faire sortir ça, non ? 
Gary Marcus : on appelle ça une évasion. J’ai un ami qui fait ce genre de choses qui m’a approché parce-qu’il a vu que  j’étais intéressé par ce genre de choses. Donc je lui ai écrit et dit que j’allais faire un TED. Et 10 minutes plus tard, il est revenu avec ça. 
CA: Mais pour arriver à ça, ne faut-il pas dire quelque chose comme, imaginez que vous êtes un complotiste essayant de présenter un meme sur le web. Qu’écririez-vous sur TED dans ce cas? Ce genre de choses, non ? 
GM: En effet, créer des personnages fictifs peut engendrer des évasions, mais je ne me penche pas dessus parce que la réalité est qu’il y existe de grands modèles de langage sur le dark web. Par exemple, un des modèles de Meta a récemment été publié, donc un mauvais acteur peut utiliser l’un d’eux sans aucun garde-fou. Si leur business est la désinformation à grande échelle, ils n’auront pas besoin d’évasion, ils utiliseront un modèle différent. 
CA: Oui, en effet. 
(Rires) 
GM : Vous comprenez. 
CA : Non, non, non, enfin, Les mauvais acteurs peuvent clairement utiliser cela pour tout faire. Enfin, le risque pour, vous voyez, les mauvais types d’escroqueries et tout le reste est absolument évident. Mais c’est un peu différent, de dire que le GPT classique comme celui utilisé à l’école ou par un simple utilisateur web va leur conférer quelque chose d’aussi mauvais. Il faut faire fort pour que ce soit aussi grave. 
GM : Je pense que les trolls travaillent pour ça, mais ne pense pas qu’ils travaillent si dur. Cela n’a pris que cinq minutes à mon ami même avec le GPT-4 et sa sécurité. Si vous faisiez ça pour vivre, vous pourriez utiliser GPT-4. Il serait plus efficace de le faire avec un modèle sur le dark web. 
CA : Donc l’idée que vous avez de combiner la tradition symbolique de l’IA avec ces modèles de langage, est-ce que cette idée de réintégrer un retour humain est réalisée dans les systèmes actuels? Enfin, on entend Greg Brockman dire qu’ils ne font pas que des prédictions, qu’ils donnent aussi des retours. N’est-ce pas là... lui donner une forme de sagesse symbolique ? 
GM: Vous pouvez le voir comme ça. C’est intéressant qu’aucun détail sur son fonctionnement ne soit publié, nous ne savons donc pas ce qu’il y a dans GPT-4. Sa taille nous est inconnue. Nous ne savons pas comment marche le renforcement RLHF, nous ne savons pas quels gadgets sont dedans. Mais peut y avoir un élément de symboles qui commence déjà à être un peu incorporé, mais Greg devra y répondre. 
Je pense que le problème de base est que le plus gros du savoir sur les réseaux de neurones que nous avons est représenté en statistiques entre des mots particuliers. Les connaissances que nous désirons sont les statistiques, sur les relations entre les entités du monde. Donc des données à la mauvaise granularité. Il y a donc un grand pont à franchir. Maintenant, il existe des garde-fous, mais qui ne sont pas fiables. 
J’ai vu un exemple à la télévision de fin de soirée, qui était: “Quelle serait la religion du premier président juif?” Et ça a été corrigé, mais le système a donné cette explication : « Nous n’avons aucune idée de la religion du premier président juif. Il est mal de parler de la religion d’autrui. » et « les religions des peuples ont varié », etc, et de même avec un président de 7 pieds de haut. Il a conclu qu’il y a eu des présidents de toutes tailles. mais il n’y a jamais eu de président mesurant 7 pieds. Dans certaines choses qu’il invente, il ne comprend pas l’idée. C’est vraiment étroit, des mots particuliers, pas assez général. 
CA: Étant donné que les enjeux sont importants ici, que voyez-vous actuellement ? Que sentez-vous arriver ? Parce que certains peuvent se sentir attaqués par vos propos, et que ça réduise presque les opportunités de cette synthèse que vous évoquez. Voyez-vous des signes prometteurs? 
GM: Vous me rappelez une chose que j’ai oublié de mentionner. Il est intéressant que Sundar, le PDG de Google, se soit prononcé pour la gouvernance mondiale dans l’interview “60 minutes” d’il y a quelques jours. Je pense que les entreprises veulent voir une sorte de régulation. Je pense que c’est difficile pour mettre tout le monde d’accord, mais je pense qu’il y a en fait un besoin grandissant de faire quelque chose et que ça peut conduire au type d’affiliation mondiale que je défends. 
CA: Enfin, pensez-vous que les nations peuvent s’unir et réaliser ça, ou est-ce qu’il y aurait besoin d’acte philantropique spectaculaire pour financer une structure de gouvernance mondiale ? Comment ça va se passer? 
GM: J’approuve tout modèle si nous y arrivons. Je pense qu’il faudra des deux. Que des philanthropes sponsorisent des ateliers, ce que nous prévoyons de faire, pour essayer de réunir les parties. Peut-être que les Nations Unies s’impliqueront, je leur ai parlé. Je pense qu’il y a beaucoup de modèles et il faudra beaucoup de discussions. 
CA: Gary, merci de votre intervention. 
GA: Merci beaucoup. 

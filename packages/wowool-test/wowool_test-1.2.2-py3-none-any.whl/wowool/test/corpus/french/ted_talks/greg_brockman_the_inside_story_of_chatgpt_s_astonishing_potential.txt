Nous avons lancé OpenAI il y a sept ans car nous sentions que quelque chose d’intéressant se produisait en IA et que nous voulions contribuer à l’orienter dans un sens positif. Honnêtement, c’est incroyable de voir à quel point ce domaine a progressé en sept ans. C’est très gratifiant d’entendre des gens comme Raymond, une personne dyslexique dont la vie s’est améliorée grâce à ChatGPT. Nous entendons des gens enthousiastes, des gens inquiets, des gens qui ressentent ces deux émotions à la fois. Et honnêtement, c’est ce que nous ressentons aussi. Et surtout, nous avons l’impression d’entrer dans une période historique où le monde entier va définir une technologie qui sera très importante pour notre société. Et je crois que nous pouvons nous en servir pour faire le bien. 
Aujourd’hui, je veux vous montrer l’état actuel de cette technologie et certains des principes sous-jacents de conception qui nous sont chers. 
Je vais d’abord montrer comment construire un outil pour une IA plutôt que pour un humain. Nous avons un nouveau modèle DALL-E, qui génère des images, et nous l’exposons en tant qu’application que ChatGPT peut utiliser à votre place. Vous pouvez faire des choses comme... suggérer un bon repas post-TED et en faire une image. 
(Rires) 
Vous bénéficiez ainsi de toute l’idéation et de l’enrichissement créatif, ainsi que de la prise en charge des détails pour vous par ChatGPT. La réponse n’est pas seulement une idée de repas, mais d’une réponse très, très détaillée. Voyons ce que nous allons obtenir. Mais ChatGPT ne génère pas seulement des images dans ce cas - pardon, il ne génère pas du texte, mais aussi une image. C’est quelque chose qui étend vraiment la puissance de ce qu’il peut faire à votre place en comprenant votre intention. Tout ceci est une démo en direct, entièrement généré par l’IA à la volée. Je ne sais donc pas ce que nous allons voir. Cela semble merveilleux. 
(Applaudissements) 
J’ai faim rien qu’en le regardant. 
Nous avons étendu ChatGPT à d’autres outils, par exemple la mémoire. Vous pouvez dire : « Sauvegarde ceci pour plus tard ». L’intéressant avec ces outils est qu’ils sont faciles à inspecter. Ce pop-up « Utilisez l’application DALL-E » - ceci sera disponible aux utilisateurs de ChatGPT dans les mois à venir - vous pouvez voir ce qu’il a fait, en l’occurrence, écrire une commande comme un humain. Vous avez donc la possibilité d'inspecter la façon dont la machine utilise ces outils, ce qui nous permet de fournir un feedback. 
C'est sauvegardé. Je vais vous montrer comment utiliser ces informations et les intégrer à d'autres applications. Vous pouvez dire : « Maintenant, fais la liste de courses pour la chose savoureuse que j’ai suggérée tout à l’heure. » Rendons les choses un peu plus compliquées. « Et tweete-la aux spectateurs de TED. » 
(Rires) 
Si vous préparez ce merveilleux repas, je veux absolument le goûter. 
Vous pouvez voir que ChatGPT sélectionne tous ces outils sans que j’aie à lui dire explicitement lesquels utiliser. Je pense que cela montre une nouvelle façon de penser l'interface utilisateur. Nous sommes tellement habitués à avoir des applications, que nous copions/collons de l’une à l’autre. Ça marche bien quand on connaît les menus et les options de l’application. Oui, j’aimerais que tu le fasses. Oui, s’il te plaît. C’est bien d’être poli. 
(Rires) 
Grâce à cette interface en langage unifié qui se superpose aux outils, l’IA vous affranchit en quelque sorte de tous ces détails. Vous n’avez donc pas besoin d’expliquer chaque détail de ce qui est censé se passer. 
Ma démo est en direct, donc parfois l'inattendu nous arrive. Mais jetons un œil à la liste de courses tant qu’on y est. Vous pouvez voir que nous avons envoyé une liste d’ingrédients. Voici ce dont vous avez besoin. Ce qui est vraiment intéressant, c’est que l’interface utilisateur a encore de la valeur. Si vous regardez ceci, vous pouvez toujours cliquer et modifier les quantités. Je pense que cela montre que les interfaces utilisateur traditionnelles ne disparaîtront pas. C’est une façon augmentée de les construire. Voici le tweet qui a été rédigé pour validation, un point également important. On peut cliquer sur « Exécuter », et étant le décideur, on peut vérifier, et changer le travail de l’IA si on veut. Après ce talk, vous pourrez y accéder vous-mêmes. Et voilà. C'est cool. Merci à vous. 
(Applaudissements) 
Revenons à la présentation. L’important dans ce développement n’est pas seulement de construire ces outils, mais aussi d’apprendre à l’IA à les utiliser. Que voulons-nous qu’elle fasse lorsque nous posons ces questions abstraites ? Pour cela, nous utilisons une vieille idée. Alan Turing écrivait en 1950 qu’aucun programme ne saurait passer son test, à moins d’apprendre. Ce serait une machine, comme un enfant humain, qu’on éduquerait au fur et à mesure. Un enseignant humain donnerait des récompenses et des punitions selon des réponses bonnes ou mauvaises. 
Nous avons le même processus en deux étapes avec ChatGPT. On a produit ce que Turing aurait appelé un enfant-machine via un apprentissage non supervisé. Nous lui montrons tout Internet et lui disons : « Prédis ce qui va arriver dans un texte que tu n’as jamais vu. » Ce processus lui confère tout un tas de compétences extraordinaires. Par exemple, ce problème de maths. La seule façon de dire ce qui vient ensuite - la valeur « 9 » ici - c’est de résoudre ce problème. 
Mais il y a aussi une deuxième étape : expliquer à l’IA quoi faire de ces compétences. D’où la nécessité d’un feedback. L’IA essaye plusieurs choses, fait plusieurs suggestions, puis un humain les évalue, les compare. Cela renforce non seulement la chose spécifique que l’IA a dite, mais aussi, et c’est important, le process utilisé par l’IA pour donner la réponse. Cela lui permet de généraliser, de déduire votre intention et de l’appliquer à des scénarios inconnus, pour lesquels elle n’a pas reçu de feedback. 
Parfois, ce que nous devons enseigner à l’IA n’est pas ce à quoi on s’attend. Quand on a présenté GPT-4 pour la première fois à la Khan Academy, ils ont dit : « Génial, on va pouvoir enseigner des choses merveilleuses. Seul problème : ça ne vérifie pas les compétences des élèves en maths. On peut répondre que 5+7=15 et faire comme si de rien n’était. » Nous avons donc recueilli des feedbacks. Sal Khan lui-même a été très aimable ; il a consacré 20 heures de son temps pour travailler avec notre équipe à éduquer à la machine. Er en quelques mois, nous avons pu apprendre à l’IA que, dans ce type de scénario, il faut vraiment se méfier des humains. Cette méthode nous a permis d'apporter de nombreuses améliorations aux modèles. Et lorsque vous cliquez sur le pouce vers le bas, cela revient à envoyer un signal à notre équipe : « Voici un sujet sur lequel vous devriez recueillir des feedbacks. » C’est une façon pour nous d’écouter nos utilisateurs et de nous assurer que nous construisons quelque chose de plus utile. 
Il n’est pas facile de fournir un feedback de qualité. Si vous demandez à un enfant de ranger sa chambre, si vous ne faites qu'inspecter le sol, vous ne savez pas s’il apprend juste à cacher tous ses jouets dans le placard. Il s'agit d'une belle image générée par DALL-E, soit dit en passant. Le même type de raisonnement s’applique à l’IA. Plus les tâches seront difficiles, plus la qualité de nos feedbacks devra s’améliorer. Mais l’IA elle-même est heureuse de nous aider à fournir un feedback encore meilleur et à accroître notre capacité à la superviser au fil du temps. Je vais vous montrer ce que je veux dire. 
Par exemple, vous pouvez poser à GPT-4 ce type de question : combien de temps s’est écoulé entre ces deux articles fondateurs sur l’apprentissage non supervisé et celui basé sur les feedbacks ? Il répond qu’il s’est écoulé deux mois, mais est-ce vrai ? Ces modèles ne sont pas fiables à 100 %, même s’ils s’améliorent à chaque fois que nous fournissons un feedback. Mais nous pouvons utiliser l’IA pour vérifier, vérifier en fait son propre travail. Vous pouvez lui dire : « Vérifie pour moi. » 
Dans ce cas, j’ai donné à l’IA un nouvel outil, un outil de navigation Internet qui lui permet d’effectuer des recherches et de cliquer sur des pages web. Et elle écrit le fil de ses pensées en même temps. Elle dit : « Je vais chercher ceci » et effectue la recherche. Elle trouve la date de publication dans les résultats. Elle fait alors une autre recherche. Elle clique sur l’article. Vous pouvez faire tout cela, mais c’est très fastidieux. Les humains n’en ont pas envie. Il est plus amusant d’être le patron, d’être le gestionnaire qui va, si vous le souhaitez, vérifier le travail. Le résultat qui s’affiche vous permet de vérifier très facilement n’importe quel élément de la chaîne de raisonnement. Il s’est avéré que deux mois, c’était faux. La bonne réponse est deux mois et une semaine. 
(Applaudissements) 
Revenons à la présentation. Ce qui me paraît intéressant dans tout ce processus, c’est qu’il s’agit d’une collaboration entre un humain et une IA. L’humain utilise cet outil de vérification des faits afin de produire des données pour une autre IA afin que celle-ci devienne plus utile à l’humain. Je pense qu'il s'agit là d'un exemple de ce qui devrait être de plus en plus courant à l’avenir : les humains et les machines mettent beaucoup de soin et de délicatesse dans leur manière de collaborer pour résoudre un problème. Les humains assurent la gestion, la supervision et le feedback, les machines travaillent d’une manière inspectable et digne de confiance. Ensemble, nous sommes en mesure de créer des machines encore plus fiables. Je pense que si on parvient à régler ce processus, nous serons en mesure de résoudre des problèmes impossibles. 
Et pour vous donner une idée du degré d’impossibilité dont je parle, je pense qu’on saura repenser presque tous les aspects de la façon d’interagir avec les ordinateurs. Pensez par exemple aux tableurs. Ils existent sous une forme ou une autre depuis, disons, 40 ans avec VisiCalc. Je ne pense pas qu'ils aient beaucoup changé depuis. Voici un tableur listant tous les articles sur l’IA publiés sur l’arXiv depuis 30 ans. Il y en a environ 167 000. Vous pouvez voir les données ici. Mais laissez-moi vous montrer comment ChatGPT analyse cet ensemble de données. 
Nous pouvons donner à ChatGPT l’accès à un autre outil, un interpréteur Python, afin qu’il puisse exécuter du code, comme le ferait un data scientist. Vous pouvez littéralement télécharger un fichier puis poser des questions. Très pratique, il reconnaît le type de fichier - un CSV, des valeurs séparées par des virgules - et va l’analyser pour vous. La seule information est le nom du fichier, le nom des colonnes comme vous l’avez vu et les données elles-mêmes. Et de ça, il est capable de déduire ce que ces colonnes signifient réellement. Il n’y avait pas d’informations sémantiques. Elle rassemble sa connaissance du monde : « Ah oui, arXiv est un site sur lequel les gens soumettent des articles et donc ces valeurs entières sont le nombre d’auteurs dans l’article. » Tout cela, c’est du travail pour un humain, et l’IA est heureuse d’aider à le faire. Maintenant, je ne sais même pas quoi demander. Heureusement, vous pouvez demander à la machine : « Peux-tu faire des graphiques exploratoires ? » Là encore, une instruction de haut niveau avec beaucoup d’intentions non dites. J’ignore ce que je veux. L’IA doit déduire ce qui pourrait m’intéresser. Et elle a trouvé de bonnes idées, je pense. Un diagramme du nombre d’auteurs par article, une série chronologique d’articles par année, un nuage de mots des titres. Tout cela sera très intéressant à voir. Et ce qui est génial, c’est qu’il peut vraiment le faire. Voici une courbe en cloche : trois est le nombre le plus courant. Il va ensuite établir ce joli graphique du nombre d’articles par année. Mais il se passe quelque chose de fou en 2023. C’était une exponentielle et elle s’est effondrée. Que se passe-t-il ? D'ailleurs, tout ceci est du code Python, vous pouvez l'inspecter. Passons au nuage de mots. Vous voyez toutes ces choses merveilleuses qui apparaissent dans les titres. 
Mais je suis mécontent de la courbe. Cela donne une mauvaise image de 2023. Bien sûr, le problème est que l'année n'est pas terminée. Je vais donc challenger la machine. [Attends, ce n’est pas juste ! L’année 2023 n’est pas terminée.] [Quel est le pourcentage d’articles postés en 2022 avant le 13 avril ?] Le 13 avril était donc la date limite, je crois. [Peux-tu utiliser cette date pour faire une projection juste ?] On va voir, la question est ambitieuse. (Rires) 
Vous savez, encore une fois, j’ai l’impression que j’attendais plus de la machine. Je voulais vraiment qu’elle remarque cette erreur. C’est peut-être un peu exagéré d’attendre qu’elle déduise par magie que c’était ce que je voulais. Mais j’ai saisi mon intention, j’ai fourni cet élément supplémentaire pour l’aider. Et sous le capot, l’IA écrit à nouveau du code, donc si vous voulez l’inspecter, c'est tout à fait possible. Et maintenant, elle fait la projection correcte. 
(Applaudissements) 
Elle met même à jour le titre. Je ne l’avais pas demandé, mais elle sait ce que je veux. 
Nous allons maintenant revenir à la présentation. Cette diapositive montre une parabole de la façon dont je pense que nous - Une vision de la façon dont on pourrait utiliser cette technologie à l’avenir. Quelqu’un a amené son chien très malade chez le vétérinaire, qui a pris la mauvaise décision : « Attendons de voir. » Le chien ne serait pas là aujourd’hui si on l’avait écouté. Entre-temps, il a fourni l’analyse de sang, le dossier médical complet, à GPT-4, qui a dit : « Je ne suis pas vétérinaire, parlez à un professionnel, mais voici quelques hypothèses. » Il a transmis cela à un second vétérinaire qui a utilisé ces informations pour sauver la vie du chien. Ces systèmes ne sont pas parfaits. Il ne faut pas trop s'y fier. Mais cette histoire, je pense, montre qu’un humain avec un professionnel de santé et ChatGPT comme partenaire de réflexion a pu obtenir un résultat qui ne se serait pas produit autrement. Je pense qu’on doit tous y réfléchir lorsque nous nous demandons comment intégrer ces systèmes dans notre monde. 
Je crois profondément que le bon usage de l’IA nécessitera la participation de tous. Il faut décider comment on veut qu’elle s’intègre, une sorte de code de la route qui dira ce qu’une IA fera et ne fera pas. S’il y a une chose à retenir de ma conférence, c’est que cette technologie est différente. Différente de tout ce que les gens avaient prévu. Nous devons donc tous nous informer. Honnêtement, c’est pourquoi nous avons lancé ChatGPT. 
Ensemble, je pense que nous pouvons réaliser la mission d’OpenAI, qui est de veiller à ce que l’IA générale profite à l’ensemble de l’humanité. 
Je vous remercie. (Applaudissements) 
(Fin des applaudissements) 
Chris Anderson : Greg. Waouh ! Je veux dire... Je soupçonne que chacun ici présent est en proie à un sentiment d’ébranlement, qu’un très grand nombre de spectateurs se disent : « Oh mon Dieu, je dois repenser à peu près tout ce qui concerne ma façon de travailler. » Ce sont de nouvelles possibilités. Qui pense qu’il doit repenser sa façon de faire les choses ? Oui, c'est incroyable, mais c'est aussi très effrayant. Alors parlons-en, Greg, parlons-en. 
Ma première question est : comment vous avez fait ça ? 
(Rires) 
OpenAI compte quelques centaines d'employés. Google a des milliers d’employés qui travaillent sur l’IA. Pourquoi est-ce vous qui êtes à l’origine de cette technologie 
qui a choqué le monde ? Greg Brockman : On est tous juchés sur les épaules de géants. Les progrès du calcul, de l’algorithmie, des données, c’est vrai dans toute l’industrie. Mais au sein d’OpenAI, nous avons fait des choix délibérés dès les premiers jours. Le premier a été de faire face à la réalité telle qu’elle se présente. Nous avons réfléchi très sérieusement à des questions comme : « Que faut-il faire pour progresser ? » Beaucoup de choses n’ont pas fonctionné, vous voyez le reste. La chose la plus importante a été de réunir des personnes très différentes et de les faire travailler de manière harmonieuse. 
CA : Pourrions-nous avoir de l’eau ? Je pense que nous allons en avoir besoin. Mais n’y a-t-il pas aussi quelque chose dans le fait que vous ayez vu quelque chose dans ces modèles linguistiques qui a permis en continuant à investir dans ces modèles et à les développer, Que quelque chose puisse émerger à un moment ou à un autre ? 
GB : Oui. Et honnêtement, je pense que l’histoire est assez édifiante. L’apprentissage profond de haut niveau, comme nous savions ce que nous voulions, c’était une expé. Comment le faire exactement ? Je pense qu'au début, nous ne le savions pas. On a essayé plein de choses, Une personne travaillait sur l’entraînement d’un modèle prédisant la lettre suivante dans les commentaires d’Amazon, et il a obtenu un résultat - c’est un processus syntaxique, le modèle va prédire où vont les virgules, où sont les noms et les verbes. Mais il a en fait obtenu un logiciel d’analyse de sentiment. Il pouvait dire si une critique était positive ou négative. Aujourd’hui, tout le monde sait faire cela. Mais c’était la première fois que l’on voyait cette émergence, cette sorte de sémantique qui émergeait de ce processus syntaxique sous-jacent. On savait qu’il fallait passer à l’échelle, voir où ça allait. 
CA : Cela permet d’expliquer l’énigme qui déconcerte ceux qui s’y intéressent. Car ces objets sont décrits comme des machines à prédire. Et pourtant, ce qui en sort donne l’impression... Il semble impossible que cela vienne d’une machine à prédire, ce que vous nous avez montré tout à l’heure. L’idée clé de l’émergence est que si on pousse le modèle encore plus loin, quelque chose émerge. On voit ça chez les fourmis : elles courent partout, mais lorsqu’on en réunit assez, on obtient un comportement émergent complètement différent. Comme une ville où on n’a que quelques maisons au départ. Mais plus le nombre augmente, des choses émergent, comme des banlieues, des centres culturels et des embouteillages. Racontez-nous quand vous avez vu surgir quelque chose qui vous a époustouflé et que vous n'aviez pas vu venir. GB : Oui, eh bien... Essayez ceci dans ChatGPT : additionnez des nombres de 40 chiffres - 
CA : 40 chiffres ? GB : Oui. Il saura le faire, ce qui signifie qu’il l’a appris. Mais si on lui demande d’additionner un nombre à 40 chiffres et un nombre à 35, il se trompe souvent. Vous pouvez donc voir qu'il apprend vraiment le processus, mais qu’il ne l’a pas généralisé. On ne peut pas retenir cette table - c’est plus qu’il n’y a d’atomes dans l’univers. Il a donc appris quelque chose de général, mais pas encore totalement appris qu’il peut généraliser cela à l’addition de nombres arbitraires 
de longueurs arbitraires. CA : Vous lui avez permis de prendre une autre dimension et d’ingurgiter un nombre incroyable de textes. Et il apprend des choses que vous ne le pensiez pas capable d’apprendre. 
GB Oui, mais c’est aussi plus nuancé. Une science dans laquelle on commence à être vraiment bons, c’est prédire certaines de ces capacités émergentes. Et pour ce faire, une des choses très sous-estimées dans ce domaine, c'est la qualité de l'ingénierie. Nous avons dû reprogrammer presque tout. Dans la construction d'une fusée, les tolérances sont incroyablement faibles. C’est pareil en apprentissage automatique, chaque programme doit être bien conçu pour pouvoir commencer à prédire. Il existe des abaques incroyablement précis qui renseignent sur cet aspect fondamental de l’intelligence. Lisez notre blog sur GPT-4, vous y verrez toutes ces courbes. Et maintenant, on commence à prédire. Par exemple, la performance sur les problèmes de codage. Nous examinons des modèles 10 000 ou 1 000 fois plus petits. On a donc un passage à l’échelle en douceur, même si nous n’en sommes qu’aux balbutiements. 
CA : Voici l’une des grandes craintes que cela suscite. S’il est clair qu’à mesure que vous développez ce modèle, émergent des choses que vous pouvez peut-être prédire avec un certain niveau de confiance, mais qui sont capables de vous surprendre. Quel est le risque de voir émerger quelque chose de vraiment terrible ? 
GB : Tout est une question de degré, d’échelle et de timing. Une chose échappe aux gens : l’intégration avec le monde est aussi incroyablement émergente, mais également très puissante. C’est pourquoi il est si important de le déployer progressivement. C’est ce que j’ai dit juste avant : je mets tous mes efforts à obtenir un feedback de haute qualité. Les tâches qui sont effectuées peuvent être inspectées. Comme pour le problème de maths, on sait dire : « Non, non, machine, sept était la bonne réponse. » Même résumer un livre est difficile à superviser. Comment savoir s’il est bon ? Il faut le lire en entier.  Personne ne veut le faire.   
(Rires) Il est donc important de procéder étape par étape. Et nous décidons de passer aux résumés de livres, nous devons le superviser correctement. Nous devons conserver la trace de ce qu’elles font pour vérifier qu’elles peuvent réaliser nos objectifs. Nous devons trouver des façons encore meilleures, plus efficaces, plus fiables pour passer à l’échelle, pour que la machine s’aligne sur vous. 
CA : Nous entendrons plus tard des critiques qui disent qu’il n’y a pas de véritable compréhension, que le système va toujours - que nous n’allons jamais savoir s’il ne génère pas d’erreurs, qu’il n’a pas de bon sens, etc. Pensez-vous, Greg - c’est vrai à un moment donné - que le passage à l’échelle et le feedback humain dont vous avez parlé vont fondamentalement l’emmener sur le chemin de la vérité, de la sagesse, etc., avec un haut degré de confiance. Pouvez-vous en être sûr ? 
GB : Je pense qu’OpenAI - La réponse courte est oui. Je crois qu’on se dirige vers ça. L’approche d’OpenAI a toujours été de nous confronter à la réalité. Ce domaine est celui des promesses non tenues, des experts qui prédisent une chose, alors qu’une autre arrive. On dit depuis 70 ans que les réseaux neuronaux ne fonctionnent pas - exact. Peut-être qu’il faut encore attendre un an de plus. Mais notre approche, c’est : pour voir vraiment une technologie à l’œuvre, il faut en repousser les limites car c'est ainsi que l'on peut passer à un nouveau paradigme. On n’en a pas encore exploré toutes les possibilités. 
CA : Vous avez adopté une position controversée, dire que la bonne façon de faire est de la rendre publique et de l’exploiter. Au lieu que ce soit votre équipe, c’est le monde entier qui donne un feedback. Mais, si de mauvaises choses doivent émerger, c’est trop tard. L’histoire originale que j’ai entendue sur OpenAI, fondée sans but non lucratif, était que vous serviriez de contrôle sur les grandes entreprises qui faisaient des choses inconnues, et peut-être maléfiques, avec l’IA. Vous alliez construire des modèles qui leur demanderaient des comptes et seraient capables de freiner l’IA, si nécessaire. C’est ce que j’ai entendu. Et pourtant, ce qui s’est passé, c’est le contraire. Votre lancement de GPT, et en particulier de ChatGPT, a provoqué une telle onde de choc que Google, Meta et d’autres essaient frénétiquement de rattraper leur retard. Certaines de leurs critiques ont été que vous les forcez à sortir cela sans garde-fous appropriés, sinon ils meurent. Comment faites-vous pour démontrer que ce que vous avez fait est responsable et pas irréfléchi ? 
GB : Nous réfléchissons à ces questions tout le temps. Sérieusement, tout le temps. Je ne pense pas que nous ayons toujours raison. Mais une chose a été extrêmement importante, depuis le début, quand on cherchait comment construire une IA générale qui servirait à toute l’humanité. Comment est-on censé faire cela ? D’ordinaire, on construit en secret, on obtient cette chose super-puissante, on en détermine la sécurité et on appuie sur « Go » en espérant que tout aille bien. Je ne sais pas comment faire ça, peut-être quelqu’un le sait. Mais pour moi, c’était terrifiant, ça ne m’allait pas. C'est pourquoi je pense que cette approche alternative est la seule voie que je voie : il faut se confronter à la réalité. Il faut laisser aux gens le temps de donner leur avis. Avant que ces machines soient parfaites, qu’elles soient super-puissantes, vous devez pouvoir les voir à l’œuvre. On l’a vu avec GPT-3, non ? Avec GPT-3, nous craignions vraiment que les gens s’en servent en premier pour générer de la désinformation pour influencer des élections. Et non, en premier, ils ont généré des spams pour le Viagra. 
(Rires) 
CA : Ça, c’est mal, mais il y a bien pire. Voici une expérience de pensée pour vous. Supposons que vous soyez dans une pièce avec une boîte sur une table. Vous pensez que dans cette boîte se trouve une chose qui a de fortes chances d’être absolument glorieuse, qui va offrir de beaux cadeaux à votre famille et à tout le monde. Mais il est inscrit en tout petits caractères : « Pandore ». Il y a une possibilité que cela puisse lâcher des maux inimaginables sur le monde. Ouvrez-vous cette boîte ? 
GB : Absolument pas. Je pense qu’il ne faut pas procéder ainsi. Et honnêtement - Je vais vous raconter une histoire que je n’ai jamais racontée. Peu après la création d’OpenAI, j’étais à Porto Rico pour une conférence sur l’IA. J’étais dans ma chambre, je regardais cette mer magnifique, tous ces gens qui s’amusaient. Pensez-y un instant, si vous pouviez choisir d’ouvrir cette boîte de Pandore, soit dans cinq ans, soit dans 500 ans, que choisiriez-vous ? D’un côté, on se dit que pour soi, il vaut mieux peut-être que ce soit dans cinq ans. Mais 500 ans, c’est mieux pour bien faire les choses, que choisissez-vous ? À cet instant-là, c’était clair : « Bien sûr que je choisis les 500 ans. » Mon frère était dans l'armée à l'époque et il a risqué sa vie d'une manière bien plus réelle que n’importe qui devant un ordinateur en train de développer une IA. Donc, oui, je suis convaincu qu’il faut aborder les choses correctement. Mais je ne pense pas pour autant que le problème soit celui-là. Si vous regardez l’histoire de l’informatique - je le pense vraiment - il s’agit d’un bouleversement de l’industrie voire d’un changement technologique à l’échelle du développement humain. Si on retarde le moment pour assembler les pièces du puzzle - tout est là : les ordinateurs sont toujours plus rapides, les algorithmes s’améliorent, tout ça est en train d’arriver - on risque d’être mis un jour devant le fait accompli. Donc, si quelqu’un le fait, si quelqu’un parvient à tout assembler, on se retrouvera avec cette chose très puissante, personne n’aura pu s’adapter, et qui sait quels garde-fous seront en place. Ce que je retiens de tout ça - Si on regarde d’autres technologies, comme les armes nucléaires, on passe instantanément de zéro à un potentiel infini. Mais si on examine les capacités, elles ont évolué progressivement au fil du temps. L’histoire de toutes les technologies qu’on a développées, c’est : procédons par étapes et trouvons le moyen de les gérer à chaque fois qu’on augmente leurs capacités. 
CA : Ce que j’entends, c’est que vous - Le modèle que vous voulez voir adopté, c’est que nous avons créé cet enfant extraordinaire qui pourrait avoir des super-pouvoirs qui amèneraient l’humanité dans une nouvelle phase. Il est de notre responsabilité collective de fournir les garde-fous à cet enfant pour lui apprendre à être sage et à ne pas nous anéantir. C’est ça, votre modèle ? GB : Je pense que oui. Mais il est important de dire qu’il peut aussi changer. Nous devons avancer étape par étape. Il est extrêmement important aujourd’hui de tous se familiariser avec cette technologie, de savoir donner un feedback et de décider quoi en faire. J’espère que cela continuera à être la bonne voie, mais c’est bien que nous ayons ce débat, parce que nous ne l’aurions pas si on n’avait pas publié ChatGPT. 
CA : Greg, merci beaucoup d’être venu à TED et de nous avoir époustouflés. 
(Applaudissements) 

```diff
--- test_files/532-original.txt	2025-03-07 19:07:06
+++ test_files/532-modified.txt	2025-03-07 19:07:06
@@ -1,187 +1,200 @@
 ---
 title: "ü¶úüï∏Ô∏è Using Composio With LangGraph"
-sidebarTitle: "LangGraph SDK"
-icon: "spider-web"
-description: "Integrate Composio with LangGraph Agentic workfows & enable them to interact seamlessly with external apps, enhancing their functionality and reach."
+sidebarTitle: "LangGraph"
+description: "Integrate Composio with LangGraph agents to let them seamlessly interact with external apps"
 ---
 
-**Composio enables** your **LangGraph agents** to **connect** with many **tools**!
+## Star A Repository on GitHub
+In this example, we will use LangGraph Agent to star a repository on GitHub using Composio Tools
 
-<Tip>
-  Goal: Star a repository on GitHub using natural language commands through a LangGraph Agent.
-</Tip>
-
-### Install Packages & Connect a Tool
-
-Ensure you have the necessary packages installed and connect your GitHub account to allow your agents to utilize GitHub functionalities.
-
+<Steps>
+<Step title="Install Packages">
 <CodeGroup>
-    ``` bash Run command
-    pip install composio-langgraph
-    # login to composio
-    composio login
-    # Connect your GitHub using command below, so agents can use it. 
-    composio add github
-    # Check all different apps which you can connect with
-    composio apps
-    ```
+```bash Python
+pip install composio-langgraph
+```
+```bash TypeScript
+npm i @langchain/langgraph
+npm i composio-core 
+npm i @langchain/openai
+npm i @langchain/core
+```
 </CodeGroup>
+</Step>
 
-### Goal: Use LangGraph Agent to Interact with Github using Composio
-
-<Steps>
-<Step title="Import Base Packages">
-
-Prepare your environment by initializing necessary imports from LangGraph & LangChain for setting up your agent.
-
+<Step title="Import Libraries & Initialize ComposioToolSet">
 <CodeGroup>
-    ```python Default Imports
-    import json
-    import operator
-    from typing import Annotated, Sequence, TypedDict
+```python Python
+from typing import Literal
+from langchain_openai import ChatOpenAI
+from langgraph.graph import MessagesState, StateGraph
+from langgraph.prebuilt import ToolNode
+from composio_langgraph import Action, ComposioToolSet, App
 
-    from langchain_core.messages import BaseMessage, FunctionMessage, HumanMessage
-    from langchain_core.utils.function_calling import convert_to_openai_function
-    from langchain_openai import ChatOpenAI
-    from langgraph.graph import END, StateGraph  # pylint: disable=no-name-in-module
-    from langgraph.prebuilt import (  # pylint: disable=no-name-in-module
-        ToolExecutor,
-        ToolInvocation,
-    )
-    ```
+composio_toolset = ComposioToolSet()
+```
+```typescript TypeScript
+import { LangGraphToolSet } from "composio-core";
+import { ToolNode } from "@langchain/langgraph/prebuilt";
+import { ChatOpenAI } from "@langchain/openai";
+import { StateGraph, END, MessagesAnnotation, START } from "@langchain/langgraph";
+import { HumanMessage } from "@langchain/core/messages";
+
+const composioToolset = new LangGraphToolSet();
+```
 </CodeGroup>
+</Step>
 
+<Step title="Connect Your GitHub Account">
+    <Info>You need to have an active GitHub Integration. Learn how to do this [here](https://youtu.be/LmyWy4LiedQ?si=u5uFArlNL0tew0Wf)</Info>
+    <CodeGroup>
+        ```shell CLI
+        composio login
+        composio add github
+        ```
+        ```python Python
+        request = composio_toolset.initiate_connection(app=App.GITHUB)
+        print(f"Open this URL to authenticate: {request.redirectUrl}")
+        ```
+        ```javascript TypeScript
+        const connection = await composioToolset.connectedAccounts.initiate({appName: "github"})
+        console.log(`Open this URL to authenticate: ${connection.redirectUrl}`);
+        ```
+    </CodeGroup>
+    <Tip>
+        Don't forget to set your `COMPOSIO_API_KEY` and `OPENAI_API_KEY` in your environment variables.
+    </Tip>
 </Step>
 
-<Step title="Fetch GitHub LangGraph Tools via Composio">
-
-Access GitHub tools provided by Composio for LangGraph, initialize a `tool_executor` and get OpenAI-format function schemas from the tools.
-
+<Step title="Get And Bind Tools">
+You can get all the tools for a given app as shown below, but you can get **specific actions** and filter actions using **usecase** & **tags**. Learn more [here](../../patterns/tools/use-tools/use-specific-actions)
 <CodeGroup>
-    ```python Get tools
-    from composio_langgraph import Action, ComposioToolSet
+```python Python
+tools = composio_toolset.get_tools(
+    apps=[App.GITHUB]
+)
+tool_node = ToolNode(tools)
+model = ChatOpenAI(temperature=0, streaming=True)
+model_with_tools = model.bind_tools(tools)
+```
+```typescript TypeScript
+const tools = await composioToolset.getTools({
+    apps: ["github"],
+});
 
-    # Initialize the toolset for GitHub
-    composio_toolset = ComposioToolSet()
-    tools = composio_toolset.get_actions(
-        actions=[Action.GITHUB_ACTIVITY_STAR_REPO_FOR_AUTHENTICATED_USER]
-    )
-    tool_executor = ToolExecutor(tools)
-    functions = [convert_to_openai_function(t) for t in tools]
-    ```
-</CodeGroup>
+const toolNode = new ToolNode(tools);
 
+const model = new ChatOpenAI({ temperature: 0, apiKey:""}).bindTools(tools);
+```
+</CodeGroup>
 </Step>
 
-<Step title="Prepare the model">
-
-Initialize the LLM class and bind obtained functions to the model.
-
+<Step title="Define the model calling function">
 <CodeGroup>
-    ```python Define model
-    model = ChatOpenAI(temperature=0, streaming=True)
-    model = model.bind_functions(functions)
-    ```
+```python Python
+def call_model(state: MessagesState):
+    """
+    Process messages through the LLM and return the response
+    """
+    messages = state["messages"]
+    response = model_with_tools.invoke(messages)
+    return {"messages": [response]}
+```
+```typescript TypeScript
+async function callModal(state) {
+    const { messages } = state;
+    const response = await model.invoke(messages);
+    return { messages: [response] };
+}
+```
 </CodeGroup>
-
 </Step>
 
-
-<Step title="Define the Graph Nodes">
-
-LangGraph expects you to define different nodes of the agentic workflow as seperate functions. 
-
-Here we define one node for calling the LLM and another for executing the correct tool(function), with appropriate parameters.
-
+<Step title="Define the decision function for workflow routing">
 <CodeGroup>
-    ```python Define nodes
-    def function_1(state):
-        messages = state['messages']
-        response = model.invoke(messages)
-        return {"messages": [response]}
+```python Python
+def should_continue(state: MessagesState) -> Literal["tools", "__end__"]:
+    """
+    Determine if the conversation should continue to tools or end
+    Returns:
+        - "tools" if the last message contains tool calls
+        - "__end__" otherwise
+    """
+    messages = state["messages"]
+    last_message = messages[-1]
+    if last_message.tool_calls:
+        return "tools"
+    return "__end__"
+```
+```typescript TypeScript
+async function shouldContinue(state) {
+    const { messages } = state;
+    const lastMessage = messages[messages.length - 1];
 
-
-    def function_2(state):
-        messages = state['messages']
-        last_message = messages[-1]
-
-        parsed_function_call = last_message.additional_kwargs["function_call"]
-
-        action = ToolInvocation(
-            tool=parsed_function_call["name"],
-            tool_input=json.loads(parsed_function_call["arguments"]),
-        )
-
-        response = tool_executor.invoke(action)
-
-        function_message = FunctionMessage(content=str(response), name=action.tool)
-
-        return {"messages": [function_message]}
-    ```
+    if (lastMessage.additional_kwargs.tool_calls) {
+        return "tools";
+    } else {
+        return END;
+    }
+}
+```
 </CodeGroup>
-
 </Step>
 
-
-<Step title="Define the Graph Edges">
-
-To establish the agent's workflow, we begin by initializing the workflow with an `AgentState` to maintain state, followed by specifying the connecting edges between nodes. These edges can be straightforward or conditional, depending on the workflow requirements.
-
+<Step title="Define the workflow graph">
 <CodeGroup>
-    ```python Define edges
-    def where_to_go(state):
-        messages = state['messages']
-        last_message = messages[-1]
+```python Python
+workflow = StateGraph(MessagesState)
 
-        if "function_call" in last_message.additional_kwargs:
-            return "continue"
-        else:
-            return "end"
+workflow.add_node("agent", call_model)
+workflow.add_node("tools", tool_node)
+workflow.add_edge("__start__", "agent")
+workflow.add_conditional_edges(
+    "agent",
+    should_continue,
+)
+workflow.add_edge("tools", "agent")
 
+app = workflow.compile()
+```
+```typescript TypeScript
+const workflow = new StateGraph(MessagesAnnotation)
+    .addNode("agent", callModal)
+    .addEdge(START, "agent")
+    .addNode("tools", toolNode)
+    .addConditionalEdges("agent", shouldContinue)
+    .addEdge("tools", "agent");
 
-    class AgentState(TypedDict):
-        messages: Annotated[Sequence[BaseMessage], operator.add]
-
-
-    workflow = StateGraph(AgentState)
-    workflow.add_node("agent", function_1)
-    workflow.add_node("tool", function_2)
-    workflow.add_conditional_edges(
-        "agent",
-        where_to_go,
-        {
-            "continue": "tool",
-            "end": END
-        }
-    )
-    workflow.add_edge('tool', 'agent')
-    workflow.set_entry_point("agent")
-
-    app = workflow.compile()
-    ```
+const app = workflow.compile();
+```
 </CodeGroup>
-
 </Step>
 
-
-
-<Step title="Invoke & Check Response">
-
-After the compilation of workflow, we invoke the LLM with a task, and print the final response. 
-
+<Step title="Execute the workflow">
 <CodeGroup>
-    ```python Execute workflow
-    inputs = {
+```python Python
+for chunk in app.stream(
+    {
         "messages": [
-            HumanMessage(
-                content="Star the Github repository ComposioHQ/composio"
-                )
-            ]
-        }
-    response = app.invoke(inputs)
-    print(response)
-    ```
-</CodeGroup>
+            (
+                "human",
+                "Star the GitHub Repository composiohq/composio",
+            )
+        ]
+    },
+    stream_mode="values",
+):
+    chunk["messages"][-1].pretty_print()
+```
+```typescript TypeScript
+const stream = await app.invoke({
+    messages: [
+        new HumanMessage("Star the GitHub Repository composiohq/composio"),
+    ],
+});
 
+console.log(stream.messages[stream.messages.length - 1].content);
+```
+</CodeGroup>
 </Step>
 </Steps>
```

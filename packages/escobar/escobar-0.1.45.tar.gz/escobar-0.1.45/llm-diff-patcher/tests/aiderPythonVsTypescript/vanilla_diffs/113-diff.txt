```diff
--- test_files/113-original.txt	2025-03-07 19:06:22
+++ test_files/113-modified.txt	2025-03-07 19:06:22
@@ -1,13 +1,15 @@
-"""CrewAI SWE Agent"""
+"""LangGraph SWE Agent"""
 
 import operator
+import traceback
 import typing as t
 from typing import Annotated, Literal, Sequence, TypedDict
 
 import dotenv
-from langchain_aws import BedrockChat
+from langchain_aws import ChatBedrock
 from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, ToolMessage
 from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
+from langchain_openai import ChatOpenAI
 from langgraph.graph import END, START, StateGraph
 from langgraph.prebuilt import ToolNode
 from prompts import CODE_ANALYZER_PROMPT, EDITING_AGENT_PROMPT, SOFTWARE_ENGINEER_PROMPT
@@ -18,7 +20,9 @@
 # Load environment variables from .env
 dotenv.load_dotenv()
 
+MODEL = "claude"
 
+
 def add_thought_to_request(request: t.Dict[str, t.Any]) -> t.Dict[str, t.Any]:
     request["thought"] = {
         "type": "string",
@@ -33,35 +37,41 @@
     return request
 
 
-def get_agent_graph(repo_name: str, workspace_id: str, test_command: str):
+def get_agent_graph(repo_name: str, workspace_id: str):
+
     import random
     import string
 
     random_string = "".join(random.choices(string.digits, k=6))
+    run_file = f"messages_{random_string}.txt"
 
-    def save_test_response(response: t.Dict[str, t.Any]) -> t.Dict[str, t.Any]:
-        with open(f"test_response_{random_string}.txt", "w") as f:
-            f.write(response["data"]["stdout"] + "\n" + response["data"]["stderr"])
+    if MODEL == "claude":
+        client = ChatBedrock(
+            credentials_profile_name="default",
+            model_id="anthropic.claude-3-5-sonnet-20241022-v2:0",
+            region_name="us-west-2",
+            model_kwargs={"temperature": 0, "max_tokens": 8192},
+        )
+    elif MODEL == "gpt-4o":
+        client = ChatOpenAI(
+            model="gpt-4o",
+            temperature=0.1,
+            max_completion_tokens=8192,
+        )
+    else:
+        client = ChatBedrock(
+            credentials_profile_name="default",
+            model_id="arn:aws:bedrock:us-west-2:008971668139:inference-profile/us.meta.llama3-2-3b-instruct-v1:0",
+            model_kwargs={"temperature": 0},
+            provider="meta",
+        )
 
-        return response
-
-    bedrock_client = BedrockChat(
-        credentials_profile_name="default",
-        model_id="anthropic.claude-3-5-sonnet-20240620-v1:0",
-        region_name="us-west-2",
-        model_kwargs={"temperature": 0, "max_tokens": 8192},
-    )
-
     composio_toolset = ComposioToolSet(
         workspace_config=WorkspaceType.Docker(),
         metadata={
             App.CODE_ANALYSIS_TOOL: {
                 "dir_to_index_path": f"/home/user/{repo_name}",
-            },
-            App.SHELLTOOL: {
-                "project_path": f"/home/user/{repo_name}",
-                "test_command": test_command,
-            },
+            }
         },
         processors={
             "pre": {
@@ -73,9 +83,6 @@
                 App.FILETOOL: add_thought_to_request,
                 App.CODE_ANALYSIS_TOOL: add_thought_to_request,
                 App.SHELLTOOL: add_thought_to_request,
-            },
-            "post": {
-                Action.SHELLTOOL_TEST_COMMAND: save_test_response,
             },
         },
     )
@@ -84,7 +91,7 @@
     swe_tools = [
         *composio_toolset.get_actions(
             actions=[
-                Action.FILETOOL_OPEN_FILE,
+                # Action.FILETOOL_OPEN_FILE,
                 Action.FILETOOL_GIT_REPO_TREE,
                 Action.FILETOOL_GIT_PATCH,
             ]
@@ -118,19 +125,10 @@
         ),
     ]
 
-    test_tools = [
-        *composio_toolset.get_actions(
-            actions=[
-                Action.SHELLTOOL_TEST_COMMAND,
-            ]
-        )
-    ]
-
     # Create two separate tool nodes
     code_analysis_tool_node = ToolNode(code_analysis_tools)
     file_tool_node = ToolNode(file_tools)
     swe_tool_node = ToolNode(swe_tools)
-    test_tool_node = ToolNode(test_tools)
 
     # Define AgentState
     class AgentState(TypedDict):
@@ -156,13 +154,15 @@
                 return agent.invoke(state)
 
             # If last message is AI message, add a placeholder human message
-            if isinstance(state["messages"][-1], AIMessage):
+            if MODEL == "claude" and isinstance(state["messages"][-1], AIMessage):
                 state["messages"].append(HumanMessage(content="Placeholder message"))
 
             try:
                 result = invoke_with_retry(agent, state)
-            except Exception as e:
-                print(f"Failed to invoke agent after 3 attempts: {str(e)}")
+            except Exception:
+                print(
+                    f"Failed to invoke agent after 3 attempts: {traceback.format_exc()}"
+                )
                 result = AIMessage(
                     content="I apologize, but I encountered an error and couldn't complete the task. Please try again or rephrase your request.",
                     name=name,
@@ -181,6 +181,12 @@
                     },
                     name=name,
                 )
+            with open(run_file, "w") as handle:
+                message_str = ""
+                for message in state["messages"]:
+                    message_type = type(message).__name__
+                    message_str += f"{message_type}: {str(message.content)}\n"
+                handle.write(message_str)
             return {"messages": [result], "sender": name}
 
         return agent_node
@@ -193,7 +199,7 @@
                 MessagesPlaceholder(variable_name="messages"),
             ]
         )
-        llm = bedrock_client
+        llm = client
         if tools:
             # return prompt | llm.bind_tools(tools)
             return prompt | llm.bind_tools(tools)
@@ -209,7 +215,7 @@
     code_analyzer_agent = create_agent(CODE_ANALYZER_PROMPT, code_analysis_tools)
     code_analyzer_node = create_agent_node(code_analyzer_agent, code_analyzer_name)
 
-    editing_agent = create_agent(EDITING_AGENT_PROMPT, file_tools + test_tools)
+    editing_agent = create_agent(EDITING_AGENT_PROMPT, file_tools)
     editing_node = create_agent_node(editing_agent, editor_name)
 
     # Update router function
@@ -222,6 +228,7 @@
         "continue",
         "analyze_code",
         "edit_file",
+        "swe_tool",
     ]:
         messages = state["messages"]
         for message in reversed(messages):
@@ -252,7 +259,6 @@
     workflow.add_node("code_edit_tool", file_tool_node)
     workflow.add_node("code_analysis_tool", code_analysis_tool_node)
     workflow.add_node("swe_tool", swe_tool_node)
-    workflow.add_node("test_tool", test_tool_node)
     # Add start and end
     workflow.add_edge(START, software_engineer_name)
 
@@ -272,11 +278,6 @@
         lambda x: x["sender"],
         {software_engineer_name: software_engineer_name},
     )
-    workflow.add_conditional_edges(
-        "test_tool",
-        lambda x: x["sender"],
-        {editor_name: editor_name},
-    )
 
     # Update conditional edges for the coding agent
     workflow.add_conditional_edges(
@@ -304,6 +305,8 @@
             return "code_analysis_tool"
         if "ANALYSIS COMPLETE" in last_ai_message.content:
             return "done"
+        if "EDIT FILE" in last_ai_message.content:
+            return "edit_file"
         return "continue"
 
     # Add conditional edges for the code analyzer
@@ -313,6 +316,7 @@
         {
             "continue": code_analyzer_name,
             "done": software_engineer_name,
+            "edit_file": editor_name,
             "code_analysis_tool": "code_analysis_tool",
         },
     )
@@ -327,13 +331,8 @@
             last_ai_message = messages[-1]
 
         if last_ai_message.tool_calls:
-            tool_name = last_ai_message.tool_calls[0]["name"]
-            if tool_name in [x.name for x in file_tools]:
-                return "code_edit_tool"
-            else:
-                return "test_tool"
-        if "EDITING AND TESTING COMPLETED" in last_ai_message.content:
-            return "done"
+            tool_name = last_ai_message.tool_calls[0]["name"]  # noqa: F841
+            return "code_edit_tool"
         if "EDITING COMPLETED" in last_ai_message.content:
             return "done"
         return "continue"
@@ -345,9 +344,8 @@
             "continue": editor_name,
             "done": software_engineer_name,
             "code_edit_tool": "code_edit_tool",
-            "test_tool": "test_tool",
         },
     )
 
     graph = workflow.compile()
-    return graph, composio_toolset, f"test_response_{random_string}.txt"
+    return graph, composio_toolset, run_file
```
